{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOT7zWkKdg5p7/YDXZTd65P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cb8e472370e345b9b2a833d03f69f789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09567ad1314141bf8eab4275f770107f",
              "IPY_MODEL_397fe42b9f384cc587ca2f78a96743ab",
              "IPY_MODEL_6f8c15e60a7040ee8ea830ffaf94c593"
            ],
            "layout": "IPY_MODEL_faf9c60def3a47ee81d2dc78a4402a91"
          }
        },
        "09567ad1314141bf8eab4275f770107f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_397bfe3379134a0e9ad1c66012cbf349",
            "placeholder": "​",
            "style": "IPY_MODEL_6f2372f49171442381ad5ff28e112dd2",
            "value": ".gitattributes: 100%"
          }
        },
        "397fe42b9f384cc587ca2f78a96743ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10aa8151120e42b399c3958dd02784d5",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8446bb9f13d4fdc80e2127f887191a9",
            "value": 1175
          }
        },
        "6f8c15e60a7040ee8ea830ffaf94c593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a5b17b01cb6410c89870c2790fd0e85",
            "placeholder": "​",
            "style": "IPY_MODEL_7506a00e8f9340e6b181043182c502a6",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 74.6kB/s]"
          }
        },
        "faf9c60def3a47ee81d2dc78a4402a91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "397bfe3379134a0e9ad1c66012cbf349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f2372f49171442381ad5ff28e112dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10aa8151120e42b399c3958dd02784d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8446bb9f13d4fdc80e2127f887191a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a5b17b01cb6410c89870c2790fd0e85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7506a00e8f9340e6b181043182c502a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5227030ca1864775b5f6dcc21044cdc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3f7b24ee5c147b39a1b5122781a309d",
              "IPY_MODEL_26618df17faf4d35a7a6a1fa97a8b9df",
              "IPY_MODEL_aa3d4310e7ef4ea89de53bda4a74b0c7"
            ],
            "layout": "IPY_MODEL_3d30882baad04d358c83a00ed8d008d7"
          }
        },
        "c3f7b24ee5c147b39a1b5122781a309d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d294acf8df1a458783f31d1226ed8b44",
            "placeholder": "​",
            "style": "IPY_MODEL_0792e429d9484f9fb3a9e2e472bc02e3",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "26618df17faf4d35a7a6a1fa97a8b9df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2db0606d1bea4459a186baa294b7bcc7",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac638640f91e4800822956b87d3239ea",
            "value": 190
          }
        },
        "aa3d4310e7ef4ea89de53bda4a74b0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_778b277d133e4cf4ba8ae4da65901638",
            "placeholder": "​",
            "style": "IPY_MODEL_0e956086344d400a85f814081651aee2",
            "value": " 190/190 [00:00&lt;00:00, 13.2kB/s]"
          }
        },
        "3d30882baad04d358c83a00ed8d008d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d294acf8df1a458783f31d1226ed8b44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0792e429d9484f9fb3a9e2e472bc02e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2db0606d1bea4459a186baa294b7bcc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac638640f91e4800822956b87d3239ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "778b277d133e4cf4ba8ae4da65901638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e956086344d400a85f814081651aee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df52491bd986457aa27bb4100ade5677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7255ea98dcb4d3e8bd6bf5a50555b74",
              "IPY_MODEL_978b4aba613a4cf387946ca0a3d7cff8",
              "IPY_MODEL_03caf456fb564346b45a4a8a70f8e06f"
            ],
            "layout": "IPY_MODEL_f4febbfff90945f192a90c419462b19d"
          }
        },
        "c7255ea98dcb4d3e8bd6bf5a50555b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15db6466426b4e3ba638e60ff411c543",
            "placeholder": "​",
            "style": "IPY_MODEL_cdc694e4d0134c7f9dab1422ea34d020",
            "value": "README.md: 100%"
          }
        },
        "978b4aba613a4cf387946ca0a3d7cff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98865fda13c14944a971de0de60a8097",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a02292c8c604481487309b5c3a6c4676",
            "value": 10659
          }
        },
        "03caf456fb564346b45a4a8a70f8e06f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ff52bafcdd4cd288ecf48882b02b8f",
            "placeholder": "​",
            "style": "IPY_MODEL_05d62dbb8c694b21ab445eb38afd4436",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 619kB/s]"
          }
        },
        "f4febbfff90945f192a90c419462b19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15db6466426b4e3ba638e60ff411c543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc694e4d0134c7f9dab1422ea34d020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98865fda13c14944a971de0de60a8097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a02292c8c604481487309b5c3a6c4676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88ff52bafcdd4cd288ecf48882b02b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05d62dbb8c694b21ab445eb38afd4436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0cb73e4be69443bb931dcb0aca6def9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c9469a9650946aea4b3e1dd813774b1",
              "IPY_MODEL_007385dd81034d92ab6f76f76400ced3",
              "IPY_MODEL_66c2e309fd1e4ee0a1cb77d130e3511c"
            ],
            "layout": "IPY_MODEL_af7a72d87ec040709a036da0c1571c39"
          }
        },
        "5c9469a9650946aea4b3e1dd813774b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c53b884d783a4a27871ce278ed92dbfc",
            "placeholder": "​",
            "style": "IPY_MODEL_f57901b44fba4880bfa45f20003f37ed",
            "value": "config.json: 100%"
          }
        },
        "007385dd81034d92ab6f76f76400ced3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01bc79bc8ad24235923639301068702e",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8406c6f322224d2790dbf14407f13a0b",
            "value": 612
          }
        },
        "66c2e309fd1e4ee0a1cb77d130e3511c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2771448e7ded4f8f9e6a0d331ea63780",
            "placeholder": "​",
            "style": "IPY_MODEL_0417e52414c64222808c1b903173acf0",
            "value": " 612/612 [00:00&lt;00:00, 46.6kB/s]"
          }
        },
        "af7a72d87ec040709a036da0c1571c39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c53b884d783a4a27871ce278ed92dbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f57901b44fba4880bfa45f20003f37ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01bc79bc8ad24235923639301068702e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8406c6f322224d2790dbf14407f13a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2771448e7ded4f8f9e6a0d331ea63780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0417e52414c64222808c1b903173acf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c30cf6da10e24c30a65674142f88dee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24d6b291db8446e0b778c61995b17457",
              "IPY_MODEL_7621752382864507a30ddef4a75c8e77",
              "IPY_MODEL_512d71ccc9544cccac2b5a8d2705e23e"
            ],
            "layout": "IPY_MODEL_65ffc66fd4e14f2aacdfd2707de67df4"
          }
        },
        "24d6b291db8446e0b778c61995b17457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19bc829741a24ca9ab5fadb98013e3fc",
            "placeholder": "​",
            "style": "IPY_MODEL_1cad8ee2fd6f4be59fb4f52e8e624db8",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "7621752382864507a30ddef4a75c8e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_318316a94fd44803a8435c8da0c392ca",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c3b231767124a19af231b3bcb61b6ee",
            "value": 116
          }
        },
        "512d71ccc9544cccac2b5a8d2705e23e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18f62ac8ca534be08dac1ab16bf67c93",
            "placeholder": "​",
            "style": "IPY_MODEL_8ff1dd7b752c4443895c509e4ecc2720",
            "value": " 116/116 [00:00&lt;00:00, 10.4kB/s]"
          }
        },
        "65ffc66fd4e14f2aacdfd2707de67df4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19bc829741a24ca9ab5fadb98013e3fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cad8ee2fd6f4be59fb4f52e8e624db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "318316a94fd44803a8435c8da0c392ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c3b231767124a19af231b3bcb61b6ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18f62ac8ca534be08dac1ab16bf67c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ff1dd7b752c4443895c509e4ecc2720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2e663e45b6041a9ad0b5fca4ef9f261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51edd47fc6214398aaccd87352e4dc36",
              "IPY_MODEL_e54e2a224f97442ea4afc01120a18809",
              "IPY_MODEL_91901bf8621a4b45b52fa5bff3cf3e7b"
            ],
            "layout": "IPY_MODEL_0075ac14bc95458ea53d0a948a3fb1dc"
          }
        },
        "51edd47fc6214398aaccd87352e4dc36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3317f7e2deb84095b6434be4c1588b6a",
            "placeholder": "​",
            "style": "IPY_MODEL_47193aecd54549be926312fc710c5789",
            "value": "data_config.json: 100%"
          }
        },
        "e54e2a224f97442ea4afc01120a18809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c23b783f1b804d0caaba96feb796f426",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1a04e49577f411d9d4b622c2efddd5c",
            "value": 39265
          }
        },
        "91901bf8621a4b45b52fa5bff3cf3e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71591a3ef5f04259952e00bd8da46a71",
            "placeholder": "​",
            "style": "IPY_MODEL_88127338b82d411e908ed32e99070090",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 2.80MB/s]"
          }
        },
        "0075ac14bc95458ea53d0a948a3fb1dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3317f7e2deb84095b6434be4c1588b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47193aecd54549be926312fc710c5789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c23b783f1b804d0caaba96feb796f426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1a04e49577f411d9d4b622c2efddd5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71591a3ef5f04259952e00bd8da46a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88127338b82d411e908ed32e99070090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a3e6f3d48b04b25989b5e62e4ab5304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d64cf7495e46437187e4402d2ca3082f",
              "IPY_MODEL_cef50578436743028fdc32524a0bbd50",
              "IPY_MODEL_32831d329bd04b8abe70e64bb5750a54"
            ],
            "layout": "IPY_MODEL_7caf9eea6db946b69482f3d3ca10a69c"
          }
        },
        "d64cf7495e46437187e4402d2ca3082f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d7962454a54451be55ef971fdbfcea",
            "placeholder": "​",
            "style": "IPY_MODEL_6d0a079eee8a4c509971f3dc5e5f4a62",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "cef50578436743028fdc32524a0bbd50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a427911a681e4460b69cb4774f18ea82",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ea85e1ca1664d059ec9988ebf425cb9",
            "value": 90888945
          }
        },
        "32831d329bd04b8abe70e64bb5750a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5dd4be2075f41dd9cf9cd47591fde85",
            "placeholder": "​",
            "style": "IPY_MODEL_f9877b76aab6471086a057ebc979c5fe",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 178MB/s]"
          }
        },
        "7caf9eea6db946b69482f3d3ca10a69c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0d7962454a54451be55ef971fdbfcea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d0a079eee8a4c509971f3dc5e5f4a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a427911a681e4460b69cb4774f18ea82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ea85e1ca1664d059ec9988ebf425cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5dd4be2075f41dd9cf9cd47591fde85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9877b76aab6471086a057ebc979c5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5cfe3eb788949f58193274c6e2fad5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_431e38e75a1341019d1e92ade09d5f1e",
              "IPY_MODEL_1362fcb72b1844a495a222f7701f90c7",
              "IPY_MODEL_4d22fbed7f0b44a7afd698ad7069e291"
            ],
            "layout": "IPY_MODEL_7e108204cfad4c3e8c2308dc96bd3592"
          }
        },
        "431e38e75a1341019d1e92ade09d5f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0d44655a91a410e8554781d320d6367",
            "placeholder": "​",
            "style": "IPY_MODEL_09edb0c6326c4f46b67f1425cccd8545",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "1362fcb72b1844a495a222f7701f90c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fed6b61a97d4fa3891d36aefafbaf14",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c3e7040e19940bea5983bf7c4f9f4c3",
            "value": 53
          }
        },
        "4d22fbed7f0b44a7afd698ad7069e291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03637d303d99421aaaa6d4f6b01a4a08",
            "placeholder": "​",
            "style": "IPY_MODEL_975b5406be4647279fed3efcca42842c",
            "value": " 53.0/53.0 [00:00&lt;00:00, 4.01kB/s]"
          }
        },
        "7e108204cfad4c3e8c2308dc96bd3592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0d44655a91a410e8554781d320d6367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09edb0c6326c4f46b67f1425cccd8545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fed6b61a97d4fa3891d36aefafbaf14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c3e7040e19940bea5983bf7c4f9f4c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03637d303d99421aaaa6d4f6b01a4a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975b5406be4647279fed3efcca42842c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bb84579724e4432b8292a1f6ac1707e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cd684600fac46c99b4c9cce4cddac0a",
              "IPY_MODEL_31a7db3a2c7b445089c7a2a63b93d3d0",
              "IPY_MODEL_1c79e0f887104e2eb63ddcef2d8260f3"
            ],
            "layout": "IPY_MODEL_7252e51231a14db299c583e2ef079657"
          }
        },
        "9cd684600fac46c99b4c9cce4cddac0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82beeabe720e4104a4910acaf5e6c0d6",
            "placeholder": "​",
            "style": "IPY_MODEL_4bbeb15f2e85415faa1fa54dd98e791b",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "31a7db3a2c7b445089c7a2a63b93d3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c0af801c7744939b574edd057b5f94e",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b2a3b5eee3c4334942b0f0cdd66df94",
            "value": 112
          }
        },
        "1c79e0f887104e2eb63ddcef2d8260f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccd9ff1c36c745edb729786061f71a4b",
            "placeholder": "​",
            "style": "IPY_MODEL_628726826b5643f7a95a935e8b4c03e3",
            "value": " 112/112 [00:00&lt;00:00, 8.67kB/s]"
          }
        },
        "7252e51231a14db299c583e2ef079657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82beeabe720e4104a4910acaf5e6c0d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bbeb15f2e85415faa1fa54dd98e791b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c0af801c7744939b574edd057b5f94e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b2a3b5eee3c4334942b0f0cdd66df94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ccd9ff1c36c745edb729786061f71a4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "628726826b5643f7a95a935e8b4c03e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c867c221a77468c9ef731308c248c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5bdfff3d37442378b5b8f56ff526d37",
              "IPY_MODEL_4a1c2d7ced3a41109f327dea519d1b09",
              "IPY_MODEL_274cace8f8fd43b399833ee2ed6fbd85"
            ],
            "layout": "IPY_MODEL_cd03ad445d6c4718800a907629e2f12f"
          }
        },
        "d5bdfff3d37442378b5b8f56ff526d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80213c5ca7e54e5e8e107679210e45a9",
            "placeholder": "​",
            "style": "IPY_MODEL_f8dc458322764e8bb7951d3222a84ca2",
            "value": "tokenizer.json: 100%"
          }
        },
        "4a1c2d7ced3a41109f327dea519d1b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d56bc2a4f6f94911868e0aebd26dd1f8",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90ce4b826aae487bb2032efcb7c37c8a",
            "value": 466247
          }
        },
        "274cace8f8fd43b399833ee2ed6fbd85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cae44044b1054e478d344c81de79ea48",
            "placeholder": "​",
            "style": "IPY_MODEL_c8004e5534ab45c989cd081e1d89e18b",
            "value": " 466k/466k [00:00&lt;00:00, 13.7MB/s]"
          }
        },
        "cd03ad445d6c4718800a907629e2f12f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80213c5ca7e54e5e8e107679210e45a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8dc458322764e8bb7951d3222a84ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d56bc2a4f6f94911868e0aebd26dd1f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90ce4b826aae487bb2032efcb7c37c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cae44044b1054e478d344c81de79ea48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8004e5534ab45c989cd081e1d89e18b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf1599f745154dbba949e6b8e2148441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_894d5099e86f4389919259043b5e10a0",
              "IPY_MODEL_d1beb4e9db0e44b0aefdf55083ca06ae",
              "IPY_MODEL_d4711df62b824bf3b42f339b0c3b1b7d"
            ],
            "layout": "IPY_MODEL_c2059a826c6c46038236119d54b5c1eb"
          }
        },
        "894d5099e86f4389919259043b5e10a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f1a5fda4b594287a1c7394932c596f4",
            "placeholder": "​",
            "style": "IPY_MODEL_f5f456b369af4099839cf14b428dc424",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d1beb4e9db0e44b0aefdf55083ca06ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb945aeafc284173a5969ace0d693f0c",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df4683d4fda4459c8088ec64532de9b2",
            "value": 350
          }
        },
        "d4711df62b824bf3b42f339b0c3b1b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bed34ffd6c54b6b95ca5d6f2e3d2472",
            "placeholder": "​",
            "style": "IPY_MODEL_9f5f82bcb0574dfe94fe11274ed92ab9",
            "value": " 350/350 [00:00&lt;00:00, 20.8kB/s]"
          }
        },
        "c2059a826c6c46038236119d54b5c1eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f1a5fda4b594287a1c7394932c596f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f456b369af4099839cf14b428dc424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb945aeafc284173a5969ace0d693f0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df4683d4fda4459c8088ec64532de9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bed34ffd6c54b6b95ca5d6f2e3d2472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f5f82bcb0574dfe94fe11274ed92ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2efeb87f56d34be09b9cfa27e7dc8418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db76ea05231b41c2a6f9a3080740698f",
              "IPY_MODEL_c4fcf186b0f5429aab295acc71083c4c",
              "IPY_MODEL_915ebac976bc4bdd820cd5389f345196"
            ],
            "layout": "IPY_MODEL_8016cba202e049ce991204430135b94b"
          }
        },
        "db76ea05231b41c2a6f9a3080740698f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_741ef82a94994200b1cb52cecb8814a7",
            "placeholder": "​",
            "style": "IPY_MODEL_af4be8fbbb0b48459aca42c5de95c150",
            "value": "train_script.py: 100%"
          }
        },
        "c4fcf186b0f5429aab295acc71083c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3a90d3933dc498e8e60765fdf67f124",
            "max": 13156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_530785cedf714e2cab8126ee841a7413",
            "value": 13156
          }
        },
        "915ebac976bc4bdd820cd5389f345196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d58b54cf12d84e84a18c492864c030de",
            "placeholder": "​",
            "style": "IPY_MODEL_3e05badb35a149bead679f2c256a92a7",
            "value": " 13.2k/13.2k [00:00&lt;00:00, 826kB/s]"
          }
        },
        "8016cba202e049ce991204430135b94b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "741ef82a94994200b1cb52cecb8814a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af4be8fbbb0b48459aca42c5de95c150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3a90d3933dc498e8e60765fdf67f124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "530785cedf714e2cab8126ee841a7413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d58b54cf12d84e84a18c492864c030de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e05badb35a149bead679f2c256a92a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d013cbfb43324f0488f798a1b2f2ec3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79370d11935242c1a7716d7742f62976",
              "IPY_MODEL_7c656bc8412d4debb4f1042312050306",
              "IPY_MODEL_ba5dda4601834cb9b3736e4d7ced1342"
            ],
            "layout": "IPY_MODEL_1ed9fd650b524c659311c6e043893f56"
          }
        },
        "79370d11935242c1a7716d7742f62976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6e40c7b28b147869de7a6d82b3f2b68",
            "placeholder": "​",
            "style": "IPY_MODEL_b20aec39c5a141f083ab501a1f976abb",
            "value": "vocab.txt: 100%"
          }
        },
        "7c656bc8412d4debb4f1042312050306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00f45eab4a354b9ab45ab71c3c5b87d1",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_951140055c624ddfb0db286dc0a03c83",
            "value": 231508
          }
        },
        "ba5dda4601834cb9b3736e4d7ced1342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09a467655d454e2bb56061faed4552c3",
            "placeholder": "​",
            "style": "IPY_MODEL_a077f58b612048a898775597324a7ef4",
            "value": " 232k/232k [00:00&lt;00:00, 13.5MB/s]"
          }
        },
        "1ed9fd650b524c659311c6e043893f56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e40c7b28b147869de7a6d82b3f2b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20aec39c5a141f083ab501a1f976abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00f45eab4a354b9ab45ab71c3c5b87d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "951140055c624ddfb0db286dc0a03c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09a467655d454e2bb56061faed4552c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a077f58b612048a898775597324a7ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7165f70572db4e9188f6695a095e8813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69992c3c479a48e190864630bc2f72bc",
              "IPY_MODEL_fe3590caf6a2470c9af82274432c4306",
              "IPY_MODEL_8eae510886f6435e8646dee360c7d81b"
            ],
            "layout": "IPY_MODEL_b71458cfa8294d30a7ba45dce142be48"
          }
        },
        "69992c3c479a48e190864630bc2f72bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80ee952c19d340b791cf4eb7b45a829f",
            "placeholder": "​",
            "style": "IPY_MODEL_ff29225b5d4641e5a2f77afaaafb2967",
            "value": "modules.json: 100%"
          }
        },
        "fe3590caf6a2470c9af82274432c4306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2733eaee805d4ea589bbf6f608ebf03b",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ba83936aca6462abf110cf78f4da9fb",
            "value": 349
          }
        },
        "8eae510886f6435e8646dee360c7d81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fbc7fe87b3e44a295e6e1fe233bf670",
            "placeholder": "​",
            "style": "IPY_MODEL_1e2e0708a7e74c3b953125d04196c999",
            "value": " 349/349 [00:00&lt;00:00, 24.1kB/s]"
          }
        },
        "b71458cfa8294d30a7ba45dce142be48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ee952c19d340b791cf4eb7b45a829f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff29225b5d4641e5a2f77afaaafb2967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2733eaee805d4ea589bbf6f608ebf03b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ba83936aca6462abf110cf78f4da9fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fbc7fe87b3e44a295e6e1fe233bf670": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2e0708a7e74c3b953125d04196c999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulsm27/Colab_practicse/blob/main/FAISS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lrwp0Woj7TD",
        "outputId": "38563037-c478-4baa-993d-7f29f119af02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.1.0\n",
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m984.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.17.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=d57da5416c8021bb86f6e96a040ed14b9ca1505bf09223a73cf9f5c1e3404509\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.2.2\n",
            "Collecting openai\n",
            "  Downloading openai-1.14.1-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.1\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "id": "mZxo6s-ClJK2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxWgwvehlTNU",
        "outputId": "6e2e8272-b4dc-4722-a39e-58b28fe507e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.28 (from langchain)\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.31 (from langchain)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.27-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.7.1)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.31->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-text-splitters-0.0.1 langsmith-0.1.27 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader"
      ],
      "metadata": {
        "id": "VU7TdisdlKyu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "_3WtHjh-kAx1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[1:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8SWFIrHlbN0",
        "outputId": "7d416316-f313-45f6-bbdb-8a3e25704309"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='CONTENTS \\xa0\\n\\xa0\\n3.1.   LINEAR   REGRESSION 8 \\xa0\\n3.1.1.   SIMPLE   LINEAR   REGRESSION 9 \\xa0\\nSimple   Linear   Regression 9 \\xa0\\nBest   Fit   Line 9 \\xa0\\nCost   Function 9 \\xa0\\nOptimizing   Cost   Function 10 \\xa0\\nOptimizing   Cost   Function   using   Gradient   Descent   Method 10 \\xa0\\nStrength   of   Linear   Regression   Model 12 \\xa0\\nCoeﬃcient   of   Determination   or   R-Squared   (R2) 12 \\xa0\\nRoot   Mean   Squared   Error   (RSME)   and   Residual   Standard   Error   (RSE) 12 \\xa0\\nAssumptions   of   Simple   Linear   Regression 13 \\xa0\\nHypothesis   Testing   in   Linear   Regression 15 \\xa0\\nPython   Code   -   Simple   Linear   Regression   using   Statsmodels 15 \\xa0\\nPython   Code   -   Simple   Linear   Regression   using   Scikit-Learn 16 \\xa0\\n3.1.2.   MULTIPLE   LINEAR   REGRESSION 17 \\xa0\\nMultiple   Linear   Regression 17 \\xa0\\nBest   Fit   Line 17 \\xa0\\nCost   Function 17 \\xa0\\nOptimizing   Cost   Function 17 \\xa0\\nAssumptions   of   Multiple   Linear   Regression 18 \\xa0\\nMulticollinearity 18 \\xa0\\nDummy   Variables 19 \\xa0\\nFeature   Scaling 20 \\xa0\\nFeature   Selection 20 \\xa0\\nAutomated   Feature   Selection 21 \\xa0\\nModel   Assessment 22 \\xa0\\nMallow’s   CP 22 \\xa0\\nAIC   (Akaike   Information   Criterion) 22 \\xa0\\nBIC   (Bayesian   Information   Criterion) 22 \\xa0\\nAdjusted   R2 22 \\xa0\\nPython   Code   -   Multiple   Linear   Regression 23 \\xa0\\n3.1.3.   INDUSTRY   RELEVANCE   OF   LINEAR   REGRESSION 24 \\xa0\\nPrediction   vs   Projection 25 \\xa0\\nInterpolation   vs   Extrapolation 25 \\xa0\\nParametric   vs   Non-Parametric 25 \\xa0\\nConstrained   Minimisation   vs   Unconstrained   Minimisation 26 \\xa0\\n3.2.   ADVANCED   REGRESSION 27 \\xa0\\n3.2.1.   GENERALIZED   LINEAR   REGRESSION 28 \\xa0\\nGeneral   Equation 28 \\xa0\\nFeature   Engineering 28 \\xa0\\nLinear   and   Non-Linear   Model 28 \\xa0\\nFeature   Matrix 29 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   2 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 1}),\n",
              " Document(page_content='Python   Code   -   Generalized   Regression 29 \\xa0\\n3.2.2.   REGULARIZED   REGRESSION 29 \\xa0\\nContour 29 \\xa0\\nRegularization   Regression 30 \\xa0\\nRidge   &   Lasso   Regularization 30 \\xa0\\nGraphical   representation   of   Ridge   &   Lasso   Regularization 31 \\xa0\\nPython   Code   -   Regularized   Regression 31 \\xa0\\n3.3.   PRINCIPLES   OF   MODEL   SELECTION 32 \\xa0\\n3.3.1.   PRINCIPLES   OF   MODEL   SELECTION 33 \\xa0\\nOccam’s   Razor 33 \\xa0\\nBias-Variance   Tradeoﬀ 33 \\xa0\\nOverﬁtting 34 \\xa0\\nRegularization 34 \\xa0\\nMachine   Learning   Scenario 34 \\xa0\\n3.3.2.   MODEL   EVALUATION 35 \\xa0\\nHyperparameters 35 \\xa0\\nData   Classiﬁcation 35 \\xa0\\nK-Fold   Cross   Validation 35 \\xa0\\nGrid   Search   Cross-Validation 35 \\xa0\\nLeave   One   Out   (LOO)   Cross   Validation 36 \\xa0\\nLeave   P-Out   (LPO)   Cross   Validation 36 \\xa0\\nStratiﬁed   K-Fold   Cross   Validation 36 \\xa0\\nPython   Code   -   Cross   Validation 36 \\xa0\\nBootstrapping   Method 37 \\xa0\\nPython   Code   -   Bootstrapping 38 \\xa0\\n4.1.   LOGISTIC   REGRESSION 40 \\xa0\\n4.1.1.   LOGISTIC   REGRESSION 41 \\xa0\\nBinary   Logistic   Regression 41 \\xa0\\nSigmoid   Curve 41 \\xa0\\nBest   Fit   Line 41 \\xa0\\nLikelihood   Function 42 \\xa0\\nCost   Function 42 \\xa0\\nOptimizing   Cost   Function 43 \\xa0\\nOdds   and   Log   Odds 43 \\xa0\\nMultivariate   Logistic   Regression 44 \\xa0\\nStrength   of   Logistic   Regression   Model 44 \\xa0\\nConfusion   Matrix 44 \\xa0\\nROC   Curve 45 \\xa0\\nGain   and   Lift   Chart 45 \\xa0\\nK-S   Statistic 46 \\xa0\\nGini   Coeﬃcient 46 \\xa0\\nPython   Code   -   Logistic   Regression 47 \\xa0\\n4.1.2.   CHALLENGES   IN   LOGISTIC   REGRESSION 47 \\xa0\\nSample   Selection 47 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   3 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 2}),\n",
              " Document(page_content='Segmentation 48 \\xa0\\nVariable   Transformation 48 \\xa0\\nWOE   (Weight   of   Evidence)   Transformation 48 \\xa0\\nInteraction   Variables 49 \\xa0\\nSplines 49 \\xa0\\nMathematical   Transformation 50 \\xa0\\nPCA   (Principal   Component   Analysis) 50 \\xa0\\n4.1.3.   IMPLEMENTATION   OF   LOGISTIC   REGRESSION 50 \\xa0\\nModel   Evaluation 50 \\xa0\\nModel   Validation 50 \\xa0\\nModel   Governance 50 \\xa0\\n4.2.   NAIVE   BAYES 52 \\xa0\\n4.2.1.   BAYES’   THEOREM 53 \\xa0\\nBuilding   Blocks   of   Bayes   Theorem   -   Probability 53 \\xa0\\nBuilding   Blocks   of   Bayes   Theorem   -   Joint   Probability 53 \\xa0\\nBuilding   Blocks   of   Bayes   Theorem   -   Conditional   Probability 53 \\xa0\\nBayes   Theorem 53 \\xa0\\n4.2.2.   NAIVE   BAYES 54 \\xa0\\nAssumptions   for   Naive   Bayes 54 \\xa0\\nNaive   Bayes   Theorem 54 \\xa0\\nNaive   Bayes   Classiﬁer 55 \\xa0\\n4.2.3.   NAIVE   BAYES   FOR   TEXT   CLASSIFICATION 55 \\xa0\\nDocument   Classiﬁer   Data 55 \\xa0\\nMultinomial   Naive   Bayes   Classiﬁer 56 \\xa0\\nMultinomial   Laplace   Smoothing 57 \\xa0\\nBernoulli   Naive   Bayes   Classiﬁer 58 \\xa0\\nBernoulli   Laplace   Smoothing 59 \\xa0\\nGaussian   Naive   Bayes   Classiﬁer 60 \\xa0\\nPython   Code   -   Naive   Bayes 60 \\xa0\\n4.3.   SUPPORT   VECTOR   MACHINES 62 \\xa0\\n4.3.1.   MAXIMAL   MARGIN   CLASSIFIER 63 \\xa0\\nHyperplanes 63 \\xa0\\nLinear   Discriminator 63 \\xa0\\nMaximal   Margin   Classiﬁer 63 \\xa0\\nSupport   Vectors 64 \\xa0\\n4.3.2.   SOFT   MARGIN   CLASSIFIER 64 \\xa0\\nSlack   Variable 65 \\xa0\\nSoft   Margin   Classiﬁer 65 \\xa0\\nCost   of   Misclassiﬁcation   ‘C’ 66 \\xa0\\nPython   Code   -   SVM 66 \\xa0\\n4.3.3.   KERNELS 66 \\xa0\\nFeature   Transformation 67 \\xa0\\nThe   Kernel   Trick 67 \\xa0\\nChoosing   a   Kernel   Function 68 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   4 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 3}),\n",
              " Document(page_content='Python   Code   -   Kernel   SVM 69 \\xa0\\n4.4.   TREE   MODELS 70 \\xa0\\n4.4.1.   DECISION   TREES 71 \\xa0\\nDecision   Trees 71 \\xa0\\nRegression   with   Decision   Trees 71 \\xa0\\nPython   Code   -   Decision   Tree   Classiﬁcation 72 \\xa0\\n4.4.2.   ALGORITHMS   FOR   DECISION   TREE   CONSTRUCTION 72 \\xa0\\nHomogeneity 72 \\xa0\\nGini   Index 73 \\xa0\\nEntropy 73 \\xa0\\nInformation   Gain 74 \\xa0\\nChi-Square 74 \\xa0\\nR-squared   Splitting 75 \\xa0\\nGeneric   Algorithm   for   Decision   Tree   Construction 75 \\xa0\\n4.4.3.   TRUNCATION   AND   PRUNING 76 \\xa0\\nAdvantages   of   Decision   Trees 76 \\xa0\\nDisadvantages   of   Decision   Trees 76 \\xa0\\nTree   Truncation 77 \\xa0\\nTree   Pruning 77 \\xa0\\nPython   Code   -   Decision   Tree   Regularization 78 \\xa0\\n4.4.4.   ENSEMBLES 79 \\xa0\\n4.4.5.   RANDOM   FORESTS   (BAGGING) 80 \\xa0\\nBagging 80 \\xa0\\nOOB   (Out-Of-Bag)   Error 80 \\xa0\\nAdvantages   of   Bagging 81 \\xa0\\nTime   taken   to   build   a   forest 81 \\xa0\\nPython   Code   -   Random   Forest 82 \\xa0\\n4.4.6.   RANDOM   FORESTS   (BOOSTING) 82 \\xa0\\n4.5.   CONSIDERATIONS   FOR   MODEL   SELECTION 83 \\xa0\\n4.5.1.   CONSIDERATIONS   FOR   MODEL   SELECTION 84 \\xa0\\nComparing   Diﬀerent   Machine   Learning   Models 84 \\xa0\\nEnd-to-End   Modelling 85 \\xa0\\n5.1.   CLUSTERING 88 \\xa0\\n5.1.1.   CLUSTERING 89 \\xa0\\nClustering 89 \\xa0\\nBehavioural   Segmentation 90 \\xa0\\n5.1.2.   K-MEANS   CLUSTERING 90 \\xa0\\nK-Means   Algorithm 90 \\xa0\\nCost   Function 91 \\xa0\\nOptimizing   Cost   Function 91 \\xa0\\nK-Means++   Algorithm 91 \\xa0\\nPractical   Considerations   for   K-Means   Clustering 92 \\xa0\\nChoosing   Number   of   Clusters   ‘K’ 93 \\xa0\\nSilhouette   Analysis 93 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   5 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 4}),\n",
              " Document(page_content='Elbow   Curve   Method 94 \\xa0\\nCluster   Tendency 95 \\xa0\\nHopkins   Test 95 \\xa0\\nPython   Code   -   K-Means   Clustering 96 \\xa0\\n5.1.3.   HIERARCHICAL   CLUSTERING 97 \\xa0\\nHierarchical   Clustering   Algorithm 97 \\xa0\\nLinkages 99 \\xa0\\nDendrogram 99 \\xa0\\nPython   Code   -   Hierarchical   Clustering 100 \\xa0\\n5.1.4.   K-MODES   CLUSTERING 101 \\xa0\\nK-Modes   Algorithm 101 \\xa0\\nPython   Code   -   K-Modes   Clustering 101 \\xa0\\n5.1.5.   K-PROTOTYPE   CLUSTERING 102 \\xa0\\nK-Prototype   Algorithm 102 \\xa0\\nPython   Code   -   K-Prototype   Clustering 102 \\xa0\\n5.1.6.   DB   SCAN   CLUSTERING 102 \\xa0\\nDBSCAN   Algorithm 103 \\xa0\\nPython   Code   -   DBSCAN   Clustering 104 \\xa0\\n5.1.7.   GAUSSIAN   MIXTURE   MODEL 104 \\xa0\\nHard   Clustering 104 \\xa0\\nSoft   Clustering 104 \\xa0\\nGaussian   Mixture   Model 104 \\xa0\\nAdvantages   of   GMM 105 \\xa0\\nGMM   or   EM   (Expectation-Maximisation)   Algorithm 105 \\xa0\\nPython   Code   -   GMM   Clustering 106 \\xa0\\n5.2.   PRINCIPAL   COMPONENT   ANALYSIS 107 \\xa0\\n5.2.1.   PRINCIPAL   COMPONENT   ANALYSIS 108 \\xa0\\nPrincipal   Component   Analysis   (PCA) 108 \\xa0\\nBuilding   Blocks   of   PCA   -   Basis   of   Space 108 \\xa0\\nBuilding   Blocks   of   PCA   -   Basis   Transformation 109 \\xa0\\nBuilding   Blocks   of   PCA   -   Variance 109 \\xa0\\nBuilding   Principal   Components 109 \\xa0\\nPCA   Algorithm   (Eigen   Decomposition   Method) 111 \\xa0\\nPCA   Algorithm   (Singular   Value   Decomposition   Method) 112 \\xa0\\nScree   Plots 113 \\xa0\\nPractical   Considerations   for   PCA 113 \\xa0\\nPython   Code   -   PCA 114 \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   6 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 5}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n3.   REGRESSION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   7 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 6}),\n",
              " Document(page_content='3.1.   LINEAR   REGRESSION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nLINEAR   REGRESSION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   8 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 7}),\n",
              " Document(page_content='3.1.1.   SIMPLE   LINEAR   REGRESSION \\xa0\\nModels  use  machine  learning  algorithms,  in  which  the  machine  learns  from  the  data  just  like  humans \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nlearn  from  their  experiences.  Machine  learning  models  can  be  broadly  divided  into  two  categories \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nbased  on  the  learning  algorithm  which  can  further  be  classiﬁed  based  on  the  task  performed  and \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nthe   nature   of   the   output. \\xa0\\n\\xa0\\n1.Supervised   learning   methods   :   Past   data   with   labels   is   used   for   building   the   model. \\xa0\\na.Regression  :  The  output  variable  to  be  predicted  is  a  continuous  variable,  e.g.  scores \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nof   a   student. \\xa0\\nb.Classiﬁcation  :  The  output  variable  to  be  predicted  is  a  categorical  variable,  e.g. \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nclassifying   incoming   emails   as   spam   or   ham. \\xa0\\n\\xa0\\n2.Unsupervised   learning   methods   :   No   predeﬁned   labels   are   assigned   to   past   data. \\xa0\\na.Clustering  :  No  predeﬁned  notion  of  a  label  is  allocated  to  groups/clusters  formed, \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ne.g.   customer   segmentation. \\xa0\\nSimple   Linear   Regression \\xa0\\nIt  is  a  model  with  only  one  independent  variable.  It  attempts  to  explain  the  relationship  between  a \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\ndependent  (output)  variable  and  an  independent  (predictor)  variable  using  a  straight  line \\xa0 \\xa0 \\xa0 y\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 x\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\n.   The   following   ﬁgure   explains   the   above   geometrically. x y=β0+β1 \\xa0\\n\\xa0\\nBest   Fit   Line \\xa0\\nThe  best  ﬁt  line  is  a  straight  line  that  is  the  best  approximation  of  the  given  set  of  data.  The  equation \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nfor   the   best   ﬁtting   line   is, \\xa0\\n\\xa0\\n\\xa0\\nCost   Function \\xa0\\nBut  the  line  that  ﬁts  the  data  best  will  be  one  for  which  the  prediction  errors  (one  for  each \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 n\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nobserved  data  point)  are  as  small  as  possible  in  some  overall  sense.  One  way  to  achieve  this  is  to \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\nuse  the  least  squares  criterion,  which  minimizes  the  sum  of  all  the  squared  prediction  errors \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 εi\\xa0\\n(diﬀerence   between   actual   value   and   predicted   value   of   the   dependent   variable   y),   i.e. \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   9 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 8}),\n",
              " Document(page_content=\"\\xa0\\n\\xa0\\nThe  function  mentioned  above,  is  known  as  the  cost  function  (also  referred  to  as  loss  or \\xa0 \\xa0(β ,) J0β1\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nerror  function).  It  is  a  measure  of  how  wrong  the  model  is  in  terms  of  its  ability  to  estimate  the \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nrelationship  between  and .  This  function  helps  us  in  reaching  at  optimal  values  for ’s  by  either \\xa0 \\xa0 x\\xa0 \\xa0 y\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0βi\\xa0\\xa0 \\xa0\\nminimizing   or   maximizing   the   function. \\xa0\\nOptimizing   Cost   Function \\xa0\\nThe   cost   function   can   be   optimized   by   using   any   of   the   below   optimisation   methods. \\xa0\\n\\xa0\\n1)Closed  Form  Solution  :  It  is  a  mathematical  process  that  can  be  completed  in  a  ﬁnite  number \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nof  operations.  A  closed  form  solution  is  nearly  always  desirable  because  it  means  that  a \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nsolution  can  be  found  eﬃciently.  Unfortunately,  closed  form  solutions  are  not  always \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\npossible. \\xa0\\n\\xa0\\n\\xa0\\n2)Iterative  Form  Solution  :  It  is  a  mathematical  process  that  uses  an  initial  guess  to  generate  a \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nsequence  of  improving  approximate  solutions  for  a  class  of  problems,  in  which  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 nt h\\napproximation  is  derived  from  the  previous  ones.  An  iterative  method  is  called \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 n ) (−1t h\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nconvergent  if  the  corresponding  sequence  converges  for  given  initial  approximations. \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nIterative  methods  are  often  the  only  choice  for  nonlinear  equations.  However,  iterative \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmethods  are  often  useful  even  for  linear  problems  involving  a  large  number  of  variables \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\n(sometimes  of  the  order  of  millions),  where  direct  methods  would  be  prohibitively  expensive \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\n(and   in   some   cases   impossible)   even   with   the   best   available   computing   power. \\xa0\\n\\xa0\\na)First   Order   (Gradient   Descent   Method) \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nb)Second   Order   (Newton’s   Method) \\xa0\\n\\xa0\\n\\xa0\\nOptimizing   Cost   Function   using   Gradient   Descent   Method \\xa0\\nGradient  descent  is  a  way  to  minimize  an  objective  function ,  parameterized  by  a  model's \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0(θ) J \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nparameters ,  by  updating  the  parameters  in  the  opposite  direction  of  the  gradient  of  the \\xa0  θ ∈ℜ\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nobjective  function  w.r.t.  to  the  parameters.  The  learning  rate  determines  the  size  of  the \\xa0 \\xa0 J(θ)d\\ndθ\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0η\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nsteps  taken  to  reach  a  (local/global)  minimum.  In  other  words,  one  follows  the  direction  of  the  slope \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nof   the   surface   created   by   the   objective   function   downhill   until   a   valley   is   reached. \\xa0\\n\\xa0\\nMathematically, \\xa0\\n\\xa0\\n1)Start   at   point   θ0\\xa0\\n\\xa0\\n2)Move   to   new   points     such   that, ,,...... θ1θ2θ3 \\xa0\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   10 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 9}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\n\\xa0\\n3)Move   till   the   sequence     converges   to   the   local   minimum. θt \\xa0\\n\\xa0\\nThe  term ,  called  the  Learning  Rate  controls  the  steps  taken  in  downward  direction  in  each \\xa0 \\xa0η\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\niteration.  If  it  is  too  low,  the  algorithm  may  take  longer  to  reach  the  minimum  value.  On  the  other \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nhand,  if  it  is  high,  the  algorithm  may  overstep  the  minimum  value.  The  following  ﬁgures  demonstrate \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nthe   diﬀerent   scenarios   one   can   encounter   while   conﬁguring   the   learning   rate. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe   various   variations   of   Gradient   Descent   are, \\xa0\\n\\xa0\\n1)Batch  or  Vanilla  gradient  descent  :  It  computes  the  gradient  of  the  cost  function  w.r.t.  to  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nparameters     for   the   entire   training   dataset. θ \\xa0\\n\\xa0\\n2)Stochastic  gradient  descent  :  In  contrast  to  batch  gradient  descent  it  performs  a  parameter \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nupdate   for   each   training   example   . θt\\xa0\\xa0\\n\\xa0\\n3)Mini-Batch   gradient  descent  :  It  takes  the  best  of  both  worlds  and  performs  an  update  for \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nevery   mini-batch   of     training   examples. n \\xa0\\n\\xa0\\nThe  following  ﬁgures  show  the  paths  taken  by  various  gradient  descent  variations  for  reaching  the \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nlocal   minima. \\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   11 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 10}),\n",
              " Document(page_content='Strength   of   Linear   Regression   Model \\xa0\\nThe  strength  of  any  linear  regression  model  can  be  assessed  using  various  metrics.  These  metrics \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nusually  provide  a  measure  of  how  well  the  observed  outcomes  are  being  replicated  by  the  model, \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nbased   on   the   proportion   of   total   variation   of   outcomes   explained   by   the   model. \\xa0\\xa0\\n\\xa0\\nThe   various   metrics   are, \\xa0\\n1)Coeﬃcient   of   Determination   or   R-Squared   (R2) \\xa0\\n2)Root   Mean   Squared   Error   (RSME)   and   Residual   Standard   Error   (RSE) \\xa0\\nCoeﬃcient   of   Determination   or   R-Squared   (R2) \\xa0\\nR-Squared  is  a  number  which  explains  what  portion  of  the  given  data  variation  is  explained  by  the \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\ndeveloped  model.  It  is  basically  the  square  of  the  Pearson’s  R  correlation  value  between  the \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nvariables.  It  always  takes  a  value  between  0  &  1.  Overall,  the  higher  the  R-squared,  the  better  the \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nmodel   ﬁts   the   data. \\xa0\\n\\xa0\\nMathematically   it   can   be   represented   as, \\xa0\\n\\xa0\\n\\xa0\\xa0\\n\\xa0\\n1)Residual  Sum  of  Squares  (RSS)  is  deﬁned  as  the  total  sum  of  error  across  the  whole  sample. \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nIt  is  the  measure  of  the  diﬀerence  between  the  expected  and  the  actual  output.  A  small  RSS \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nindicates   a   tight   ﬁt   of   the   model   to   the   data.   Mathematically   RSS   is, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n2)Total  Sum  of  Squares  (TSS)  is  deﬁned  as  the  sum  of  errors  of  the  data  points  from  the  mean \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nof   the   response   variable.   Mathematically   TSS   is, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe   following   ﬁgures   show   the   signiﬁcance   of   R2. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nRoot   Mean   Squared   Error   (RSME)   and   Residual   Standard   Error   (RSE) \\xa0\\nThe  Root  Mean  Squared  Error  is  the  square  root  of  the  variance  of  the  residuals.  It  indicates  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nabsolute  ﬁt  of  the  model  to  the  data  i.e.  how  close  the  observed  data  points  are  to  the  model’s \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\npredicted   values.   Mathematically    it   can   be   represented   as, \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   12 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 11}),\n",
              " Document(page_content=\"\\xa0\\n\\xa0\\nTo  make  this  estimate  unbiased,  one  has  to  divide  the  sum  of  the  squared  residuals  by  the  degrees \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nof  freedom  rather  than  the  total  number  of  datapoints  in  the  model.  This  term  is  then  called  the \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nResidual   Standard   Error.   Mathematically    it   can   be   represented   as, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe  R-squared  is  a  relative  measure  of  ﬁt,  whereas  the  RMSE   is  an  absolute  measure  of  ﬁt.  Because \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nthe  value  of  RMSE   depends  on  the  units  of  the  variables  (i.e.  it  is  not  a  normalized  measure),  it  can \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nchange   with   the   change   in   the   unit   of   the   variables.   Thus,   R-squared   is   a   better   measure   than   RSME. \\xa0\\nAssumptions   of   Simple   Linear   Regression \\xa0\\nRegression  is  a  parametric  approach,  meaning  it  makes  assumptions  about  the  data  for  the  purpose \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nof  analysis.  Due  to  its  parametric  side,  regression  is  restrictive  in  nature  and  fails  to  deliver  good \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nresults  with  data  sets  which  don't  fulﬁll  its  assumptions.  Therefore,  for  a  successful  regression \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nanalysis,   it’s   essential   to   validate   the   below   assumptions. \\xa0\\n\\xa0\\n1)Linearity  of  residuals  :  There  needs  to  be  a  linear  and  additive  relationship  between  the \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ndependent  variable  and  independent  variable(s).  A  linear  relationship  suggests  that  a \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nchange  in  response  due  to  one  unit  change  in  is  constant,  regardless  of  the  value  of . \\xa0\\xa0 \\xa0 y\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 x\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 x\\xa0\\nAn   additive   relationship   suggests   that   the   eﬀect   of     on     is   independent   of   other   variables. x y \\xa0\\xa0\\n\\xa0\\nIf  a  linear  model  is  ﬁt  to  a  non-linear,  non-additive  data  set,  then  the  regression  algorithm \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nwould  fail  to  capture  the  trend  mathematically,  thus  resulting  in  an  ineﬃcient  model.  Also, \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nthis   will   result   in   erroneous   predictions   on   an   unseen   data   set. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n2)Independence  of  residuals  :  The  error  terms  should  not  be  dependent  on  one  another  (like \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nin  a  time-series  data  wherein  the  next  value  is  dependent  on  the  previous  one).  There \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nshould  be  no  correlation  between  the  residual  (error)  terms.  Absence  of  this  phenomenon  is \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nknown   as   Autocorrelation. \\xa0\\n\\xa0\\nAutocorrelation  causes  the  conﬁdence  intervals  and  prediction  intervals  to  be  narrower. \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nNarrower  conﬁdence  interval  means  that  a  95%  conﬁdence  interval  would  have  lesser \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nprobability   than   0.95   that   it   would   contain   the   actual   value   of   coeﬃcients. \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   13 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 12}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\n3)Normal  distribution  of  residuals  :  The  mean  of  residuals  should  follow  a  normal  distribution \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nwith  mean  equal  to  zero  or  close  to  zero.  This  is  done  in  order  to  check  whether  the  selected \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nline   is   actually   the   line   of   best   ﬁt   or   not. \\xa0\\n\\xa0\\nIf  the  error  terms  are  non-normally  distributed,  the  conﬁdence  intervals  may  become  too \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nwide  or  narrow.  Once  the  conﬁdence  interval  becomes  unstable,  it  leads  to  diﬃculty  in \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nestimating  coeﬃcients  based  on  minimisation  of  least  squares.  This  also  suggests  that  there \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nare   a   few   unusual   data   points   which   must   be   studied   closely   to   make   a   better   model. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n4)Equal  variance  of  residuals  :  The  error  terms  must  have  constant  variance.  This  phenomenon \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nis  known  as  homoscedasticity.  The  presence  of  non-constant  variance  is  referred  to \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nheteroskedasticity. \\xa0\\xa0\\n\\xa0\\nThe  presence  of  non-constant  variance  in  the  error  terms  results  in  heteroskedasticity. \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nGenerally,  non-constant  variance  arises  in  the  presence  of  outliers  or  extreme  leverage \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nvalues  and  when  these  values  get  too  much  weight,  they  disproportionately  inﬂuence  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nmodel’s  performance.  Thereby,  causing  the  conﬁdence  interval  to  be  unrealistically  wide  or \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nnarrow   for   the   out   of   sample   predictions. \\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   14 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 13}),\n",
              " Document(page_content='Hypothesis   Testing   in   Linear   Regression \\xa0\\nEvery  time  a  linear  regression  is  performed,  one  needs  to  test  whether  the  ﬁttest  line  (  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ncoeﬃcient   )   is   signiﬁcant   or   not.   And   in   comes   the   idea   of   Hypothesis   Testing   on   . β1 β1\\xa0\\n\\xa0\\n1)Null   Hypothesis   ( )   :   H0β1=0\\xa0\\n2)Alternate   Hypothesis   ( )   :   HA= β1/0\\xa0\\n3)t-score   =     =    X )  ( s ) (−μ//√ n  β )  S E( β) (︿\\n1−0/︿\\n1\\xa0\\n4)p-value   is   calculated   from   the   cumulative   probability   for   the   given   t-score   using   the   t-table. \\xa0\\n5)If   the   p-value   is   less   than   0.05,   the   null   hypothesis   is   rejected   stating   is   signiﬁcant. β1 \\xa0\\nPython   Code   -   Simple   Linear   Regression   using   Statsmodels \\xa0\\nVisualize   Data  \\n>>   sns.pairplot(df,x_vars=[columns],y_vars=column,size=4,aspect=1,kind= \\'scatter\\' )  \\n \\nCheck   Collinearity  \\n>>   sns.heatmap(df.corr(),   cmap= \"YlGnBu\" ,   annot= True )  \\n \\nCreate   Train   &   Test   Data  \\n>>    from    sklearn.model_selection    import    train_test_split  \\n>>   X_train,X_test,y_train,y_test   =   train_test_split(X,   y,   train_size=0.7,  \\n                                    test_size=0.3,   random_state=100)  \\n \\nScaling   Features  \\n>>    from    sklearn.preprocessing    import    StandardScaler,   MinMaxScaler  \\n>>   scaler   =   StandardScaler()  \\n>>   scaler   =   MinMaxScaler()  \\n#   Any   one   of   the   scaling   methods   can   be   used  \\n>>   X_train_scaled   =   scaler.fit_transform(X_train.values.reshape(-1,1))  \\n>>   X_test_scaled   =   scaler.transform(X_test.values.reshape(-1,1))  \\n#    It   is   a   general   convention   in   scikit-learn   that   observations   are   rows,   while  \\nfeatures   are   columns.   This   is   needed   only   when   using   a   single   feature  \\n>>    import    statsmodels.api    as    sm  \\n>>   X_train_scaled_sm   =   sm.add_constant(X_train_scaled)  \\n>>   X_test_scaled_sm   =   sm.add_constant(X_test_scaled)  \\n#   In   statsmodels   intercept   variable   needs   to   be   added   explicitly  \\n \\nTrain   Model  \\n>>   model   =   sm.OLS(y_train,   X_train_scaled_sm).fit()  \\n \\nAnalyze   Model  \\n>>   model.params  \\n>>   model.summary()  \\n#   Statsmodel   provides   an   extensive   summary   of   various   metrics  \\n>>   plt.scatter(X_train_scaled,   y_train)  \\n>>   plt.plot(X_train_scaled,   coeff_A   +   coeff_B   *   X_train_scaled,    \\'r\\' )  \\n \\nAnalyze   Residuals  \\n>>   y_train_pred   =   model.predict(X_train_scaled_sm)  \\n>>   residual   =   (y_train   -   y_train_pred)  \\n>>   sns.distplot(residual,   bins   =   15)  \\n#   Checking   if   residuals   are   normally   distributed  \\n>>   plt.scatter(X_train_scaled,   residual)  \\n#   Checking   for   independence   of   residuals  \\n \\nPredict  \\n>>   y_pred   =   model.predict(X_test_scaled_sm)  \\n \\n \\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   15 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 14}),\n",
              " Document(page_content='Evaluate   Model  \\n>>    from    sklearn.metrics    import    mean_squared_error  \\n>>    from    sklearn.metrics    import    r2_score  \\n>>   rsme   =   np.sqrt(mean_squared_error(y_test,   y_pred))  \\n>>   r_squared   =   r2_score(y_test,   y_pred)  \\n \\nVisualize   Model  \\n>>   plt.scatter(X_test_scaled,   y_test)  \\n>>   plt.plot(X_test_scaled,   coeff_A   +   coeff_B   *   X_test_scaled,    \\'r\\' )  \\n \\nPython   Code   -   Simple   Linear   Regression   using   Scikit-Learn \\xa0\\nVisualize   Data  \\n>>   sns.pairplot(df,x_vars=[columns],y_vars=column,size=4,   aspect=1,kind= \\'scatter\\' )  \\n \\nCheck   Collinearity  \\n>>   sns.heatmap(df.corr(),   cmap= \"YlGnBu\" ,   annot= True )  \\n \\nCreate   Train   &   Test   Data  \\n>>    from    sklearn.model_selection    import    train_test_split  \\n>>   X_train,X_test,y_train,y_test   =   train_test_split(X,   y,   train_size=0.7,  \\n                                    test_size=0.3,   random_state=100)  \\n \\nScaling   Features  \\n>>    from    sklearn.preprocessing    import    StandardScaler,   MinMaxScaler  \\n>>   scaler   =   StandardScaler()  \\n>>   scaler   =   MinMaxScaler()  \\n#   Any   one   of   the   scaling   methods   can   be   used  \\n>>   X_train_scaled   =   scaler.fit_transform(X_train.values.reshape(-1,1))  \\n>>   X_test_scaled   =   scaler.transform(X_test.values.reshape(-1,1))  \\n#    It   is   a   general   convention   in   scikit-learn   that   observations   are   rows,   while  \\nfeatures   are   columns.   This   is   needed   only   when   using   a   single   feature  \\n \\nTrain   Model  \\n>>    from    sklearn.linear_model    import    LinearRegression  \\n>>   model   =   LinearRegression()  \\n>>   model.fit(X_train_scaled,   y_train)  \\n \\nAnalyze   Model  \\n>>   coeff_A   =   model.intercept_  \\n>>   coeff_B   =   model.coef_[0]  \\n>>   plt.scatter(X_train_scaled,   y_train)  \\n>>   plt.plot(X_train_scaled,   coeff_A   +   coeff_B   *   X_train_scaled,    \\'r\\' )  \\n \\nAnalyze   Residuals  \\n>>   y_train_pred   =   model.predict(X_train_scaled)  \\n>>   residual   =   (y_train   -   y_train_pred)  \\n>>   sns.distplot(residual,   bins   =   15)  \\n#   Checking   if   residuals   are   normally   distributed  \\n>>   plt.scatter(X_train,   residual)  \\n#   Checking   for   independence   of   residuals  \\n \\nPredict  \\n>>   y_pred   =   model.predict(X_test_scaled)  \\n \\nEvaluate   Model  \\n>>    from    sklearn.metrics    import    mean_squared_error  \\n>>    from    sklearn.metrics    import    r2_score  \\n>>   rsme   =   np.sqrt(mean_squared_error(y_test,   y_pred))  \\n>>   r_squared   =   r2_score(y_test,   y_pred)  \\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   16 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 15}),\n",
              " Document(page_content=\" \\nVisualize   Model  \\n>>   plt.scatter(X_test_scaled,   y_test)  \\n>>   plt.plot(X_test_scaled,   coeff_A   +   coeff_B   *   X_test_scaled,    'r' )  \\n \\n3.1.2.   MUL TIPLE   LINEAR   REGRESSION \\xa0\\nThe  term  multiple  in  multiple  linear  regression  gives  a  fair  idea  in  itself.  It  represents  the  relationship \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nbetween  one  dependent  variable  (response  variable)  and  several  independent  variables \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n(explanatory  variables).  The  objective  of  multiple  regression  is  to  ﬁnd  a  linear  equation  that  can  best \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\ndetermine   the   value   of   the   dependent   variable     for   diﬀerent   values   independent   variables   in   . y x\\xa0\\nMultiple   Linear   Regression \\xa0\\nMost  of  the  concepts  in  multiple  linear  regression  are  quite  similar  to  those  in  simple  linear \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nregression.   It   is   basically   an   extension   of   the   earlier   equation     to   add   more   factors. x y=β0+β1 \\xa0\\nBest   Fit   Line \\xa0\\nThe   equation   for   the   best   ﬁtting   line   with     predictors   is, k \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe  above  equation  can  be  interpreted  as  the  coeﬃcient  for  any  independent  variable  gives \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0βi\\xa0\\xa0\\xa0 \\xa0 \\xa0 xi\\xa0 \\xa0\\nthe  amount  of  increase  in  mean  response  of  per  unit  increase  in  the  variable  provided  all  the \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 y\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 xi\\xa0 \\xa0\\xa0\\xa0\\nother   predictors   are   held   constant.   The   model   now   ﬁts   a   hyperplane   instead   of   a   line. \\xa0\\nCost   Function \\xa0\\n\\xa0\\nOptimizing   Cost   Function \\xa0\\n1)Batch  Gradient  Descent  Method  :  can  be  solved  for  using  the  Batch  Gradient  Descent \\xa0 \\xa0 \\xa0 \\xa0\\xa0β\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nalgorithm   give   below, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   17 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 16}),\n",
              " Document(page_content='2)Normal  Equation  :  It  is  an  analytical  approach  to  solving  the  cost  function.  One  can  directly \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nﬁnd  the  value  of  without  using  Gradient  Descent.  This  approach  is  an  eﬀective  and  a \\xa0\\xa0 \\xa0\\xa0β\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\ntime-saving   option   when   working   with   a   dataset   with   small   features.     is   given   by, β \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThis  of  course  works  only  if  the  inverse  exists.  If  the  inverse  does  not  exist,  the  normal \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nequations  can  still  be  solved,  but  the  solution  may  not  be  unique.  The  inverse  of  exists, \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 X XT\\xa0 \\xa0\\nif  the  columns  of  are  linearly  independent  (i.e.  no  column  can  be  written  as  a  linear \\xa0 \\xa0 \\xa0\\xa0 X\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\ncombination   of   the   other   columns). \\xa0\\nAssumptions   of   Multiple   Linear   Regression \\xa0\\nAll  the  four  assumptions  made  for  Simple  Linear  Regression  (Linearity  of  residuals,  Independence  of \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nresiduals,  Normal  distribution  of  residuals,  Equal  variance  of  residuals)  still  hold  true  for  Multiple \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nLinear   Regression   along   with   a   few   new   additional   assumptions. \\xa0\\n\\xa0\\n1)Overﬁtting  :  When  more  and  more  variables  are  added  to  a  model,  the  model  may  become \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nfar  too  complex  and  usually  ends  up  with  memorizing  all  the  data  points  in  the  training  set. \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nThis  phenomenon  is  known  as  overﬁtting  of  a  model.  Overﬁtting  causes  the  model  to \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nbecome  speciﬁc  rather  than  generic.  This  usually  leads  to  high  training  accuracy  and  very \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nlow   test   accuracy. \\xa0\\n\\xa0\\n2)Multicollinearity  :  It  is  the  phenomenon  where  a  model  with  several  independent  variables, \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmay  have  some  variables  interrelated.  When  the  variables  are  correlated  to  each  other,  they \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\ncan   easily   explain   each   other   and   thus,   their   presence   in   the   model   becomes   redundant. \\xa0\\n\\xa0\\n3)Feature  Selection  :  With  more  variables  present,  selecting  the  optimal  set  of  predictors  from \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nthe  pool  of  given  features  (many  of  which  might  be  redundant)  becomes  an  important  task \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nfor   building   a   relevant   and   better   model. \\xa0\\nMulticollinearity \\xa0\\nMulticollinearity  refers  to  the  phenomenon  of  having  related  predictor  variables  in  the  input  dataset. \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nAs  multicollinearity  makes  it  diﬃcult  to  ﬁnd  out  which  variable  is  actually  contributing  towards  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nprediction  of  the  response  variable,  it  leads  one  to  conclude  incorrectly  the  eﬀects  (strong/weak)  of \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\na  variable  on  the  target  variable.  Thus,  it  is  a  big  issue  when  one  is  trying  to  interpret  the  model. \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nThough,  it  does  not  aﬀect  the  precision  of  the  predictions,  it  is  essential  to  properly  detect  and  deal \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nwith  the  multicollinearity  present  in  the  model,  as  random  removal  of  any  of  these  correlated \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nvariables  from  the  model  cause  the  coeﬃcient  values  to  swing  wildly  and  even  change  signs. \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nMulticollinearity   can   be   detected   using   the   below   methods. \\xa0\\n\\xa0\\n1)Pairwise  Correlations  :  Checking  the  pairwise  correlations  between  diﬀerent  pairs  of \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nindependent   variables   can   throw   useful   insights   in   detecting   multicollinearity. \\xa0\\n\\xa0\\n2)Variance  Inﬂation  Factor  (VIF)  :  Pairwise  correlations  may  not  always  be  useful  as  it  is \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\npossible  that  just  one  variable  might  not  be  able  to  completely  explain  some  other  variable \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nbut  some  of  the  variables  combined  might  be  able  to  do  that.  Thus,  to  check  these  sorts  of \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nrelations  between  variables,  one  can  use  VIF.  VIF  basically  explains  the  relationship  of  one \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nindependent   variable   with   all   the   other   independent   variables. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   18 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 17}),\n",
              " Document(page_content='VIF   is   given   by, \\xa0\\n\\xa0\\n\\xa0\\nwhere, \\xa0\\n refers  to  the  variable  which  is  being  represented  as  a  linear  combination  of  the i\\xa0 \\xa0\\xa0\\xa0 it h\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nrest   of   the   independent   variables. \\xa0\\n\\xa0\\nThe   common   heuristic   followed   for   the   VIF   values   is, \\xa0\\na)VIF   >   10   indicates   deﬁnite   removal   of   the   variable. \\xa0\\xa0\\nb)VIF   >   5   indicates   variable   needs   inspection. \\xa0\\nc)VIF   <   5   indicates   variable   is   good   to   go. \\xa0\\n\\xa0\\nOnce   multicollinearity   has   been   detected   in   the   dataset   then   this   can   be   dealt   using   below   methods. \\xa0\\n1)Highly   correlated   variables   can   be   dropped. \\xa0\\n2)Business   Interpretable   variables   can   be   picked   up. \\xa0\\n3)New   interpretable   features   can   be   derived   using   the   correlated   variables. \\xa0\\n4)Variable  transformations  can  be  done  using  PCA  (Principal  Component  Analysis)  or  PLS \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\n(Partial   Least   Squares). \\xa0\\nDummy   Variables \\xa0\\nWhile  dealing  with  multiple  variables,  there  may  be  some  categorical  variables  that  might  turn  out  to \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nbe  useful  for  the  model.  So,  it  is  essential  to  handle  these  variables  appropriately  in  order  to  get  a \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\ngood   model.   These   can   be   dealt   with   using   the   below   methods. \\xa0\\n\\xa0\\n1)Label  Encoding  :  It  is  used  to  transform  the  non-numerical  categorical  variables  to  numerical \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nlabels.  Numerical  labels  are  always  between  0  and .  This  is  usually  performed  on \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 n−1\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nordinal   categorical   variables. \\xa0\\n\\xa0\\n2)One  Hot  Encoding  :  One  way  to  deal  with  the  nominal  categorical  variables  is  to  create \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 n\\xa0\\nnew  columns  (dummy  variables)  each  indicating  whether  that  level  exists  or  not  using  a  zero \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\nor  one.  For  example  for  a  variable  say,  Relationship  with  three  levels  namely,  Single,  In  a \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nrelationship   and   Married,   one   can   create   a   dummy   table   like   the   following. \\xa0\\n\\xa0\\nRelationship   Status \\xa0 Single \\xa0 In   a   relationship \\xa0 Married \\xa0\\nSingle \\xa0 1 \\xa0 0 \\xa0 0 \\xa0\\nIn   a   relationship \\xa0 0 \\xa0 1 \\xa0 0 \\xa0\\nMarried \\xa0 0 \\xa0 0 \\xa0 1 \\xa0\\n\\xa0\\nDummy  Variable  Trap  is  a  scenario  where  there  are  attributes  which  are  highly  correlated \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\n(multicollinear).  After  using  one  hot  encoding,  any  one  of  the  dummy  variables  can  be \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\npredicted  with  the  help  of  the  other  remaining  dummy  variables.  Hence,  one  dummy  variable \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nis  always  highly  correlated  with  the  other  remaining  dummy  variables.  Thus,  one  needs  to \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nexclude   one   of   the   dummy   variables   before   proceeding   with   the   model. \\xa0\\n\\xa0\\n3)Dummy  Encoding  :  Another  way  to  deal  with  the  nominal  categorical  variables  is  to  create \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\n dummy  variables  rather  than  as  is  done  in  case  of  one  hot  encoding.  Taking  the n−1\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 n\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nsame   example   as   above   one   can   drop   the   Single   status   from   among   the   dummy   variables. \\xa0\\n\\xa0\\nRelationship   Status \\xa0 Single   (base) \\xa0 In   a   relationship   ( ) x1\\xa0 Married   ( ) x2\\xa0\\nSingle \\xa0 1 \\xa0 0 \\xa0 0 \\xa0\\nIn   a   relationship \\xa0 0 \\xa0 1 \\xa0 0 \\xa0\\nMarried \\xa0 0 \\xa0 0 \\xa0 1 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   19 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 18}),\n",
              " Document(page_content='\\xa0\\nIt  can  be  clearly  seen  that  there  is  no  need  of  deﬁning  three  diﬀerent  levels  as  even  after \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ndropping  one  of  the  levels,  one  can  still  be  able  to  explain  the  three  levels.  Here,  the  Single \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nstatus  is  the  base  state  and  the  other  two  statuses  represent  the  eﬀect  of  that  state  vs  the \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nbase   state.   This   can   be   mathematically   represented   as   below. \\xa0\\n\\xa0\\n\\xa0\\nFeature   Scaling \\xa0\\nAnother  important  aspect  to  consider  is  feature  scaling.  When  there  are  many  independent \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nvariables  in  a  model,  a  lot  of  them  might  be  on  very  diﬀerent  scales.  This  leads  to  a  model  with  very \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nweird  coeﬃcients,  which  are  diﬃcult  to  interpret.  Thus,  one  needs  to  scale  the  features  for  ease  of \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\ninterpretation  of  the  coeﬃcients  as  well  as  for  the  faster  convergence  of  gradient  descent  methods. \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nOne   can   scale   the   features   using   the   below   methods. \\xa0\\n\\xa0\\n1)Standardization  :  The  variables  are  scaled  in  such  a  way  that  their  mean  is  zero  and  standard \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\ndeviation   is   one. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n2)Min  Max  Scaling  :  The  variables  are  scaled  in  such  a  way  that  all  the  values  lie  between  zero \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nand   one. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nIt  is  important  to  note  that  scaling  just  aﬀects  the  coeﬃcients  and  none  of  the  other  parameters  like \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nt-statistic,  F  statistic,  p-values,  R-square,  etc.  Also  the  dummy  variables  are  usually  never  scaled  for \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nthe   ease   of   their   interpretation. \\xa0\\nFeature   Selection \\xa0\\nThere  may  be  quite  a  few  potential  predictors  for  the  model,  but  choosing  the  correct  features  (not \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nthe  redundant  ones  but  the  ones  that  add  some  value  to  the  model)  is  quite  essential.  To  get  the \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\noptimal  model,  one  can  always  try  all  the  possible  combinations  of  independent  variables  and  check \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nwhich  model  ﬁts  the  best.  But  this  method  is  obviously  time-consuming  and  infeasible.  Hence,  one \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nneeds  some  other  method  to  get  a  decent  model.  Below  is  given  a  list  of  diﬀerent  approaches  for \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nchoosing   the   best   set   of   predictors   that   will   give   the   least   test   error. \\xa0\\xa0\\n\\xa0\\n1)Manual   Feature   Elimination   Approach \\xa0\\n\\xa0\\na)Build   the   model   with   all   the   features \\xa0\\nb)Drop   the   features   that   are   least   helpful   in   prediction   (high   p-value) \\xa0\\nc)Drop   features   that   are   redundant   (using   correlations,   VIF) \\xa0\\nd)Rebuild   the   model \\xa0\\ne)Repeat   the   above   steps   till   a   good   model   is   reached \\xa0\\n\\xa0\\n2)Automated   Feature   Elimination   Approach \\xa0\\n\\xa0\\na)Recursive   Feature   Elimination   (RFE) \\xa0\\nb)Best   Subset   Selection   method \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   20 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 19}),\n",
              " Document(page_content='c)Forward/Backward   Stepwise   Selection   method \\xa0\\nd)Lasso   Regularization \\xa0\\n\\xa0\\n3)Balanced   Feature   Elimination   Approach \\xa0\\n\\xa0\\na)Use  automated  feature  elimination  for  the  coarse  tuning  (i.e.  remove  most  of  the \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nfeatures  to  drastically  bring  down  the  number  of  features  to  a  minimum  number  of \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\npotential   variables) \\xa0\\nb)Use  manual  feature  elimination  for  ﬁne  tuning  on  the  remaining  small  set  of  potential \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nvariables \\xa0\\nAutomated   Feature   Selection \\xa0\\nThe   following   table   provides   a   comparison   of   the   feature   selection   algorithms. \\xa0\\n\\xa0\\nBest   Subset \\xa0 Forward   Stepwise   Subset \\xa0 Backward   Stepwise   Subset \\xa0\\n\\xa0\\n \\xa0\\n \\xa0\\nTotal   number   of   models   to   be \\xa0\\nbuilt   is   2p\\xa0Total   number   of   models   to   be \\xa0\\nbuilt   is   1+2p( p+1)\\xa0Total   number   of   models   to   be \\xa0\\nbuilt   is   1+2p( p+1)\\xa0\\nIt   cannot   be   used   when   0 p>4\\xa0\\nas   it    becomes   computationally \\xa0\\ninfeasible. \\xa0It   can   be   applied   even   when \\xa0\\n  where     is   the   number   of n< p n \\xa0\\nobservations. \\xa0It   cannot   be   applied   when   n< p\\n, where     is   the   number n \\xa0\\nof   observations,   as   a   full   model \\xa0\\ncannot   be   ﬁt. \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   21 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 20}),\n",
              " Document(page_content=\"Model   Assessment \\xa0\\nWhile  creating  the  best  model  for  any  problem  statement,  one  ends  up  choosing  from  a  set  of \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\nmodels  which  gives  the  least  test  error.  Hence,  the  test  error,  and  not  only  the  training  error,  needs \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nto  be  estimated  in  order  to  select  the  best  model.  Besides,  selecting  the  best  model  to  obtain \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ndecent  predictions,  one  also  needs  to  maintain  a  balance  between  keeping  the  model  simple  and \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nexplaining   the   highest   variance   (i.e.   keeping   as   many   required   variables   as   possible). \\xa0\\xa0\\n\\xa0\\nR-squared  is  never  used  for  comparing  the  models  as  the  value  of  R2  increases  with  the  increase  in \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe  number  of  predictors  (even  if  these  predictors  did  not  add  any  value  to  the  model).  Thus,  in \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\ncame  the  idea  to  penalize  the  model  for  every  predictor  variable  (if  the  variable  did  not  improve  the \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nmodel  more  than  would  be  expected  by  chance)  being  added  to  the  model.  This  can  be  done  in  the \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nfollowing   two   ways. \\xa0\\n\\xa0\\n1)Use  metrics  which  take  into  account  both  model  ﬁt  and  simplicity,  such  metrics  are  Mallow's \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nCp,   Adjusted   R2,   AIC   and   BIC. \\xa0\\n2)Estimate   the   test   error   via   a   validation   set   or   a   cross-validation   approach. \\xa0\\n\\xa0\\nFor   a   model   where, \\xa0\\n  is   the   number   of   training   examples, n \\xa0\\n  is   the   number   of   predictors, d \\xa0\\nis   the   estimate   of   the   variance   of   training   error, σ2\\xa0\\n  is   the   entire   dataset, D \\xa0\\xa0\\n  is   the   model, M \\xa0\\nis   the   model   likelihood, n P( D ∣ M) l \\xa0\\n\\xa0\\nS S E S S R S S  T   (∑n\\ni=1( y  )i− y2)=   (∑n\\ni=1( y  )i ︿− y2)+   (∑n\\ni=1( y  )i︿− yi2)\\xa0\\nMallow’s   CP \\xa0\\n\\xa0\\nAIC   (Akaike   Information   Criterion) \\xa0\\n\\xa0\\nBIC   (Bayesian   Information   Criterion) \\xa0\\nIt   is   valid   for   sample   size   much   larger   than   the   number   of   parameters   in   the   model. \\xa0\\n\\xa0\\n\\xa0\\nAdjusted   R2 \\xa0\\nIt  can  have  a  negative  value,  if  the  predictors  do  not  explain  the  dependent  variables  at  all  such  that \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\n. S S S S R ~ T \\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   22 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 21}),\n",
              " Document(page_content=\"Python   Code   -   Multiple   Linear   Regression \\xa0\\nVisualize   Data  \\n>>   sns.pairplot(df)  \\n>>   sns.boxplot(x= 'column' ,   y= 'column1' ,   hue= 'column2' ,   data=df)  \\n \\nPrepare   Data  \\n>>    def     binary_map (x):  \\n         return    x.map({ 'yes' :   1,    'no' :   0})  \\n \\n>>   df[column_list]   =   df[column_list].apply(binary_map)  \\n#   Handling   the   categorical   variables   using   custom   binary   encoding  \\n>>    from    sklearn.preprocessing    import    LabelEncoder  \\n>>   label_encoder   =   LabelEncoder()  \\n>>   df[column_list]   =   label_encoder.fit_transform(df[column_list])  \\n#   Handling   of   ordinal   categorical   variables   using   label   encoding  \\n>>   dummy_df   =   pd.get_dummies(df[column_list],   drop_first= True )  \\n>>   df   =   pd.concat([df,   dummy_df],   axis=1)  \\n>>   df.drop([column_list],   axis=1,   inplace= True )  \\n#   Handling   the   nominal   categorical   variables   using   dummy   method  \\n>>    from    sklearn.preprocessing    import    OneHotEncoder  \\n>>   one_hot_encoder   =   OneHotEncoder(handle_unknown= 'ignore' )  \\n>>   one_hot_encoder.fit_transform(df)  \\n#   Handling   the   nominal   categorical   variables   using   one   hot   encoding.   It   gives   a  \\nsparse   matrix   as   output  \\n \\nCreate   Train   &   Test   Data  \\n>>    from    sklearn.model_selection    import    train_test_split  \\n>>   X_train,X_test,y_train,y_test   =   train_test_split(X,   y,   train_size=0.7,  \\n                                    test_size=0.3,   random_state=100)  \\n \\nScaling   Features  \\n>>    from    sklearn.preprocessing    import    StandardScaler  \\n>>   scaler   =   StandardScaler()  \\n>>   X_train_scaled[column_list]   =   scaler.fit_transform(X_train[column_list])  \\n>>   X_test_scaled[column_list]   =   scaler.transform(X_test[column_list])  \\n \\nTrain   Model  \\n>>    from    sklearn.linear_model    import    LinearRegression  \\n>>   model   =   LinearRegression()  \\n>>   model.fit(X_train,   y_train)  \\n \\nAutomated   Feature   Selection   (RFE)  \\n>>    from    sklearn.feature_selection    import    RFE  \\n>>   rfe   =   RFE(model,   no_of_features_to_be_selected)  \\n>>   rfe   =   rfe.fit(X_train,   y_train)  \\n>>   list(zip(X_train.columns,rfe.support_,rfe.ranking_))  \\n>>   selected_features   =   X_train.columns[rfe.support_]  \\n>>   insignificant_features   =   X_train.columns[~rfe.support_]   \\n \\nAnalyze   Model   using   Statsmodels  \\n>>    import    statsmodels.api    as    sm  \\n>>   X_train_rfe   =   X_train[selected_features]  \\n>>   X_train_rfe   =   sm.add_constant(X_train_rfe)  \\n#   In   statsmodels   intercept   variable   needs   to   be   added   explicitly  \\n>>   model_1   =   sm.OLS(y_train,X_train_rfe).fit()  \\n>>   lm.summary()  \\n \\nChecking   VIF  \\n>>    from    statsmodels.stats.outliers_influence    import    variance_inflation_factor  \\n>>   vif   =   pd.DataFrame()  \\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   23 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 22}),\n",
              " Document(page_content=\">>   vif[ 'VIF' ]   =   [variance_inflation_factor(X_train_rfe.values,   i)  \\n                  for    i    in    range(X_train_rfe.shape[1])]  \\n>>   vif[ 'VIF' ]   =   round(vif[ 'VIF' ],   2)  \\n>>   vif[ 'Features' ]   =   X_train_rfe.columns  \\n>>   vif   =   vif.sort_values(by= 'VIF' ,   ascending= False )  \\n \\nManual   Feature   Selection  \\n#   Remove   only   one   feature   at   a   time  \\n#   Remove   the   feature   with   p-value   >   0.05  \\n#   Remove   the   feature   with   VIF   >   5  \\n>>   selected_features.remove(feature_to_be_removed)  \\n>>   X_train_manual   =   X_train[selected_features]  \\n>>   X_train_manual   =   sm.add_constant(X_train_manual)  \\n#   Repeat   steps   Analyze   Model   using   Statsmodels,   Checking   VIF   and   Manual   Feature  \\nSelection   till   all   the   features   are   within   proper   parameters   and   the   model  \\nperformance   metrics   are   acceptable  \\n \\nTrain   Final   Model  \\n>>   model_final   =   LinearRegression()  \\n>>   model_final.fit(X_train_manual,   y_train)  \\n \\n \\nAnalyze   Residuals  \\n>>   y_train_pred   =   model_final.predict(X_train_manual)  \\n>>   residual   =   (y_train   -   y_train_pred)  \\n>>   sns.distplot(residual,   bins   =   15)  \\n#   Checking   if   residuals   are   normally   distributed  \\n>>   plt.scatter(X_train_manual,   residual)  \\n#   Checking   for   independence   of   residuals  \\n \\nPredict  \\n>>   X_test   =   X_test[selected_features]  \\n>>   X_test   =   sm.add_constant(X_test)  \\n>>   y_pred   =   model_final.predict(X_test)  \\n \\nEvaluate   Model  \\n>>    from    sklearn.metrics    import    mean_squared_error  \\n>>    from    sklearn.metrics    import    r2_score  \\n>>   rsme   =   np.sqrt(mean_squared_error(y_test,   y_pred))  \\n>>   r_squared   =   r2_score(y_test,   y_pred)  \\n \\n3.1.3.   INDUSTRY   RELEVANCE   OF   LINEAR   REGRESSION \\xa0\\nSome   of   the   important   properties   of   Linear   Regression   are   given   below, \\xa0\\n\\xa0\\n1)In  statistical  modelling,  linear  regression  is  a  process  of  estimating  the  relationships  among \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nvariables.  The  focus  here  is  to  establish  the  relationship  between  a  dependent  variable  and \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\none   or   more   independent   variable(s). \\xa0\\n\\xa0\\n2)Regression  explains  the  change  in  value  of  dependent  variable  with  the  change  in  values  of \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\none   predictor,   holding   the   other   predictors   static. \\xa0\\n\\xa0\\n3)Regression   only   shows   relationship,   i.e.   correlation   and   not   causality. \\xa0\\xa0\\n4)Regression  analysis  is  widely  used  for  Forecasting  and  Prediction.  It  does  guarantee \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ninterpolation  (predict  value  within  the  range  of  data  used  for  model  building)  but  not \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nnecessarily   extrapolation. \\xa0\\n\\xa0\\n5)Linear   regression   is   a   form   of   parametric   regression. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   24 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 23}),\n",
              " Document(page_content='Prediction   vs   Projection \\xa0\\nAlthough  prediction  and  projection  sound  synonymous  but  they  are  diﬀerent  applications  in \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nanalytics. \\xa0\\n\\xa0\\nPrediction \\xa0 Projection/Forecast \\xa0\\nIdentiﬁcation  of  predictor  variables  and \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmeasurement   of   their   impact   is   the   main   aim. \\xa0Finding  the  ﬁnal  projected  result  or  forecasted \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nvalue   is   the   main   aim. \\xa0\\nNo  new  assumptions  are  made  other  than  those \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nof   linear   regression. \\xa0Forecasts  for  the  next  day  are  made  with  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nassumption  that  everything  remains  the  same \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ntoday.  Inclusion  of  any  new  incidents  changes \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nthe   forecast   for   the   next   day. \\xa0\\nSimplicity   of   the   model   is   important. \\xa0 Accuracy  of  the  model  is  important  and  not  the \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nmodel. \\xa0\\n\\xa0\\nInterpolation   vs   Extrapolation \\xa0\\nThe   basic   diﬀerence   between   interpolation   and   extrapolation   is   given   in   the   following   table, \\xa0\\n\\xa0\\nInterpolation \\xa0\\xa0 Extrapolation \\xa0\\nModel  is  used  to  predict  the  value  of  a \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\ndependent  variable  on  independent  values  that \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nlie   within   the   range   of   data   one   already   has. \\xa0Model  is  used  to  predict  the  dependent  variable \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\non  the  independent  values  that  lie  outside  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nrange   of   the   data   the   model   was   built   on. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nIn  the  preceding  ﬁgure  when  one  predicts  Y1  for  X1,  which  lies  between  a  and  b,  it  is  called \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\ninterpolation.  On  the  other  hand,  extrapolation  would  be  extending  the  line  to  predict  Y2  for  X2 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\nwhich  lies  outside  the  range  on  which  the  linear  model  was  trained.  Also  one  can  easily  visualize \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nthat  for  the  models  P1  and  P2  which  were  trained  on  values  of  X  lying  within  P1  and  P2  respectively, \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\nupon   extrapolation   will   give   incorrect   predictions. \\xa0\\nParametric   vs   Non-Parametric \\xa0\\nThe   basic   diﬀerence   between   parametric   and   non-parametric   is   given   in   the   following   table, \\xa0\\n\\xa0\\nParametric \\xa0\\xa0 Non-Parametric \\xa0\\nThe  number  of  parameters  is  ﬁxed  with  respect \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nto  the  sample  size,  simplifying  the  function  to  a \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nknown   form. \\xa0The  number  of  parameters  can  grow  with  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nsample  size,  freeing  it  to  learn  any  functional \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nform   from   the   training   data. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   25 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 24}),\n",
              " Document(page_content='Simpler  and  faster  to  train  requiring  less  training \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ndata. \\xa0Flexible  and  highly  accurate,  but  slower  to  train \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nrequiring   a   lot   of   training   data. \\xa0\\nEg:  Simple  Neural  Networks,  Naive  Bayes, \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nLogistic   Regression,   etc. \\xa0Eg:  k-Nearest  Neighbors,  Decision  Trees  like \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nCART   and   C4.5,   SVM.   etc. \\xa0\\n\\xa0\\nConstrained   Minimisation   vs   Unconstrained   Minimisation \\xa0\\nThe  basic  diﬀerence  between  constrained  minimisation  and  unconstrained  minimisation  is  given  in \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe   following   table, \\xa0\\n\\xa0\\nConstrained   Minimisation \\xa0   Unconstrained   Minimisation \\xa0\\nIt  is  the  process  of  optimizing  a  cost  function \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nwith  respect  to  some  variables  in  the  presence \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nof  constraints  on  those  variables.  The \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nconstraints  can  be  either  a  hard  (needs  to \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nsatisfy)  or  a  soft  (penalties  for  not  satisfying) \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nconstraint. \\xa0It  is  the  process  of  optimizing  a  cost  function \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nwith  respect  to  some  variables  without  any \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nconstraints   on   those   variables. \\xa0\\nEg:   Ridge/Lasso   Regression,   SVM   etc. \\xa0 Eg:   Linear/Logistic   Regression   etc. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   26 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 25}),\n",
              " Document(page_content='3.2.   ADVANCED   REGRESSION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nADVANCED   REGRESSION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   27 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 26}),\n",
              " Document(page_content='3.2.1.   GENERALIZED   LINEAR   REGRESSION \\xa0\\nIn  linear  regression  problems  the  dependent  variable  is  always  linearly  related  to  the  predictor \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nvariables.  But  when  this  is  not  the  case  (i.e  the  relationship  is  not  linear),  the  Generalised  Regression \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nis   used   to   tackle   the   problem. \\xa0\\nGeneral   Equation \\xa0\\n( x) a  f( x)  f( x) . . . . .  f( x) F = 0+ a11+ a22+. + ak k\\xa0\\nFeature   Engineering \\xa0\\nRaw  attributes,  as  these  appear  in  the  training  data,  may  not  be  the  ones  best  suited  to  predict  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nresponse  variable  values.  Thus,  in  comes  the  derived  features,  which  is  a  combination  of  two  or \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nmore  raw  attributes  and/or  transformations  of  individual  raw  attributes.  These  combinations  and \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ntransformations   could   be   linear   (only   multiplication   by   a   constant   or   addition)   or   nonlinear. \\xa0\\nLinear   and   Non-Linear   Model \\xa0\\nThe  term  linear  in  linear  regression  refers  to  the  linearity  in  the  coeﬃcients,  i.e.  the  target  variable  y \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nis  linearly  related  to  the  model  coeﬃcients.  It  does  not  require  that  y  should  be  linearly  related  to  the \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nraw  attributes  or  features  i.e  a  model  is  linear  if  y  is  linearly  (only  multiplication  by  a  constant  or \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\naddition)   related   to   the   coeﬃcients   whereas   the   feature   functions   could   be   non-linear. \\xa0\\n\\xa0\\nData   points   :   , x, x . . . . . , x x1 2 3  d\\xa0\\n\\xa0\\nFeatures   :   ( x),Φ( x),Φ( x) . . . . . .,( x) Φ1  2  3 Φk\\xa0\\n\\xa0\\nModel   :   .   Also   represented   as   ,   where, ( x) ( x) . . . . . aΦ( x) y= a0+ aΦ11+ aΦ22+.k k a y= X \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSolution   :   On   minimizing   the   cost   function, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe   ﬁnal   solution   is   given   by   X y a=( X X)T1− T\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   28 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 27}),\n",
              " Document(page_content='Feature   Matrix \\xa0\\nBelow   matrix   provides   the   feature   matrix. \\xa0\\n\\xa0\\n\\xa0\\nPython   Code   -   Generalized   Regression \\xa0\\n>>    from    sklearn.preprocessing    import    PolynomialFeatures  \\n>>    from    sklearn.linear_model    import    LinearRegression  \\n>>    from    sklearn.pipeline    import    make_pipeline  \\n>>    from    sklearn    import    metrics  \\n>>   pipeline   =   make_pipeline(PolynomialFeatures(degree),   LinearRegression())  \\n>>   pipeline.fit(X_train,   y_train)  \\n>>   y_pred=pipeline.predict(X_test)  \\n>>   metrics.r2_score(y_test,y_pred)  \\n\\xa0\\n3.2.2.   REGULARIZED   REGRESSION \\xa0\\nThe  predictive  models  need  to  be  as  simple  as  possible,  but  no  simpler  as  there  is  an  important \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nrelationship  between  the  complexity  of  a  model  and  its  usefulness  in  a  learning  context.  The  simpler \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmodels  are  usually  more  generic,  widely  applicable  and  require  fewer  training  samples  for  eﬀective \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ntraining  in  comparison  to  the  more  complex  models.  Thus,  the  Regularization  process  is  used  to \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ncreate  an  optimally  complex  model,  i.e.  a  model  which  is  as  simple  as  possible  while  performing  well \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\non  the  training  data.  Through  regularization,  one  tries  to  strike  the  delicate  balance  between \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nkeeping   the   model   simple,   yet   not   making   it   too   naive   to   be   of   any   use. \\xa0\\nContour \\xa0\\n\\xa0\\n\\xa0\\nFor  any  function  when  a  third  axis  say  is  used  to  plot  the  output  values  of  the  function  and \\xa0\\xa0 \\xa0( x,) f y\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 z\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nthen  these  values  are  connected,  we  get  a  surface.  Now  on  slicing  this  surface  by  a  plane  and \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\njoining   all   the   points   where   the   function   value   is   same   gives   us   a   contour. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   29 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 28}),\n",
              " Document(page_content='Regularization   Regression \\xa0\\nIn  regularization  regression,  an  additional  regularization  term  is  added  to  the  cost  function  along \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nwith   the   error   term,   while   minimizing   the   cost   function   as   shown   below. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nWhere   Regularization   terms   can   be   as   below, \\xa0\\n\\xa0\\n1)Ridge   Regression   (Sum   of   squares   of   coeﬃcients)   :    Σαλ 2\\xa0\\xa0\\n2)Lasso   Regression   (Sum   of   absolute   values   of   coeﬃcients)   :    Σ ∣α ∣ λ \\xa0\\n\\xa0\\nThe   ﬁnal   solution   changes   to, \\xa0\\n\\xa0\\n\\xa0\\nRidge   &   Lasso   Regularization \\xa0\\nRidge  regression  almost  always  has  a  matrix  representation  for  the  solution  while  Lasso  regression \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nrequires  iterations  to  get  to  the  ﬁnal  solution.  Thus,  making  Lasso  regression  computationally  more \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nintensive. \\xa0\\xa0\\n\\xa0\\nThe  cost  function  for  both  ridge  and  lasso  can  also  be  represented  as  where, \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0(ω x ) R(ω) ΣT\\ni− yi2+λ \\xa0 \\xa0\\n⍵  is  the  model  parameters  (coeﬃcients)  and  λ  is  the  regularisation  hyperparameter  and  R( ⍵)  in  case \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nof   ridge   is     while   for   lasso   is   .  ωΣi2 ∣ω ∣ Σi \\xa0\\xa0\\n\\xa0\\nNow  if is  the  best  model  that  one  ends  up  getting,  which  is  given  as , \\xa0\\xa0θ*\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 r g m i n[ E(θ) R(θ)] θ*= a +λ \\xa0\\nthen  the  lasso  regression  results  in  a  sparse  solution  (i.e  many  of  the  model  coeﬃcients \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nautomatically  become  exactly  or ).  The  sparsity  increases  with  the  increase  in  λ,  where \\xa0 \\xa0 \\xa00\\xa0\\xa0ωi=0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nsparsity   of   a   model   is   deﬁned   by   the   number   of   parameters   in     that   are   exactly   equal   to   . θ* 0\\xa0\\n\\xa0\\nIt  is  already  known  that  the  error  term  with  a  regularised  term  is  given  as .  Now,  if  an \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0(θ) R(θ) E +λ \\xa0 \\xa0\\xa0\\xa0\\noptimal  solution  is  obtained  using with and  another  using with ,  where ,  one  can \\xa0 \\xa0\\xa0 \\xa0 \\xa0λ1 \\xa0θ1 \\xa0 \\xa0 \\xa0λ2 \\xa0θ2\\xa0 \\xa0λ1>λ2\\xa0 \\xa0\\xa0\\nconclude, \\xa0\\n(θ) R(θ) E(θ) R(θ)   a s θ i s t h e o p t i m a l s o l u t i o n f o r λ E1+λ1 1≤ 2+λ1 2 1 1\\xa0\\nand \\xa0\\n(θ) R(θ) E(θ) R(θ)   a s θ i s t h e o p t i m a l s o l u t i o n f o r λ E2+λ2 2≤ 1+λ2 1 2 2\\xa0\\n\\xa0\\nOn   solving   these   two   equations   it   can   be   concluded     and   (θ) (θ) E1≥ E2 (θ) (θ) R1≤ R2\\xa0\\n\\xa0\\nThus,  one  can  infer  that  with  the  increase  in  the  value  of  λ,  the  error  term  increases  and  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nregularization  term  decreases  and  vice-versa.  It  suggests  that  higher  values  of  λ  does  not  allow  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nmodel  to  become  complex  by  clamping  it  down  forcefully  whereas,  lower  values  of  λ  ( )  allows \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0→0\\xa0 \\xa0\\nthe  model  to  bring  the  error  down  irrespective  of  the  complexity  of  the  model.  Brieﬂy,  L1  and  L2 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nregularisation,  also  called  Lasso  and  Ridge  respectively,  use  the  L-1  norm  and  L-2  norm \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 Σ ∣ω ∣) (i \\xa0 \\xa0\\xa0 \\xa0\\n  respectively   as   the   penalty   terms.  Σ ω ) (i2\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   30 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 29}),\n",
              " Document(page_content=\"Graphical   representation   of   Ridge   &   Lasso   Regularization \\xa0\\nThe  following  ﬁgure  is  a  schematic  illustration  of  the  error  (red  lines)  and  regularisation  terms  (green \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\ndotted  curves  for  ridge  regression  and  solid  blue  lines  for  lasso  regression)  contours.  The  two  axes \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nrepresent     and     respectively. α1 α2 \\xa0\\n\\xa0\\n\\xa0\\nAt  every  crossing  one  could  move  along  the  arrow  (black  arrow  in  the  zoomed  image)  shown  to \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nkeep  the  error  term  same  and  reduce  the  regularisation  term,  giving  a  better  solution.  Thus,  for  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\noptimum  solution  for  𝛼  (sum  of  the  error  and  regularisation  terms  is  minimum),  the  corresponding \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nregularisation   contour   and   the   error   contour   must   touch   each   other   tangentially   and   not   cross. \\xa0\\xa0\\n\\xa0\\nThe  blue  stars  highlight  the  touch  points  between  the  error  contours  and  the  lasso  regularisation \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ncontours.  The  green  stars  highlight  the  touch  points  between  the  error  contours  and  the  ridge \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nregularisation  terms.  The  picture  illustrates  the  fact  that  because  of  the  corners  in  the  lasso  contours \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\n(unlike  ridge  regression),  the  touch  points  are  more  likely  to  be  on  one  or  more  of  the  axes.  This \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nimplies  that  the  other  coeﬃcients  become  zero.  Hence,  lasso  regression  also  serves  as  a  feature \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nselection   method,   whereas   ridge   regression   does   not. \\xa0\\xa0\\nPython   Code   -   Regularized   Regression \\xa0\\n>>    from    sklearn.preprocessing    import    Ridge  \\n>>   hyper_params   =   { 'alpha' :   [parameters]}  \\n>>   ridge   =   Ridge()  \\n>>   model_cv   =   GridSearchCV(estimator=ridge,   param_grid=hyper_params,   cv=folds,   \\n                            scoring= 'neg_mean_absolute_error' ,   verbose=1,  \\n                            return_train_score=True) \\xa0\\n>>   model_cv.ﬁt(X_train,   y_train) \\xa0\\n>>   cv_results   =   pd.DataFrame(model_cv.cv_results_) \\xa0\\n>>   plt.plot(cv_results['param_alpha'],   cv_results['mean_train_score']) \\xa0\\n>>   plt.plot(cv_results['param_alpha'],   cv_results['mean_test_score']) \\xa0\\n>>   ridge   =   Ridge(alpha=alpha_value_selected) \\xa0\\n>>   ridge.ﬁt(X_train,   y_train) \\xa0\\n>>   ridge.coef_ \\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   31 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 30}),\n",
              " Document(page_content='3.3.   PRINCIPLES   OF   MODEL   SELECTION \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nPRINCIPLES   OF   MODEL \\xa0\\nSELECTION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   32 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 31}),\n",
              " Document(page_content='3.3.1.   PRINCIPLES   OF   MODEL   SELECTION \\xa0\\nThere  have  always  been  situations  where  a  model  performs  well  on  training  data  but  not  on  the  test \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\ndata.  Add  to  it  the  confusion  about  which  model  to  choose  for  a  given  problem.  Problems  like  these \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nfrequently  arise  irrespective  of  the  choice  of  model,  data  or  the  problem  itself.  Some  thumb  rules \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nand   general   pointers   on   selecting   the   appropriate   models   are   discussed   below. \\xa0\\nOccam’s   Razor \\xa0\\nIt   states   when   in   dilemma   choose   the   simpler   model. \\xa0\\xa0\\n\\xa0\\nA   simpler   model   can   be   any   of   these. \\xa0\\n1)Number  of  parameters  required  to  specify  the  model  completely.  The  model  is \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 x y= a1+ b x2\\xa0\\xa0\\nsimpler   than   the   model   . x y= a1+ b x2+ c x3\\xa0\\n2)The   degree   of   the   function,   if   it   is   a   polynomial. \\xa0\\n3)Size  of  the  best-possible  representation  of  the  model.  The  expression \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n  could   be   considered   to   be   more   complex   than   . .552984567 x 32.4710001276 0 +9 x 0 2+1\\xa0\\n4)The   depth   or   size   of   a   decision   tree. \\xa0\\xa0\\n\\xa0\\nAdvantages   of   simpler   models. \\xa0\\n1)Simpler  models  are  usually  more  generic  and  are  more  widely  applicable.  One  who \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nunderstands  a  few  basic  principles  of  a  subject  (simple  model)  well,  is  better  equipped  to \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nsolve  any  new  unfamiliar  problem  than  someone  who  has  memorized  an  entire  set  of \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nspeciﬁc   problems   (complex   model). \\xa0\\n2)Simpler  models  require  fewer  training  samples  for  eﬀective  training  than  the  more  complex \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nones   and   are   consequently   easier   to   train. \\xa0\\n3)Simpler  models  are  more  robust  as  these  are  not  as  sensitive  to  the  speciﬁcs  of  the  training \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\ndata   set   in   comparison   to   their   more   complex   counterparts   are. \\xa0\\n4)Simpler  models  make  more  errors  in  the  training  set  but  give  better  results  on  test  samples. \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nWhereas  complex  models  lead  to  overﬁtting  and  work  very  well  for  the  training  samples,  but \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nfail   miserably   when   applied   to   test   samples. \\xa0\\nBias-Variance   Tradeoﬀ \\xa0\\nThe   following   ﬁgure   represents   the   Bias-Variance   tradeoﬀ. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nBias  :  It  quantiﬁes  how  accurate  is  the  model  likely  to  be  on  future  (test)  data.  Complex  models, \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nassuming  there  is  enough  training  data  available,  can  do  a  very  accurate  job  of  prediction.  Models \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nthat   are   too   naive,   are   very   likely   to   do   badly. \\xa0\\xa0\\n\\xa0\\nVariance  :  It  refers  to  the  degree  of  changes  in  the  model  itself  with  respect  to  changes  in  the \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\ntraining   data   or   how   sensitive   is   a   model   to   the   changes   in   the   training   data. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   33 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 32}),\n",
              " Document(page_content='\\xa0\\nAs  the  complexity  increases,  bias  reduces  and  variance  increases.  The  aim  is  to  ﬁnd  the  optimal \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\nmodel. \\xa0\\xa0\\nOverﬁtting \\xa0\\nA  model  memorizing  the  data  rather  than  intelligently  learning  the  underlying  trends  in  it  is  called \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\noverﬁtting.  It  happens  because  it  is  possible  for  the  model  to  memorize  data,  which  is  a  problem  as \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nthe  real  test  happens  on  unseen  real  world  data.  In  the  following  ﬁgure  the  higher  degree \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\npolynomial   ﬁt   (red   line)   shows   an   overﬁt   model. \\xa0\\n\\xa0\\n\\xa0\\nRegularization \\xa0\\nIt  is  the  process  of  deliberately  simplifying  models  to  achieve  the  correct  balance  between  keeping \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nthe   model   simple   and   yet   not   too   naive. \\xa0\\nMachine   Learning   Scenario \\xa0\\nThe   following   ﬁgure   shows   a   typical   Machine   Learning   scenario. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThere  is  a  data  source  and  an  underlying  system  that  needs  to  be  replicated.  For  example  the  data \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nsource  could  be  an  email  repository.  The  system  is  a  human,  who  is  trying  to  segregate  spam  emails \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nfrom  the  rest.  There  is  a  Learning  Algorithm  that  takes  samples  from  the  data  source  (this  will \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nconstitute  the  training  data)  and  the  expected  responses  from  the  system  and  comes  up  with  a \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nmodel  that  behaves  a  lot  like  the  system  being  replicated.  The  inputs  are  denoted  by  the  vector , \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 x\\xa0\\nthe  expected  system  output  as  and  model  output  as .  The  learning  algorithm  works  with  a \\xa0 \\xa0 \\xa0 \\xa0\\xa0 y\\xa0 \\xa0 \\xa0 \\xa0\\xa0 y ′\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   34 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 33}),\n",
              " Document(page_content='hypothesis  class  (each  learning  algorithm  searches  for  the  best  possible  model  from  a  ﬁxed  class \\xa0 \\xa0 K\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nof  models  speciﬁc  to  the  algorithm).  The  output  of  the  model  is  a  speciﬁc  hypothesis  or  model. \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nHyperparameters   govern   the   way   the   learning   algorithm   produces   the   model. \\xa0\\n3.3.2.   MODEL   EVALUATION \\xa0\\nOnce  the  class  of  models  to  be  used  has  been  decided,  one  needs  to  evaluate  it.  Below  are  some \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nof   the   evaluation   strategies   for   situations   when   there   is   abundant   or   limited   (or   little)    training   data. \\xa0\\xa0\\nHyperparameters \\xa0\\nThese  are  the  parameters  that  the  algorithm  designer  passes  to  the  learning  algorithm  in  order  to \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ncontrol  the  complexity  of  the  ﬁnal  model.  This  in  term  ﬁne  tunes  the  behaviour  of  the  learning \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nalgorithm  and  has  a  lot  of  bearing  on  the  ﬁnal  model  produced.  In  short  it  governs  the  behaviour  of \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe   algorithm. \\xa0\\nData   Classiﬁcation \\xa0\\nThe   data   is   divided   into   three   parts   as   below \\xa0\\n1)Training   data   :    data   on   which   model   trains. \\xa0\\n2)Validation  data  :  data  on  which  model  is  evaluated  for  various  hyperparameters  (prevents \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\npeeking   of   test   data). \\xa0\\n3)Test   data   :   data   on   which   model   predicts. \\xa0\\nK-Fold   Cross   Validation \\xa0\\nWhen  the  data  is  limited,  the  CV  is  used.  In  the  basic  approach,  called  k-fold  CV,  the  training  set  is \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nsplit   into   k   smaller   sets.   Below   steps   are   followed   for   each   of   the   k   folds. \\xa0\\n1)A   model   is   trained   using   of   the   folds   as   training   data. k−1 \\xa0\\n2)The   resulting   model   is   validated   on   the   remaining   part   of   the   data. \\xa0\\n\\xa0\\nThe  performance  measure  reported  by  k-fold  cross-validation  is  then  the  average  of  the  values \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\ncomputed   in   the   loop.   The   model   with   the   best   performance   is   then   selected. \\xa0\\n\\xa0\\n\\xa0\\nGrid   Search   Cross-Validation \\xa0\\nHyperparameter  tuning  is  one  of  the  most  essential  tasks  in  the  model  building  process.  For  a \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nmodel  to  perform  best  on  the  data,  one  needs  to  tune  the  hyperparameter(s).  For  example  in  the \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\ncase  of  linear  regression,  the  number  of  features  is  a  hyperparameter  that  can  be  optimised.  One \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\ncan  use  the  Grid  Search  Cross  Validation  to  tune  the  hyperparameter(s)  in  python.  The  grid  is  a \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   35 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 34}),\n",
              " Document(page_content=\"matrix  which  is  populated  on  each  iteration.  The  model  is  trained  using  a  diﬀerent  value  of  the \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nhyperparameter  each  time.  The  estimated  score  is  then  populated  in  the  grid  shown  in  the  following \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nﬁgure. \\xa0\\n\\xa0\\n\\xa0\\nLeave   One   Out   (LOO)   Cross   Validation \\xa0\\nThis  strategy  takes  each  data  point  as  the  test  sample  once  and  trains  the  model  on  the  rest \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 n−1\\ndata  points.  Thus,  it  trains  on total  models.  It  utilizes  the  data  well  since  each  model  is  trained  on \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 n \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nsamples,   but   it   is   computationally   expensive. n−1 \\xa0\\nLeave   P-Out   (LPO)   Cross   Validation \\xa0\\nThis  strategy  creates  all  possible  splits  after  leaving  P  samples  out.  For  n  data  points  there  are \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 C p n\\npossible   train   test   splits. \\xa0\\nStratiﬁed   K-Fold   Cross   Validation \\xa0\\nThis  strategy  is  used  for  classiﬁcation  problems.  It  ensures  that  the  relative  class  proportion  is \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\napproximately  preserved  in  each  train  and  validation  fold.  It  is  very  important  when  there  is  huge \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nclass   imbalance   (98%   to   2%). \\xa0\\nPython   Code   -   Cross   Validation \\xa0\\nMultiple   Polynomial   Features  \\n>>    from    sklearn.preprocessing    import    PolynomialFeatures  \\n>>    from    sklearn.linear_model    import    LinearRegression  \\n>>    from    sklearn.pipeline    import    make_pipeline  \\n>>   model   =   make_pipeline(PolynomialFeatures(degree),LinearRegression())  \\n#   Creates   feature   and   then   feeds   to   the   model  \\n>>   model.fit(X_train,   y_train)  \\n \\nK-Fold   CV  \\n>>    from    sklearn.model_selection    import    KFold  \\n>>    from    sklearn.model_selection    import    cross_val_score  \\n>>   lm   =   LinearRegression()  \\n>>   folds   =   KFold(n_splits=5,   shuffle= True ,   random_state=100)  \\n>>   scores   =   cross_val_score(lm,X_train,y_train,cv=folds,scoring= 'r2' )  \\n>>   scores   =   cross_val_score(lm,   X_train,   y_train,   cv=folds,   \\n                             scoring= 'mean_squared_error' )  \\n \\nGrid   Search   CV  \\n>>   hyper_params   =   [{ 'n_features_to_select' :   [parameters]}]  \\n>>   lm.fit(X_train,   y_train)  \\n>>   rfe   =   RFE(lm)  \\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   36 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 35}),\n",
              " Document(page_content=\">>    from    sklearn.model_selection    import    GridSearchCV  \\n>>   model_cv   =   GridSearchCV(estimator=rfe,   param_grid=hyper_params,   cv=folds,   \\n                            scoring= 'r2' ,   verbose=1,   return_train_score= True )  \\n>>   model_cv.fit(X_train,   y_train)  \\n>>   scores   =   pd.DataFrame(model_cv.cv_results_)  \\n \\nBootstrapping   Method \\xa0\\nIt  is  very  important  to  both  present  the  expected  skill  of  a  machine  learning  model  as  well  as  the \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nconﬁdence  intervals  for  that  model  skill.  Conﬁdence  intervals  provide  a  range  of  model  skills  and  a \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nlikelihood  that  the  model  skill  will  fall  between  the  ranges  when  making  predictions  on  new  data.  A \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nrobust  way  to  calculate  conﬁdence  intervals  for  machine  learning  algorithms  is  to  use  the  bootstrap \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\nmethod.  Bootstrap  refers  to  random  sampling  with  replacement.  This  is  a  general  technique  for \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nestimating  statistics  mean  and  standard  deviation  from  the  dataset  (such  as  R-squared,  Adjusted  R2, \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nAIC,  BIC,  etc.)  that  can  be  used  to  calculate  empirical  conﬁdence  intervals,  regardless  of  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ndistribution  of  skill  scores.  Calculation  of  conﬁdence  intervals  with  the  bootstrap  involves  the  below \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nsteps. \\xa0\\n\\xa0\\n1)Calculation   of   Population   of   Statistics   : \\xa0\\nThe  ﬁrst  step  is  to  use  the  bootstrap  procedure  to  resample  the  original  data  a  number  of \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ntimes  and  calculate  the  statistic  of  interest.  The  dataset  is  sampled  with  replacement  (i.e. \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\neach  time  an  item  is  selected  from  the  original  dataset,  it  is  not  removed,  allowing  that  item \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nto  possibly  be  selected  again  for  the  sample).  The  statistic  is  calculated  on  the  sample  and  is \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nstored  so  as  to  build  up  a  population  of  the  statistic  of  interest.  The  number  of  bootstrap \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nrepeats   deﬁnes   the   variance   of   the   estimate,   the   more   the   better. \\xa0\\n\\xa0\\n2)Calculation   of   Conﬁdence   Interval   : \\xa0\\nThe  second  step  is  to  calculate  the  conﬁdence  intervals.  This  is  done  by  ﬁrst  ordering  the \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\npopulation  of  statistics  and  then  selecting  the  values  at  the  chosen  percentile  for  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nconﬁdence  interval.  For  example,  if  a  conﬁdence  interval  of  95%  is  chosen,  then  for  the \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\ncalculated  1,000  statistics  from  an  ordered  1,000  bootstrap  samples,  the  lower  bound  would \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nbe  the  25th  value  and  the  upper  bound  would  be  the  975th  value.  Here  a  non-parametric \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nconﬁdence  interval  is  calculated  which  does  not  make  any  assumption  about  the  functional \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nform  of  the  distribution  of  the  statistic.  This  conﬁdence  interval  is  often  called  the  empirical \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nconﬁdence   interval. \\xa0\\n\\xa0\\n3)Bootstrap   Model   Performance   : \\xa0\\nThe  bootstrap  method  can  also  be  used  to  evaluate  the  performance  of  machine  learning \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nalgorithms  without  the  need  for  any  test/validation  sample.  Usually,  the  size  of  the  sample \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\n(training  sample)  taken  in  each  iteration  may  be  limited  to  60%  or  80%  of  the  available  data. \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nThis  means  that  there  will  always  be  some  observations  in  the  dataset  that  are  not  included \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\nin  the  training  sample.  These  are  called  the  out  of  bag  (OOB)  samples.  A  model  can  then  be \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\ntrained  on  the  data  sample  of  each  bootstrap  iteration  and  evaluated  on  the  out  of  bag \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\nsamples  of  that  iteration  to  give  a  performance  statistic.  These  performance  statistics  can  be \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\ncollected   and   the   conﬁdence   intervals   be   calculated. \\xa0\\n\\xa0\\nThe   following   ﬁgure   shows   a   graphical   representation   of   the   above   mentioned   steps. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   37 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 36}),\n",
              " Document(page_content=\"\\xa0\\nPython   Code   -   Bootstrapping \\xa0\\nConfigure   Bootstrap  \\n>>   n_iterations   =   no_of_iterations  \\n>>   n_size   =   int(len(df)   *   percentage_of_sample)  \\n \\nRun   Bootstrap  \\n>>    from    sklearn.utils    import    resample  \\n>>    from    sklearn.metrics    import    accuracy_score  \\n>>   stats   =   list()  \\n>>   values   =   df.values  \\n>>    for    i    in    range(n_iterations):  \\n>>       train   =   resample(values,   n_samples=n_size)  \\n>>       X_train   =   train[:,:-1]  \\n>>       y_train   =   train[:,-1]  \\n>>       test   =   numpy.array([x    for    x    in    values    if    x.tolist()    not     in    train.tolist()])  \\n>>       X_test   =   test[:,:-1]  \\n>>       y_test   =   test[:,-1]  \\n>>       model   =   any_machine_learning_model()  \\n>>       model.fit(X_train,   y_train)  \\n>>       predictions   =   model.predict(X_test)  \\n>>       score   =   accuracy_score(y_test,   predictions)  \\n>>       stats.append(score)  \\n \\nPlot   Stats  \\n>>    from    matplotlib    import    pyplot  \\n>>   pyplot.hist(stats)  \\n>>   pyplot.show()  \\n \\nFind   Confidence   Intervals  \\n>>   alpha   =   0.95  \\n>>   p   =   ((1.0-alpha)/2.0)   *   100  \\n>>   lower   =   max(0.0,   numpy.percentile(stats,   p))  \\n>>   p   =   (alpha+((1.0-alpha)/2.0))   *   100  \\n>>   upper   =   min(1.0,   numpy.percentile(stats,   p))  \\n>>   print( 'confidence   interval' ,   alpha*100,   lower*100,   upper*100)) \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   38 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 37}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n4.   CLASSIFICATION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   39 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 38}),\n",
              " Document(page_content='4.1.   LOGISTIC   REGRESSION \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nLOGISTIC   REGRESSION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   40 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 39}),\n",
              " Document(page_content=\"4.1.1.   LOGISTIC   REGRESSION \\xa0\\nLinear  regression  models  can  make  predictions  for  continuous  variables.  But  if  the  output  variable  is \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\ncategorical,  then  one  has  to  move  to  classiﬁcation  models.  Logistic  Regression  is  a  supervised \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nclassiﬁcation  model,  which  can  make  predictions  from  labelled  data  (i.e.  if  the  target  variable  is \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\ncategorical). \\xa0\\nBinary   Logistic   Regression \\xa0\\nIt  is  a  classiﬁcation  problem  in  which  the  target  variable  has  only  two  possible  values,  or  in  other \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nwords,  two  classes.  Some  examples  of  binary  classiﬁcation  being,  determining  whether  a  particular \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ncustomer  will  default  on  a  loan  or  not,  whether  an  email  is  spam  or  not,  etc.  Now  let's  consider  the \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ndiabetes  example  i.e  predicting  whether  a  person  has  diabetes  or  not,  based  on  that  person’s  blood \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nsugar  level.  As  can  be  seen  from  the  following  ﬁgure,  a  simple  boundary  decision  approach  is  able \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nto  diﬀerentiate  most  of  the  data  points  in  their  respective  classes.  But,  it  is  going  to  be  too  risky  to \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\ndecide  the  class  blatantly  on  the  basis  of  a  cutoﬀ,  as  especially  in  the  middle,  the  person  could \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nbasically  belong  to  any  class,  diabetic  or  non-diabetic.  So,  with  the  change  of  the  threshold  values \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nthe  number  of  misclassiﬁcations  also  keep  changing.  Thus,  with  this  kind  of  solution  there  is  a  higher \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nchance   of   misclassiﬁcation. \\xa0\\n\\xa0\\n\\xa0\\nSigmoid   Curve \\xa0\\nAs  a  simple  boundary  decision  method  is  not  going  to  work  for  the  diabetic  example,  one  needs  to \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ncome  up  with  some  other  method.  A  curve  having  extremely  low  values  in  the  start,  extremely  high \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nvalues  in  the  end  and  intermediate  values  in  the  middle  would  be  a  good  choice.  The  sigmoid  curve \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nhas  got  all  these  required  properties,  but  so  does  also  a  straight  line.  The  main  reason  for  not  using \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nthe  straight  line  is  that  it  is  not  steep  enough.  As  can  be  easily  seen  from  the  ﬁgure  above,  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nsigmoid  curve  has  low  values  for  a  lot  of  points,  then  the  values  rise  all  of  a  sudden  and  at  last  there \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\nare  a  lot  of  high  values.  Whereas,  for  a  straight  line,  the  values  rise  from  low  to  high  very  uniformly, \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nand   hence,   the   boundary   region   (where   the   probabilities   transition   from   high   to   low)   is   not   present. \\xa0\\xa0\\nBest   Fit   Line \\xa0\\nThe  best  ﬁt  line  is  a  sigmoid  curve  that  is  the  best  approximation  dividing  the  two  classes.  The \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nequation   for   the   best   ﬁt   line   is   given   by   the   sigmoid   curve   or   the   logit   equation, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   41 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 40}),\n",
              " Document(page_content='However,  it  is  not  the  only  equation  that  has  this  form,  there  is  also  the  probit  form  of  logistic \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nregression,   given   by, \\xa0\\n\\xa0\\n\\xa0\\nAlso,   there   is   the   cloglog   form   of   logistic   regression,   given   by, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nFor  the  diabetes  example,  the  best  ﬁtting  combination  of  and  will  be  the  one  which  will  have \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0β0\\xa0\\xa0β1\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nlower  probabilities  for  non-diabetic  persons  and  higher  probabilities  for  diabetic  persons.  This  can \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nbe   done   by   maximizing   the   product   of   all   the   probabilities,   i.e. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThis  product  of  probabilities  is  called  the  likelihood  function.  The  cost  function  can  be  derived  using \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nthe   likelihood   function. \\xa0\\nLikelihood   Function \\xa0\\nConsider  a  dataset  with  probability  density  function ,  then  the  likelihood  function \\xa0\\xa0 \\xa0 x,,... x) (1 x2. n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0( x,) f1θ\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nis  given  by  the  joint  density  of  the  dataset  which  by  independence  is  equal  to  the  product  of  the \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nmarginal   densities, \\xa0\\n\\xa0\\n\\xa0\\nCost   Function \\xa0\\nFor   a   discrete   function, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe   cost   function   is   given   by, \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   42 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 41}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\nFor   a   continuous   function, \\xa0\\n\\xa0\\n\\xa0\\nThe   cost   function   is   given   by, \\xa0\\n\\xa0\\nOptimizing   Cost   Function \\xa0\\nThe  regression  coeﬃcients  are  usually  estimated  using  maximum  likelihood  estimation.  The \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmaximum  likelihood  estimator,  denoted  by  is  the  value  of  that  maximizes  the  likelihood \\xa0 \\xa0 \\xa0 \\xa0\\xa0θm l e\\xa0\\xa0\\xa0 \\xa0\\xa0θ\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nfunction .  Unlike  linear  regression  with  normally  distributed  residuals,  it  is  not  possible  to  ﬁnd \\xa0(θ ∣ x) L \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\na  closed-form  expression  for  the  coeﬃcient  values  that  maximize  the  likelihood  function,  so  an \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\niterative  process  must  be  used  instead.  It  is  often  quite  diﬃcult  to  directly  maximize .  But,  it  is \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0(θ ∣ x) L \\xa0 \\xa0\\xa0\\xa0\\nusually  much  easier  to  maximize  the  log-likelihood  function .  Since  is  a  monotonic \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 o g L(θ ∣ x) l \\xa0 \\xa0 o g l\\xa0\\xa0\\xa0 \\xa0\\nfunction,  the  value  of  the  that  maximizes ,  also  maximizes .  Thus,  one  can  also \\xa0\\xa0 \\xa0\\xa0\\xa0θ\\xa0 \\xa0 \\xa0 o g L(θ ∣ x) l \\xa0 \\xa0 \\xa0(θ ∣ x) L \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ndeﬁne     as   the   value   of     that   solves, θm l e θ \\xa0\\n\\xa0\\n\\xa0\\nFor   a   discrete   function, \\xa0\\n\\xa0\\n\\xa0\\nFor   a   continuous   function, \\xa0\\n\\xa0\\n\\xa0\\nOdds   and   Log   Odds \\xa0\\nThis  best  ﬁt  line  function  gives  the  relationship  between  (the  probability  of  target  variable)  and \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 P\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 x\\xa0\\n(the  independent  variables).  But,  this  form  of  the  function  is  not  very  intuitive  i.e  the  relationship \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nbetween  and  is  so  complex  that  it  becomes  diﬃcult  to  understand  what  kind  of  trend  exists \\xa0 P\\xa0 \\xa0 x\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nbetween  the  two.  However,  on  converting  the  function  to  a  slightly  diﬀerent  form,  one  can  achieve  a \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nmuch  more  intuitive  relationship.  Upon  taking  the  natural  logarithm  on  both  sides  of  the  equation \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\none   gets, \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   43 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 42}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\nOdds  are  deﬁned  as  the  ratio  of  probability  of  success  to  the  probability  of  failure.  So,  if  the  odds  of \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\nsuccess  is  4  (0.8/0.2),  it  shows  that  the  odds  of  success  (80%)  has  an  accompanying  odds  of  failure \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\n(20%).   Whereas   ,   Log   odds   is   the   logarithm   of   the   odds   i.e.   Log(4)   =   1.386. \\xa0\\n\\xa0\\nMultivariate   Logistic   Regression \\xa0\\nMultivariate  Logistic  regression  is  just  an  extension  of  the  Univariate  Logistic  regression.  So,  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nform  is  identical  to  Univariate  Logistic  regression,  but  with  more  than  one  covariate.  The  logit \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nequation   is   given   by, \\xa0\\n\\xa0\\n\\xa0\\nStrength   of   Logistic   Regression   Model \\xa0\\nThe  strength  of  any  logistic  regression  model  can  be  assessed  using  various  metrics.  These  metrics \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nusually  provide  a  measure  of  how  well  the  observed  outcomes  are  being  replicated  by  the  model, \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nbased  on  the  proportion  of  total  variation  of  outcomes  explained  by  the  model.  The  various  metrics \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nare, \\xa0\\n\\xa0\\n1)Confusion   Matrix \\xa0\\n2)ROC   Curve \\xa0\\n3)Gain   and   Lift   Chart \\xa0\\n4)KS   -   Statistic \\xa0\\n5)Gini   Coeﬃcient \\xa0\\nConfusion   Matrix \\xa0\\nA  confusion  matrix  is  an  matrix,  where  is  the  number  of  classes  being  predicted.  The \\xa0 \\xa0 \\xa0\\xa0\\xa0 n× n\\xa0 \\xa0 \\xa0 n\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nfollowing   ﬁgure   represents   a   confusion   matrix   for   2   classes. \\xa0\\n\\xa0\\nConfusion   Matrix \\xa0Target \\xa0 Precision \\xa0\\nTP/(TP+FP) \\xa0 Positive \\xa0 Negative \\xa0\\nModel \\xa0Positive \\xa0True \\xa0\\nPositive \\xa0False \\xa0\\nPositive \\xa0Positive   Predictive   Value \\xa0\\nTP/(TP+FP) \\xa0\\nNegative \\xa0False \\xa0\\nNegative \\xa0True \\xa0\\nNegative \\xa0Negative   Predictive   Value \\xa0\\nTN/(FN+TN) \\xa0\\nRecall \\xa0\\nTP/(TP+FN) \\xa0Sensitivity \\xa0\\nTP/(TP+FN) \\xa0Speciﬁcity \\xa0\\nTN/(FP+TN) \\xa0Accuracy \\xa0\\n(TP+TN)/(TP+FP+FN+TN) \\xa0\\n\\xa0\\nAccuracy   of   the   model   is   the   proportion   of   the   total   number   of   predictions   that   were   correct. \\xa0\\nc c u r a c y T P N)  ( T P P N N)  A =( + T/ + F+ F+ T \\xa0\\n\\xa0\\nFor  a  model  with  an  accuracy  of  about  90%  (which  looks  good),  on  revaluation  of  the  confusion \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nmatrix,  one  could  see  that  there  were  still  a  lot  of  misclassiﬁcations  present.  Thus,  other  new \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ndiscriminative   metrics   were   brought   in. \\xa0\\n\\xa0\\n1)Positive   Predictive   Value   /   Precision   :   Proportion   of   positive   cases   correctly   identiﬁed. \\xa0\\no s i t i v e P r e d i c t i v e V a l u e o r P r e c i s i o n T P( T P P) P = / + F \\xa0\\n\\xa0\\n2)Negative   Predictive   Value   :   Proportion   of   negative   cases   correctly   identiﬁed. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   44 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 43}),\n",
              " Document(page_content='e g a t i v e P r e d i c t i v e V a l u e T N( T N N) N = / + F \\xa0\\n\\xa0\\n3)Sensitivity   /   Recall   :   Proportion   of   actual   positive   cases   correctly   identiﬁed. \\xa0\\ne n s i t i v i t y o r R e c a l l T P( T P N) S = / + F \\xa0\\n4)Speciﬁcity   :   Proportion   of   actual   negative   cases   correctly   identiﬁed. \\xa0\\np e c i f i c i t y T N( T N P) S = / + F \\xa0\\n\\xa0\\n5)True   Positive   Rate   :    Proportion   of   actual   positive   cases   correctly   identiﬁed. \\xa0\\nr u e P o s i t i v e R a t e T P( T P N) T = / + F \\xa0\\n\\xa0\\n6)False   Positive   Rate   :   Proportion   of   actual   negative   cases   incorrectly   identiﬁed. \\xa0\\np e c i f i c i t y F P( F P N) S = / + T \\xa0\\n\\xa0\\n7)F1   Score   :   It   is   the   harmonic   mean   of   Precision   and   Recall. \\xa0\\n1 S c o r e 2 P r e c i s i o n e c a l l)( P r e c i s i o n e c a l l) F = ×( × R / + R \\xa0\\nROC   Curve \\xa0\\nThe  ROC  or  Receiver  Operating  Characteristics  curve  is  the  plot  between  True  Positive  Rate  and  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nFalse  Positive  Rate,  or  simply,  a  tradeoﬀ  between  sensitivity  and  speciﬁcity.  The  biggest  advantage \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nof  using  the  ROC  curve  is  that  it  is  independent  of  the  change  in  proportion  of  responders.  This  is \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nbecause  it  has  both  the  axes  derived  out  from  the  columnar  calculations  of  confusion  matrix,  where \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nthe  numerator  and  denominator  of  both  and  axis  change  on  a  similar  scale  for  any  change  in \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 x\\xa0 \\xa0 y\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nthe  response  rate.  The  following  ﬁgure  shows  the  more  the  ROC  curve  is  towards  the  upper-left \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\ncorner   (i.e   the   more   is   the   area   under   the   curve),   the   better   is   the   model. \\xa0\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nAs  can  be  seen,  from  the  accuracy,  sensitivity  and  speciﬁcity  tradeoﬀ,  that  when  the  probability \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nthresholds  are  very  low,  the  sensitivity  is  very  high  and  the  speciﬁcity  is  very  low.  Similarly,  for  larger \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nprobability  thresholds,  the  sensitivity  values  are  very  low  but  the  speciﬁcity  values  are  very  high. \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nOne  could  choose  any  cut-oﬀ  point  based  on  which  of  these  metrics  is  required  to  be  high  (like  if \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\none  wants  to  capture  the  positives  better  then  some  accuracy  could  be  let  oﬀ  for  the  sake  of  higher \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\nsensitivity  and  a  lower  cut-oﬀ  be  chosen).  It  is  completely  dependent  on  the  situation.  But  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\noptimal  cut-oﬀ  point  (where  accuracy,  sensitivity  and  speciﬁcity  meet)  can  give  a  fair  idea  of  how  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthresholds   should   be   chosen. \\xa0\\nGain   and   Lift   Chart \\xa0\\nBefore  going  to  Gain  and  Lift  chart  one  needs  to  understand  the  decile.  The  following  steps  are \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nused   to   get   the   decile   chart. \\xa0\\n\\xa0\\n1)Probability   of   each   observation   is   calculated. \\xa0\\n2)The   probabilities   are   ranked   in   decreasing   order. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   45 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 44}),\n",
              " Document(page_content='3)Deciles   are   built   with   each   group   having   almost   10%   of   the   observations. \\xa0\\n4)The   response   rates   are   calculated   at   each   decile. \\xa0\\n\\xa0\\nGain  Chart  :  The  gain  of  a  given  decile  level  is  the  proportion  of  the  cumulative  number  of  targets  till \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthat   decile   to   the   total   number   of   targets   in   the   dataset. \\xa0\\n\\xa0\\nLift  Chart  :  The  lift  of  a  given  decile  level  is  the  proportion  of  gain  percentage  to  the  random \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nexpectation  percentage  at  that  decile  level  or  simply  put  it  measures  how  much  better  one  can \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nexpect   to   do   with   the   model   compared   to   a   random   model. \\xa0\\n\\xa0\\nThe   following   ﬁgures   give   a   representation   of   the   Gain   and   Lift   charts. \\xa0\\n\\xa0\\nDecile \\xa0Data \\xa0\\nPoints \\xa0Class \\xa0\\n1 \\xa0Cum \\xa0\\nClass \\xa0\\n1 \\xa0%   Cum \\xa0\\n  Class \\xa0\\n1 \\xa0Class \\xa0\\n0 \\xa0Cum \\xa0\\nClass \\xa0\\n0 \\xa0%Cum \\xa0\\nClass \\xa0\\n0 \\xa0Gain \\xa0\\nRandom \\xa0\\nModel \\xa0%   Gain \\xa0\\nClass \\xa0\\n1 \\xa0Lift \\xa0\\nClass \\xa0\\n1 \\xa0K-S \\xa0\\nStatistic \\xa0\\n1 \\xa0 211 \\xa0 159 \\xa0 159 \\xa0 28.3% \\xa0 52 \\xa0 52 \\xa0 3.4% \\xa0 10% \\xa0 28.3% \\xa0 2.83 \\xa0 25.0% \\xa0\\n2 \\xa0 211 \\xa0 129 \\xa0 288 \\xa0 51.3% \\xa0 82 \\xa0 134 \\xa0 8.7% \\xa0 20% \\xa0 51.3% \\xa0 2.57 \\xa0 42.7% \\xa0\\n3 \\xa0 211 \\xa0 86 \\xa0 374 \\xa0 66.7% \\xa0 125 \\xa0 259 \\xa0 16.7% \\xa0 30% \\xa0 66.7% \\xa0 2.22 \\xa0 49.9% \\xa0\\n4 \\xa0 211 \\xa0 75 \\xa0 449 \\xa0 80.0% \\xa0 136 \\xa0 395 \\xa0 25.5% \\xa0 40% \\xa0 80.0% \\xa0 2.00 \\xa0 54.5% \\xa0\\n5 \\xa0 211 \\xa0 34 \\xa0 483 \\xa0 86.1% \\xa0 177 \\xa0 572 \\xa0 36.9% \\xa0 50% \\xa0 86.1% \\xa0 1.72 \\xa0 49.2% \\xa0\\n6 \\xa0 211 \\xa0 37 \\xa0 520 \\xa0 92.7% \\xa0 174 \\xa0 746 \\xa0 48.2% \\xa0 60% \\xa0 92.7% \\xa0 1.54 \\xa0 44.5% \\xa0\\n7 \\xa0 211 \\xa0 23 \\xa0 543 \\xa0 96.8% \\xa0 188 \\xa0 934 \\xa0 60.3% \\xa0 70% \\xa0 96.8% \\xa0 1.38 \\xa0 36.5% \\xa0\\n8 \\xa0 211 \\xa0 13 \\xa0 556 \\xa0 99.1% \\xa0 198 \\xa0 1132 \\xa0 73.1% \\xa0 80% \\xa0 99.1% \\xa0 1.24 \\xa0 26.0% \\xa0\\n9 \\xa0 211 \\xa0 2 \\xa0 558 \\xa0 99.5% \\xa0 209 \\xa0 1341 \\xa0 86.6% \\xa0 90% \\xa0 99.5% \\xa0 1.11 \\xa0 12.9% \\xa0\\n10 \\xa0 211 \\xa0 3 \\xa0 561 \\xa0 100% \\xa0 208 \\xa0 1549 \\xa0 100% \\xa0 100% \\xa0 100% \\xa0 1.00 \\xa0 0.0% \\xa0\\n\\xa0 2110 \\xa0 561 \\xa0 1549 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nK-S   Statistic \\xa0\\nK-S  or  Kolmogorov-Smirnov  statistic  is  an  indicator  of  how  well  the  model  discriminates  between  the \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ntwo  classes  i.e  a  measure  of  the  degree  of  separation  between  the  positive  and  negative \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ndistributions.  It  is  equal  to  0%  for  the  random  model,  and  100%  for  the  perfect  model.  Thus,  the  KS \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nstatistic   gives   an   indicator   of   where   the   model   lies   between   the   two. \\xa0\\xa0\\n\\xa0\\nA   good   model   is   one   for   which   the   K-S   statistic   follows   the   below   conditions, \\xa0\\n1)It   is   equal   to   40%   or   more. \\xa0\\n2)It   lies   in   the   top   deciles,   i.e.   1st,   2nd,   3rd   or   4th. \\xa0\\n\\xa0\\nGini   Coeﬃcient \\xa0\\nThe   Gini   coeﬃcient   is   nothing   but   a   fancy   word   and   is   given   by, \\xa0\\ni n i A U C A r e a U n d e r R O C C u r v e G = = \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   46 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 45}),\n",
              " Document(page_content=\"Python   Code   -   Logistic   Regression  \\nModel  \\n>>    import    statsmodels.api    as    sm  \\n>>   links   =   sm.families.links  \\n>>   model   =   sm.GLM(y_train,(sm.add_constant(X_train)),  \\n                   family=sm.families.Binomial(link=links.logit))  \\n#   link   can   take   any   of   the   value   logit,   probit   or   cloglog  \\n>>   model.fit().summary()  \\n \\n>>    from    sklearn.linear_model    import    LogisticRegression  \\n>>   model   =   LogisticRegression(class_weight= 'balanced' )  \\n#   For   balancing   the   imbalanced   classes  \\n>>   model.fit(X_train,   y_train)  \\n \\nPredict  \\n>>   y_pred   =   model.predict(X_test)  \\n \\nAnalyze   Model  \\n>>    from    sklearn    import    metrics  \\n>>   metrics.accuracy_score(y_test,   y_pred)  \\n#   Accuracy  \\n>>    from    sklearn.metrics    import    confusion_matrix  \\n>>   confusion_matrix   =   confusion_matrix(y_test,   y_pred)  \\n#   Confusion   Matrix  \\n>>    from    sklearn.metrics    import    classification_report  \\n>>   classification_report(y_test,   y_pred)  \\n#   Precision,   Recall   and   F1-score  \\n>>    from    sklearn.metrics    import    roc_auc_score  \\n>>    from    sklearn.metrics    import    roc_curve  \\n>>   logit_roc_auc   =   roc_auc_score(y_test,   y_pred)  \\n>>   fpr,   tpr,   thresholds   =   roc_curve(y_test,   y_pred[:,1])  \\n>>   plt.plot(fpr,tpr,   label= 'Logistic   Regression   (area   =   %0.2f)'    %   logit_roc_auc)  \\n#   ROC-AUC   Curve  \\n\\xa0\\n4.1.2.   CHALLENGES   IN   LOGISTIC   REGRESSION \\xa0\\nLogistic   regression   is   a   widely   used   technique   in   various   industries   because   of   two   main   reasons, \\xa0\\n\\xa0\\n1)It   is   very   easy   to   understand   and   oﬀers   an   intuitive   explanation   of   the   variables. \\xa0\\n\\xa0\\n2)The  output  (i.e.  the  probabilities)  has  a  linear  relationship  with  the  log  of  odds,  which  can  be \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nvery  useful  for  explaining  results  to  stakeholders.  Usually  scores  are  used  instead  of  log \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nodds.  Scores  are  just  another  way  of  reporting  the  ﬁndings  in  a  more  elegant  way  than  the \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nlog  odds  by  using  a  user  deﬁned  function  to  scale  the  log  odds  into  a  very  presentable \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nmanner.  For  example,  one  can  use  the  user  deﬁned  function \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n  to   ﬁnd   the   scores   from   log   odds. c o r e 50020 o g( o d d s) l o g(2)) S = +(× l / \\xa0\\nBut  there  are  certain  nuances  that  need  to  be  taken  care  of  before  proceeding  for  a  Logistic \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nregression   model   for   getting   better   results.   Following   are   the   various   nuances   discussed   in   detail. \\xa0\\nSample   Selection \\xa0\\nSelecting  the  right  sample  is  very  essential  for  solving  any  business  problem.  One  should  look  out \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nfor   the   below   mentioned   errors   while   selecting   a   sample. \\xa0\\n\\xa0\\n1)Seasonal  or  cyclic  ﬂuctuations  population  :  The  seasonal  ﬂuctuations  in  the  business  (such \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nas  new  year  sales,  economic  ups  and  downs  etc.)  needs  to  be  taken  care  of  while  building \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   47 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 46}),\n",
              " Document(page_content=\"the  samples.  A  wide  range  of  data  should  be  selected  representing  each  type  of  seasonal \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nﬂuctuations. \\xa0\\n\\xa0\\n2)Representative  population  :  The  sample  should  be  representative  of  the  population  on  which \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\nthe   model   is   going   to   be   applied   in   the   future   rather   than   all   the   types   of   population. \\xa0\\n\\xa0\\n3)Rare  incidence  population  :  For  rare  events  samples  (such  as  fraud  transactions,  anomaly \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\ndetection  etc.),  the  samples  should  be  stratiﬁed  and  balanced  before  being  used  for \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nmodelling. \\xa0\\nSegmentation \\xa0\\nUsually  even  after  having  selected  the  samples  with  all  due  considerations,  there  are  chances  that \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nthe  model  may  not  perform  well  on  the  chosen  sample  data  set.  Assuming  that  no  new  sample  is \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\navailable,  the  performance  of  the  model  can  still  be  improved  upon  by  performing  the  segmentation \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nof   the   population. \\xa0\\n\\xa0\\nIt  is  used  when  there  are  diﬀerent  segments  in  the  dataset.  The  notion  being  that  the  behaviour  of \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ndiﬀerent  segments  of  the  population  depend  on  diﬀerent  variables  (such  as  a  student  defaulting  on \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nloan  depends  on  factors  like  course  enrolled  for,  institute  enrolled  in,  parents'  income  etc.  while  a \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nsalaried  person  defaulting  on  loan  depends  on  factors  like  marital  status,  income,  etc.).  As  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\npredictive  patterns  across  these  segments  are  very  diﬀerent,  it  makes  more  sense  to  build  diﬀerent \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nchild  models  for  each  of  these  segments  rather  than  building  one  single  parent  model.  Now,  a \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ncombined  model  of  these  child  models  gives  a  better  result  than  a  single  model.  It  should  be  noted \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nthat  segmentation  should  only  be  done  on  a  variable  which  is  more  likely  to  provide  diﬀerent  sets  of \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\npredictors   for   the   model. \\xa0\\nVariable   Transformation \\xa0\\nIt  is  already  known  that  the  categorical  variables  need  to  be  transformed  into  dummies  and  numeric \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nvariables  be  standardised  before  proceeding  with  any  modelling.  However,  one  could  also  convert \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nthe  numeric  variables  into  dummy  variables.  The  major  advantage  oﬀered  by  dummies  especially  for \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\ncontinuous  variables  is  that  they  stabilize  the  model  i.e  small  variations  in  the  variables  will  not  have \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\na  very  big  impact  on  a  model  built  using  dummies  whereas  it  will  still  have  a  sizable  impact  on  a \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nmodel  built  using  continuous  variables  as  is.  But,  it  also  has  a  certain  disadvantage  of  the  data \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\ngetting   compressed   into   very   few   categories   resulting   in   information   loss   and   data   clumping. \\xa0\\n\\xa0\\nApart  from  creating  dummies,  there  are  also  other  techniques  for  transforming  variables.  The \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\ncommonly   used   techniques   used   for   transforming   variables   are, \\xa0\\n\\xa0\\n1)WOE   (Weight   of   Evidence)   Transformation \\xa0\\n2)Interaction   Variables \\xa0\\n3)Splines \\xa0\\n4)Mathematical   Transformation \\xa0\\n5)PCA   (Principal   Component   Analysis) \\xa0\\nWOE   (Weight   of   Evidence)   Transformation \\xa0\\nWOE  can  be  calculated  for  both  the  categorical  as  well  as  continuous  data.  For  continuous  data  the \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nWOE   is   usually   calculated   after   having   binned   the   continuous   data. \\xa0\\n\\xa0\\nO E l o g ( P e r c e n t a g e o f E v e n t s i n b u c k e t) l o g ( P e r c e n t a g e o f N o n E v e n t s i n b u c k e t) W = − \\xa0\\n\\xa0\\nThe   following   ﬁgure   gives   a   representation   of   the   WOE   and   IV   calculated. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   48 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 47}),\n",
              " Document(page_content='Bins \\xa0 Number   of   Events \\xa0 Number   of   Non   Events \\xa0 WOE \\xa0 Information   Value \\xa0\\n0-5 \\xa0 627 \\xa0 744 \\xa0 -1.19 \\xa0 0.33 \\xa0\\n6-19 \\xa0 947 \\xa0 489 \\xa0 -0.36 \\xa0 0.03 \\xa0\\n20-39 \\xa0 1095 \\xa0 320 \\xa0 0.21 \\xa0 0.01 \\xa0\\n40-59 \\xa0 1121 \\xa0 217 \\xa0 0.62 \\xa0 0.06 \\xa0\\n60-72 \\xa0 1384 \\xa0 99 \\xa0 1.62 \\xa0 0.35 \\xa0\\nTotal \\xa0 5174 \\xa0 1869 \\xa0 \\xa0 0.78 \\xa0\\n\\xa0\\nOnce  the  WOE  values  have  been  calculated,  it  is  important  to  note  that  the  WOE  values  should \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nfollow  an  increasing  or  decreasing  trend  across  the  bins.  If  the  trend  is  not  monotonic,  then  the \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nbuckets/bins  (coarse  buckets)  need  to  be  compressed  and  the  WOE  values  calculated  again.  It  can \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nbe  seen  that  WOE  does  something  similar  to  creating  dummy  variables  (i.e.  replacing  a  range  of \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nvalues  with  an  indicative  variable).  It  is  just  that,  instead  of  replacing  it  with  a  simple  0  or  1  (which  is \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\nnot  thought  out  at  all),  it  is  getting  replaced  with  a  well  thought  out  WOE  value  (which  reduces  the \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nchances   of   undesired   score   clumping). \\xa0\\n\\xa0\\nThe   advantages   of   using   WOE   are, \\xa0\\n\\xa0\\n1)WOE  reﬂects  the  group  identity,  i.e.  it  captures  the  general  trend  of  distribution  of  both  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nevents   and   non-events. \\xa0\\n\\xa0\\n2)NA  values  can  be  treated  with  WOE  values.  However,  one  can  replace  the  NA  bucket  with  a \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nbucket  which  shows  similar  WOE  values.  Thus,  WOE  helps  in  treating  missing  values  logically \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nfor   both   categorical   and   continuous   variables. \\xa0\\xa0\\n\\xa0\\n3)WOE  makes  the  model  more  stable  as  small  changes  in  the  continuous  variables  will  not \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nimpact   the   input   so   much. \\xa0\\n\\xa0\\nThe   only   disadvantage   of   WOE   is   that   one   might   end   up   doing   some   score   clumping. \\xa0\\n\\xa0\\nRank  Ordering  :  It  is  a  simple  line  graph  of  percentage  of  events  against  deciles  (scoring  bins).  On \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ncalculating  the  percentage  of  events  in  each  decile  group,  one  can  easily  see  the  event  rate \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nmonotonically  decreasing.  It  means  that  the  model  predicts  the  highest  number  of  events  in  the  ﬁrst \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\ndecile   and   then   goes   progressively   down. \\xa0\\n\\xa0\\nIV  (Information  Value)  :  It  is  an  important  indicator  of  predictive  power.  It  mainly  helps  in \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nunderstanding  how  the  binning  of  variables  should  be  done.  The  binning  should  be  done  such  that \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nthe   WOE   trend   across   bins   is   monotonic   and   the   IV   (information   value)   should   be   high.   IV   is   given   by, \\xa0\\n\\xa0\\nV W O E P e r c e n t a g e o f E v e n t s i n b u c k e t P e r c e n t a g e o f N o n E v e n t s i n b u c k e t) I= ×( − \\xa0\\nInteraction   Variables \\xa0\\nCombining  various  variables  to  create  one  variable  which  gives  a  good  meaning  is  called  an \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ninteraction  variable.  The  interaction  variables  can  be  built  on  a  judgement  call  based  upon  a  deep \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nunderstanding  of  the  business  or  by  building  a  small  decision  tree.  Indeed,  it  is  only  because  of \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nincorporation  of  these  interaction  variables  that  Random  Forests  and  Neural  networks  are  able  to \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\noutperform   the   Logistic   Regression. \\xa0\\nSplines \\xa0\\nA  spline  is  basically  obtained  by  ﬁtting  a  polynomial  over  the  WOE  values.  Using  a  spline  oﬀers  high \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\npredictive   power,   but   it   may   result   in   unstable   models. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   49 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 48}),\n",
              " Document(page_content='Mathematical   Transformation \\xa0\\nMathematical  transformations  such  as , and  are  commonly  used  under  this  type  of \\xa0 \\xa0 \\xa0\\xa0 x2\\xa0 x3\\xa0 o g x l \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ntransformation.   But   these   transformations   are   not   very   easy   to   explain   or   very   intuitive. \\xa0\\nPCA   (Principal   Component   Analysis) \\xa0\\nIt  is  a  very  important  and  eﬀective  transformation  technique.  It  transforms  the  variables  into \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ncomponents  that  are  orthogonal  to  each  other  or  to  say  the  variables  are  grouped  and  modiﬁed  and \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nput  into  a  few  bunches  such  that  each  bunch  is  not  correlated  with  the  other.  But  these \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ntransformations   are   also   not   very   easy   to   explain   or   very   intuitive. \\xa0\\n4.1.3.   IMPLEMENTATION   OF   LOGISTIC   REGRESSION \\xa0\\nOnce  the  data  is  prepared  for  modelling  by  performing  tasks  such  as  sample  selection, \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nsegmentation,  and  variable  transformation  techniques,  then  the  model  is  built.  Next  comes  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nmodel   evaluation,   validation   and   governance. \\xa0\\nModel   Evaluation \\xa0\\nThe   model   performance   measures   are   grouped   under   three   broad   classes. \\xa0\\n1)Discriminatory  Power  :  It  measures  how  good  the  model  is  at  separating  out  the  classes. \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nVarious   metrics   being   used   for   measuring   are, \\xa0\\na)KS   -   Statistic \\xa0\\nb)Gini   Coeﬃcient \\xa0\\nc)Rank   Ordering \\xa0\\nd)Sensitivity \\xa0\\ne)Speciﬁcity \\xa0\\n\\xa0\\n2)Accuracy  :  It  measures  how  accurately  the  model  is  able  to  predict  the  classes.  Various \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nmetrics   being   used   for   measuring   are, \\xa0\\na)Sensitivity \\xa0\\nb)Speciﬁcity \\xa0\\nc)Comparing   Actual   v/s   Predicted   Log   Odds \\xa0\\n\\xa0\\n3)Stability  :  It  measures  how  stable  the  model  is  in  predicting  unseen  data.  Various  metrics \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nbeing   used   for   measuring   are, \\xa0\\na)Performance  Stability  :  Results  of  in-sample  validation  approximately  match  those  of \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nout-of-time   validation. \\xa0\\nb)Variable  Stability  :  The  sample  used  for  model  building  has  not  changed  too  much \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nand  has  the  same  general  characteristics.  PSI  (Population  Stability  Index)  is  used  for \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe   same. \\xa0\\nModel   Validation \\xa0\\nThe   model   can   be   validated   using   the   following   methods, \\xa0\\n1)In   Sample   validation \\xa0\\n2)Out   of   Time   validation \\xa0\\n3)K-Cross   validation \\xa0\\nModel   Governance \\xa0\\nThe  process  of  tracking  the  model  over  time  as  it  is  being  used  is  referred  to  as  Model  Governance. \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nThe  tracking  is  done  by  evaluating  the  model  on  the  basis  of  the  ongoing  samples  and  comparing  it \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nwith  the  previous  such  evaluation  result.  The  following  ﬁgure  represents  the  metrics  of  a  model \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nperformance   over   time. \\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   50 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 49}),\n",
              " Document(page_content='Quarter \\xa0 Performance   Metric   (Gini) \\xa0Action   Taken \\xa0\\n2014   Q4 \\xa0 0.84 \\xa0 \\xa0\\n2015   Q1 \\xa0 0.82 \\xa0 \\xa0\\n2015   Q2 \\xa0 0.80 \\xa0 \\xa0\\n2015   Q3 \\xa0 0.78 \\xa0 \\xa0\\n2015   Q4 \\xa0 0.75 \\xa0 \\xa0\\n2016   Q1 \\xa0 0.72 \\xa0 Recalibration \\xa0\\n2016   Q2 \\xa0 0.80 \\xa0 \\xa0\\n2016   Q3 \\xa0 0.76 \\xa0 \\xa0\\n2016   Q4 \\xa0 0.71 \\xa0 Rebuilding \\xa0\\n2017   Q1 \\xa0 0.83 \\xa0 \\xa0\\n\\xa0\\nAs  can  be  seen,  when  the  model’s  performance  dropped  to  0.72  for  the  ﬁrst  time,  building  of  a  new \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nmodel  was  avoided.  Basically,  the  model  was  just  recalibrated  (i.e.  only  the  coeﬃcients  of  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nvariables  were  updated),  which  resulted  in  a  slight  increase  in  the  performance.  However,  the  next \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ntime  the  performance  dropped  to  a  low  value  of  0.71,  the  model  was  rebuilt  (i.e.  an  entire  new \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nmodel   was   built   with   new   sample   data). \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   51 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 50}),\n",
              " Document(page_content='4.2.   NAIVE   BAYES \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNAIVE   BAYES \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   52 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 51}),\n",
              " Document(page_content=\"4.2.1.   BAYES’   THEOREM \\xa0\\nNaive  Bayes  is  a  probabilistic  classiﬁer  which  returns  the  probability  of  a  test  point  belonging  to  a \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nclass   rather   than   the   label   of   the   test   point. \\xa0\\nBuilding   Blocks   of   Bayes   Theorem   -   Probability \\xa0\\nThe  probability  of  an  event  is  a  measure  of  the  chance  that  the  event  will  occur  as  a  result  of  an \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nexperiment  i.e.  the  number  of  ways  the  event  can  occur  divided  by  the  total  number  of  possible \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\noutcomes.   The   probability   of   an   event     is   given   by, E \\xa0\\n\\xa0\\n( E) N u m b e r o f f a v o u r a b l e o u t c o m e s  T o t a l n u m b e r o f p o s s i b l e o u t c o m e s P = / \\xa0\\n\\xa0\\nFor   example,   the   probability   that   a   card   is   a   Four   is, \\xa0\\n\\xa0\\n( F o u r) 52 13 P =4/=1/\\xa0\\nBuilding   Blocks   of   Bayes   Theorem   -   Joint   Probability \\xa0\\nJoint  probability  is  a  statistical  measure  that  calculates  the  likelihood  of  two  events  occurring \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\ntogether   and   at   the   same   point   in   time.   The   joint   probability   of   two   events   A   and   B   is   given   by, \\xa0\\n\\xa0\\n( A,) P( A ) P( B ) P( A) ( B) P B= ⋂ B= ⋂ A= × P \\xa0\\n\\xa0\\nFor   example,   the   probability   that   a   card   is   a   Four   and   Red   is, \\xa0\\n\\xa0\\n( F o u r e d) ( F o u r) ( R e d)452)2652) 26 P ⋂ R = P × P =(/ ×(/ =1/\\xa0\\nBuilding   Blocks   of   Bayes   Theorem   -   Conditional   Probability \\xa0\\nConditional  probability  is  a  measure  of  the  probability  of  an  event  occurring  given  that  another  event \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nhas   occurred.   The   probability   of   event   A   given   that   event   B   has   already   occurred   is   given   by, \\xa0\\n\\xa0\\n( A ∣ B) P( A )  P( B) P = ⋂ B/ \\xa0\\xa0\\n\\xa0\\nFor   example,   the   probability   that   a   card   is   a   Four   given   that   the   card   is   Red   is, \\xa0\\n\\xa0\\n( F o u r ∣ R e d) ( F o u r e d)  P( R e d) ( F o u r) ( R e d)  P( R e d)452) 13 P = P ⋂ R/ = P × P / =(/ =1/\\xa0\\nBayes   Theorem \\xa0\\nBayes  Theorem  gives  the  probability  of  an  event,  based  on  prior  knowledge  of  conditions  that  might \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nbe  related  to  the  event.  For  example,  if  diabetes  is  related  to  age,  then,  using  Bayes'  theorem,  a \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nperson's  age  can  be  used  to  more  accurately  assess  the  probability  that  they  have  diabetes, \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ncompared  to  the  assessment  of  the  probability  of  diabetes  made  without  the  knowledge  of  the \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nperson's  age.  If  the  probability  of  an  event  B  occurring  given  that  event  A  has  already  occurred  and \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nindividual  probabilities  of  A  and  B  are  known,  then  the  probability  of  event  A  given  B  has  already \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\noccurred   can   be   found   out   using   Bayes   Theorem   given   by, \\xa0\\xa0\\n\\xa0\\n( A ∣ B) P( B ∣ A) ( A)  P( B) P = × P/ \\xa0\\n\\xa0\\nor \\xa0\\n\\xa0\\n( B ∣ A) P( A ∣ B) ( B)  P( A) P = × P/ \\xa0\\n\\xa0\\nThe   above   equation   can   also   be   deﬁned   in   terms   prior,   posterior   and   likelihood   as   follows, \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   53 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 52}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\n4.2.2.   NAIVE   BAYES \\xa0\\nNaive  Bayes  is  a  classiﬁcation  technique  based  on  Bayes’  Theorem  with  an  assumption  of \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nindependence  among  predictors.  In  simple  terms,  a  Naive  Bayes  classiﬁer  assumes  that  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\npresence  of  a  particular  feature  in  a  class  is  unrelated  to  the  presence  of  any  other  feature.  For \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nexample,  a  fruit  may  be  considered  to  be  an  apple  if  it  is  red,  round,  and  about  3  inches  in  diameter. \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nEven  if  these  features  depend  on  each  other  or  upon  the  existence  of  the  other  features,  all  of  these \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nproperties  independently  contribute  to  the  probability  that  this  fruit  is  an  apple  and  that  is  why  it  is \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nknown   as   Naive. \\xa0\\nAssumptions   for   Naive   Bayes \\xa0\\nThe   fundamental   Naive   Bayes   assumptions   are, \\xa0\\n\\xa0\\n1)Independence  :  No  pair  of  features  are  dependent.  For  example,  the  colour  of  apple  being \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nred  has  nothing  to  do  with  the  apple  being  round  and  the  apple  being  round  has  no  eﬀect \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\non  the  apple  being  of  3  inches  in  diameter.  Hence,  the  features  are  assumed  to  be \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nindependent.   i.e.   ( A,) P( A) ( B) P B= × P \\xa0\\n\\xa0\\n2)Equal  contribution  to  the  outcome  :  Each  feature  is  given  the  same  weightage  (or \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nimportance).  For  example,  knowing  only  the  colour  and  the  size  alone  cannot  predict  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\noutcome  accurately.  Hence,  none  of  the  attributes  are  irrelevant  and  assumed  to  be \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\ncontributing   equally   to   the   outcome. \\xa0\\n\\xa0\\nThe  assumptions  made  by  Naive  Bayes  are  not  generally  correct  in  real-world  situations.  In-fact,  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nindependence   assumption   is   never   correct   but   often   works   well   in   practice. \\xa0\\nNaive   Bayes   Theorem \\xa0\\nOn   applying   the   above   assumptions   to   the   Bayes   theorem   one   gets, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe  value  of  the  denominator  remains  the  same  for  all  the  classes  for  any  given \\xa0 \\xa0\\xa0\\xa0 \\xa0( x) P( x)... P( x) P1 2 n\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\ndata  point.  Thus,  the  denominator  behaving  as  a  scaling  factor  out  here  can  be  ignored  without \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\naﬀecting   the   ﬁnal   outcome   giving, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   54 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 53}),\n",
              " Document(page_content='Prior  Probability  :  is  known  as  the  prior  probability.  It  is  the  probability  of  an  event  occurring \\xa0 \\xa0\\xa0( C) Pk\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nbefore  the  collection  of  new  data  i.e.  our  prior  beliefs  before  the  collection  of  speciﬁc  information. \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nPrior  plays  an  important  role  while  classifying  using  Naive  Bayes,  as  it  highly  inﬂuences  the  class  of \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe   new   test   point. \\xa0\\n\\xa0\\nLikelihood  Function  :  represents  the  likelihood  function.  It  gives  the  likelihood  of  a  data \\xa0 \\xa0\\xa0( x ∣ C) Pi k\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\npoint   occurring   in   a   class   i.e.   updates   our   prior   beliefs   with   the   new   information. \\xa0\\n\\xa0\\nPosterior  Probability  :  is  called  the  posterior  probability,  which  is  ﬁnally  compared \\xa0 \\xa0\\xa0( C ∣ x,,.. x) Pk1 x2. n\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nfor  the  classes  and  the  test  point  is  assigned  to  the  class  for  which  the  posterior  probability  is \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\ngreater  i.e.  the  ﬁnal  outcome  is  a  balanced  combination  of  the  prior  beliefs  and  case-speciﬁc \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ninformation. \\xa0\\nNaive   Bayes   Classiﬁer \\xa0\\nIn  the  Naive  Bayes  theorem  only  the  independent  feature  model  had  been  derived,  i.e.  the  Naive \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nBayes  probability  model.  Now,  one  can  easily  build  the  Naive  Bayes  Classiﬁer  by  combining  this \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nmodel  with  a  decision  rule.  One  of  the  most  common  rules  is  to  pick  the  hypothesis  that  is  the  most \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\nprobable  i.e  a  class  whose  posterior  is  greater  than  the  posterior  of  the  other  classes.  This  is  known \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nas  the  Maximum  A  Posteriori  or  MAP  decision  rule.  The  Bayes  classiﬁer  that  assigns  a  class  label \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\n  to   an   observation     is   given   by, y= Ck x \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n4.2.3.   NAIVE   BAYES   FOR   TEXT   CLASSIFICATION \\xa0\\nNaive  Bayes  is  most  commonly  used  for  text  classiﬁcation  in  applications  such  as  predicting  spam \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nemails,  classifying  text  into  categories  such  as  politics,  sports,  lifestyle  etc.  In  general,  Naive  Bayes \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nhas  proven  to  perform  well  in  text  classiﬁcation  applications.  The  following  segments  explain  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nMultinomial  Naive  Bayes  Classiﬁer  and  Bernoulli  Naive  Bayes  Classiﬁer  for  the  categorical  data \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nusing   an   example   of   Naive   Bayes   document   classiﬁer. \\xa0\\nDocument   Classiﬁer   Data \\xa0\\n1)Test   Data \\xa0\\n\\xa0\\nIndex \\xa0 Document \\xa0 Class \\xa0\\n0 \\xa0 Harvard   is   a   great   educational   institution. \\xa0 education \\xa0\\n1 \\xa0 Educational   greatness   depends   on   ethics. \\xa0 education \\xa0\\n2 \\xa0 A   story   of   great   ethics   and   educational   greatness. \\xa0 education \\xa0\\n3 \\xa0 Titanic   is   a   great   cinema. \\xa0 cinema \\xa0\\n4 \\xa0 A   good   movie   depends   on   a   good   story. \\xa0 cinema \\xa0\\n\\xa0\\nThe  above  documents  can  be  converted  into  features  by  breaking  the  sentences  into  words \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nand  putting  the  unique  words  into  a  bag.  While  selecting  the  unique  words  the  stopwords \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\n(such  as  a,  an,  the,  is  etc.)  are  removed  as  these  are  inconsequential  in  terms  of  providing \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nany   discriminatory   information   helpful   in   classiﬁcation   process. \\xa0\\n\\xa0\\n2)Vocabulary   Dictionary \\xa0\\n\\xa0\\nDictionary   with   Stop   Words \\xa0\\n\\xa0Stop   Words \\xa0\\n\\xa0Final   Vocabulary \\xa0\\n0 \\xa0 and \\xa0 0 \\xa0 and \\xa0 \\xa0 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   55 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 54}),\n",
              " Document(page_content='1 \\xa0 cinema \\xa0 \\xa0 \\xa0 0 \\xa0 cinema \\xa0\\n2 \\xa0 depends \\xa0 \\xa0 \\xa0 1 \\xa0 depends \\xa0\\n3 \\xa0 educational \\xa0 \\xa0 \\xa0 2 \\xa0 educational \\xa0\\n4 \\xa0 ethics \\xa0 \\xa0 \\xa0 3 \\xa0 ethics \\xa0\\n5 \\xa0 good \\xa0 \\xa0 \\xa0 4 \\xa0 good \\xa0\\n6 \\xa0 great \\xa0 \\xa0 \\xa0 5 \\xa0 great \\xa0\\n7 \\xa0 greatness \\xa0 \\xa0 \\xa0 6 \\xa0 greatness \\xa0\\n8 \\xa0 institution \\xa0 \\xa0 \\xa0 7 \\xa0 institution \\xa0\\n9 \\xa0 is \\xa0 9 \\xa0 is \\xa0 \\xa0 \\xa0\\n10 \\xa0 movie \\xa0 \\xa0 \\xa0 8 \\xa0 movie \\xa0\\n11 \\xa0 of \\xa0 11 \\xa0 of \\xa0 \\xa0 \\xa0\\n12 \\xa0 on \\xa0 12 \\xa0 on \\xa0 \\xa0 \\xa0\\n13 \\xa0 titanic \\xa0 \\xa0 \\xa0 9 \\xa0 titanic \\xa0\\n14 \\xa0 story \\xa0 \\xa0 \\xa0 10 \\xa0 story \\xa0\\n15 \\xa0 harvard \\xa0 \\xa0 \\xa0 11 \\xa0 harvard \\xa0\\n\\xa0\\nNow  that  the  vocabulary  is  ready,  next  it  can  be  converted  into  a  Bag  of  Word \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nRepresentation. \\xa0\\n\\xa0\\n3)Bag   Of   Words   (BOW)   Representation \\xa0\\n\\xa0\\nVocabulary \\xa0 D0 \\xa0 D1 \\xa0 D2 \\xa0 Total \\xa0P(W|C) \\xa0D3 \\xa0 D4 \\xa0 Total \\xa0P(W|C) \\xa0\\n0 \\xa0 cinema \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/13 \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 1/8 \\xa0\\n1 \\xa0 depends \\xa0 0 \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 1/13 \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 1/8 \\xa0\\n2 \\xa0 educational \\xa0 1 \\xa0 1 \\xa0 1 \\xa0 3 \\xa0 3/13 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/8 \\xa0\\n3 \\xa0 ethics \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 2 \\xa0 2/13 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/8 \\xa0\\n4 \\xa0 good \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/13 \\xa0 0 \\xa0 2 \\xa0 2 \\xa0 2/8 \\xa0\\n5 \\xa0 great \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 2 \\xa0 2/13 \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 1/8 \\xa0\\n6 \\xa0 greatness \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 2 \\xa0 2/13 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/8 \\xa0\\n7 \\xa0 institution \\xa0 1 \\xa0 0 \\xa0 0 \\xa0 1 \\xa0 1/13 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/8 \\xa0\\n8 \\xa0 movie \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/13 \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 1/8 \\xa0\\n9 \\xa0 titanic \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/13 \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 1/8 \\xa0\\n10 \\xa0 story \\xa0 0 \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 1/13 \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 1/8 \\xa0\\n11 \\xa0 harvard \\xa0 1 \\xa0 0 \\xa0 0 \\xa0 1 \\xa0 1/13 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/8 \\xa0\\nTotal   Words   Count \\xa0 4 \\xa0 4 \\xa0 5 \\xa0 13 \\xa0 \\xa0 3 \\xa0 5 \\xa0 8 \\xa0 \\xa0\\n\\xa0\\nBag  of  Word  Representation  basically  breaks  down  the  sentences  available  into  words  and \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmakes  the  order  of  the  words  irrelevant  as  if  the  words  were  put  in  a  bag  and  shuﬄed.  All \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nthat  one  is  concerned  about  is  the  number  of  occurrences  of  the  words  present  (for \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nMultinomial   Naive   Bayes)   or   if   a   word   is   present   or   not   (for   Bernoulli   Naive   Bayes). \\xa0\\nMultinomial   Naive   Bayes   Classiﬁer \\xa0\\nThe   Multinomial   Naive   Bayes   Classiﬁer   is   given   by, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   56 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 55}),\n",
              " Document(page_content=\"Using  the  above  BOW  table  and  the  Multinomial  Naive  Bayes  classiﬁer  let's  classify  some \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ndocuments. \\xa0\\n\\xa0\\nDocument   :   “Great   Story” \\xa0\\nClass   :   education \\xa0 Class   :   cinema \\xa0 Result \\xa0\\nPrior   : \\xa0\\xa0\\nP(education)   =   3/5 \\xa0\\n\\xa0\\nLikelihood   : \\xa0\\nP(Great   |   education)   =   2/13 \\xa0\\nP(Story   |   education)   =   1/13 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\nLikelihood   x   Prior   =   0.007 \\xa0Prior   : \\xa0\\xa0\\nP(cinema)   =   2/5 \\xa0\\n\\xa0\\nLikelihood   : \\xa0\\xa0\\nP(Great   |   cinema)   =   1/8 \\xa0\\nP(Story   |   cinema)   =   1/8 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\nLikelihood   x   Prior   =   0.006 \\xa0As \\xa0\\nP(education   |   document)   > \\xa0\\nP(cinema   |   document) \\xa0\\n\\xa0\\nThus,   it   can   be   concluded \\xa0\\nthat   the   document   belongs   to \\xa0\\neducation \\xa0\\n\\xa0\\nDocument   :   “Good   Story” \\xa0\\nClass   :   education \\xa0 Class   :   cinema \\xa0 Result \\xa0\\nPrior   : \\xa0\\xa0\\nP(education)   =   3/5 \\xa0\\n\\xa0\\nLikelihood   : \\xa0\\nP(Good   |   education)   =   0/13 \\xa0\\nP(Story   |   education)   =   1/13 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\nLikelihood   x   Prior   =   0 \\xa0Prior   : \\xa0\\xa0\\nP(cinema)   =   2/5 \\xa0\\n\\xa0\\nLikelihood   : \\xa0\\xa0\\nP(Good   |   cinema)   =   2/8 \\xa0\\nP(Story   |   cinema)   =   1/8 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\nLikelihood   x   Prior   =   0.012 \\xa0As   can   be   seen, \\xa0\\none   has   encountered   the \\xa0\\nzero   probability   problem. \\xa0\\nIt   needs   to   be   solved. \\xa0\\n\\xa0\\nMultinomial   Laplace   Smoothing \\xa0\\nNow  having  encountered  the  zero  probability  problem  i.e.  the  probability  of  a  word  which  has  never \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nappeared  in  a  class  (though  it  may  have  appeared  in  the  dataset  in  another  class)  is  0,  one  needs  to \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nﬁx  the  same.  This  is  where  the  technique  Laplace  smoothing  comes  into  play.  In  this  technique  the \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nwords  with  frequency  0  in  a  particular  class  is  assigned  to  a  new  frequency  value  of  along  with \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0α\\xa0 \\xa0 \\xa0\\nadding  the  same  value  to  the  frequency  of  all  the  other  words  in  the  class.  This  in  turn  changes \\xa0\\xa0 \\xa0 \\xa0α\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nthe  probability  of  each  word  in  the  particular  class  to \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\n.  Taking  the  value  of and ( W) ( F r e q u e n c y o f W o r d )  (α S i z e o f V o c a b u l a r y) P = +α/ × \\xa0 \\xa0 \\xa0 \\xa0\\xa0α=1 \\xa0\\napplying   Laplace   Smoothing   to   it,   the   new   BOW   table   can   be   converted   as   below, \\xa0\\n\\xa0\\nVocabulary \\xa0 D0 \\xa0 D1 \\xa0 D2 \\xa0 Total \\xa0P(W|C) \\xa0D3 \\xa0 D4 \\xa0 Total \\xa0P(W|C) \\xa0\\n0 \\xa0 cinema \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/13 \\xa0 1 \\xa0 0 \\xa0 1+1 \\xa0 2/20 \\xa0\\n1 \\xa0 depends \\xa0 0 \\xa0 1 \\xa0 0 \\xa0 1+1 \\xa0 2/25 \\xa0 0 \\xa0 1 \\xa0 1+1 \\xa0 2/20 \\xa0\\n2 \\xa0 educational \\xa0 1 \\xa0 1 \\xa0 1 \\xa0 3+1 \\xa0 4/25 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/20 \\xa0\\n3 \\xa0 ethics \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 2+1 \\xa0 3/25 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/20 \\xa0\\n4 \\xa0 good \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/25 \\xa0 0 \\xa0 2 \\xa0 2+1 \\xa0 3/20 \\xa0\\n5 \\xa0 great \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 2+1 \\xa0 3/25 \\xa0 1 \\xa0 0 \\xa0 1+1 \\xa0 2/20 \\xa0\\n6 \\xa0 greatness \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 2+1 \\xa0 3/25 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/20 \\xa0\\n7 \\xa0 institution \\xa0 1 \\xa0 0 \\xa0 0 \\xa0 1+1 \\xa0 2/25 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/20 \\xa0\\n8 \\xa0 movie \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/25 \\xa0 0 \\xa0 1 \\xa0 1+1 \\xa0 2/20 \\xa0\\n9 \\xa0 titanic \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/25 \\xa0 1 \\xa0 0 \\xa0 1+1 \\xa0 2/20 \\xa0\\n10 \\xa0 story \\xa0 0 \\xa0 0 \\xa0 1 \\xa0 1+1 \\xa0 2/25 \\xa0 0 \\xa0 1 \\xa0 1+1 \\xa0 2/20 \\xa0\\n11 \\xa0 harvard \\xa0 1 \\xa0 0 \\xa0 0 \\xa0 1+1 \\xa0 2/25 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/20 \\xa0\\nTotal   Words   Count \\xa0 4 \\xa0 4 \\xa0 5 \\xa0 25 \\xa0 \\xa0 3 \\xa0 5 \\xa0 20 \\xa0 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   57 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 56}),\n",
              " Document(page_content=\"\\xa0\\nNow  using  the  new  modiﬁed  BOW  table  and  the  Multinomial  Naive  Bayes  classiﬁer  let's  classify \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nanother   document. \\xa0\\n\\xa0\\nDocument   :   “Pretty   Good   Story” \\xa0\\nClass   :   education \\xa0 Class   :   cinema \\xa0 Result \\xa0\\nPrior   :   P(education)   =   3/5 \\xa0\\n\\xa0\\nLikelihood   : \\xa0\\nP(Pretty   |   education)   =   Ignore \\xa0\\nP(Good   |   education)   =   1/25 \\xa0\\nP(Story   |   education)   =   2/25 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\nLikelihood   x   Prior   =   0.001 \\xa0Prior   :   P(cinema)   =   2/5 \\xa0\\n\\xa0\\nLikelihood   : \\xa0\\xa0\\nP(Pretty   |   cinema)   =   Ignore \\xa0\\nP(Good   |   cinema)   =   3/20 \\xa0\\nP(Story   |   cinema)   =   2/20 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\xa0\\nLikelihood   x   Prior   =   0.006 \\xa0As \\xa0\\nP(cinema   |   document)   > \\xa0\\nP(education   |   document) \\xa0\\n\\xa0\\nThus,   it   can   be   concluded \\xa0\\nthat   the   document   belongs   to \\xa0\\ncinema \\xa0\\n\\xa0\\nAs  can  be  seen  if  there  are  words  occurring  in  the  test  document  which  are  not  a  part  of  the \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nvocabulary,  then  they  will  not  be  considered  as  a  part  of  the  feature  vector  since  it  only  considers \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nthe   words   that   are   part   of   the   dictionary.   Thus,   these   new   words   are   to   be   completely   ignored. \\xa0\\nBernoulli   Naive   Bayes   Classiﬁer \\xa0\\nThe   Bernoulli   Naive   Bayes   Classiﬁer   is   given   by, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe  most  fundamental  diﬀerence  in  Bernoulli  Naive  Bayes  Classiﬁer  is  that  unlike  Multinomial  way \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nwhich  is  concerned  about  the  number  of  occurrences  of  the  word  in  the  class,  in  Bernoulli  one  is  just \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nconcerned  about  whether  the  word  is  present  or  not.  The  bag  of  words  representation  in  this  case  is \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\njust  a  0  or  1  rather  than  the  frequency  of  occurrence  of  the  word.  The  BOW  representation  for \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nBernoulli   Naive   Bayes   is   as   follows, \\xa0\\n\\xa0\\nVocabulary \\xa0 D0 \\xa0 D1 \\xa0 D2 \\xa0 Total \\xa0P(W|C) \\xa0D3 \\xa0 D4 \\xa0 Total \\xa0P(W|C) \\xa0\\n0 \\xa0 cinema \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/3 \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 1/2 \\xa0\\n1 \\xa0 depends \\xa0 0 \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 1/3 \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 1/2 \\xa0\\n2 \\xa0 educational \\xa0 1 \\xa0 1 \\xa0 1 \\xa0 3 \\xa0 3/3 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/2 \\xa0\\n3 \\xa0 ethics \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 2 \\xa0 2/3 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/2 \\xa0\\n4 \\xa0 good \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/3 \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 1/2 \\xa0\\n5 \\xa0 great \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 2 \\xa0 2/3 \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 1/2 \\xa0\\n6 \\xa0 greatness \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 2 \\xa0 2/3 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/2 \\xa0\\n7 \\xa0 institution \\xa0 1 \\xa0 0 \\xa0 0 \\xa0 1 \\xa0 1/3 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/2 \\xa0\\n8 \\xa0 movie \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/3 \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 1/2 \\xa0\\n9 \\xa0 titanic \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/3 \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 1/2 \\xa0\\n10 \\xa0 story \\xa0 0 \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 1/3 \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 1/2 \\xa0\\n11 \\xa0 harvard \\xa0 1 \\xa0 0 \\xa0 0 \\xa0 1 \\xa0 1/3 \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0/2 \\xa0\\n\\xa0\\nUsing   the   above   BOW   table   and   the   Bernoulli   Naive   Bayes   classiﬁer   let's   classify   some   documents. \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   58 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 57}),\n",
              " Document(page_content=\"Document   :   “Very   good   educational   institution” \\xa0\\nVocabulary \\xa0Class   :   education \\xa0 Class   :   cinema \\xa0 Result \\xa0\\nP(W|C) \\xa0 Likelihood \\xa0 P(W|C) \\xa0 Likelihood \\xa0\\nThe   word   Very   is \\xa0\\nnot   there   in   the \\xa0\\nvocabulary   so   it \\xa0\\nhas   been   ignored. \\xa0\\n\\xa0\\nAs   the   Likelihood \\xa0\\nof   word   good   is \\xa0\\nbecoming   0,   it \\xa0\\ngives   rise   to   the \\xa0\\n  zero   probability \\xa0\\nproblem. \\xa00 \\xa0 cinema \\xa0 0 \\xa0 0/3 \\xa0(0x0/3)+(1-0)(1-0/3) \\xa01/2 \\xa0 (0x1/2)+(1-0)(1-1/2) \\xa0\\n1 \\xa0 depends \\xa0 0 \\xa0 1/3 \\xa0 (0x1/3)+(1-0)(1-1/3) \\xa0 1/2 \\xa0 (0x1/2)+(1-0)(1-1/2) \\xa0\\n2 \\xa0educational \\xa01 \\xa0 3/3 \\xa0 (1x3/3)+(1-1)(1-3/3) \\xa0 0/2 \\xa0 (1x0/2)+(1-1)(1-0/2) \\xa0\\n3 \\xa0 ethics \\xa0 0 \\xa0 2/3 \\xa0(0x2/3)+(1-0)(1-2/3) \\xa00/2 \\xa0(0x0/2)+(1-0)(1-0/2) \\xa0\\n4 \\xa0 good \\xa0 1 \\xa0 0/3 \\xa0 (1x0/3)+(1-1)(1-0/3) \\xa0 1/2 \\xa0 (1x1/2)+(1-1)(1-1/2) \\xa0\\n5 \\xa0 great \\xa0 0 \\xa0 2/3 \\xa0(0x2/3)+(1-0)(1-2/3) \\xa0 1/2 \\xa0 (0x1/2)+(1-0)(1-1/2) \\xa0\\n6 \\xa0greatness \\xa00 \\xa0 2/3 \\xa0(0x2/3)+(1-0)(1-2/3) \\xa00/2 \\xa0(0x0/2)+(1-0)(1-0/2) \\xa0\\n7 \\xa0institution \\xa01 \\xa0 1/3 \\xa0 (1x1/3)+(1-1)(1-1/3) \\xa0 0/2 \\xa0 (1x0/2)+(1-1)(1-0/2) \\xa0\\n8 \\xa0 movie \\xa0 0 \\xa0 0/3 \\xa0(0x0/3)+(1-0)(1-0/3) \\xa01/2 \\xa0 (0x1/2)+(1-0)(1-1/2) \\xa0\\n9 \\xa0 titanic \\xa0 0 \\xa0 0/3 \\xa0(0x0/3)+(1-0)(1-0/3) \\xa01/2 \\xa0 (0x1/2)+(1-0)(1-1/2) \\xa0\\n10 \\xa0 story \\xa0 0 \\xa0 1/3 \\xa0 (0x1/3)+(1-0)(1-1/3) \\xa0 1/2 \\xa0 (0x1/2)+(1-0)(1-1/2) \\xa0\\n11 \\xa0 harvard \\xa0 0 \\xa0 1/3 \\xa0 (0x1/3)+(1-0)(1-1/3) \\xa0 0/2 \\xa0(0x0/2)+(1-0)(1-0/2) \\xa0\\n\\xa0Prior   :   P(education)   =   3/5 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\nLikelihood   x   Prior   =   0.001 \\xa0Prior   :   P(cinema)   =   2/5 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\xa0\\nLikelihood   x   Prior   =   0.006 \\xa0\\n\\xa0\\nBernoulli   Laplace   Smoothing \\xa0\\nNow  having  encountered  the  zero  probability  problem,  Laplace  smoothing  is  applied  to  the  BOW \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nrepresentation.  The  process  remains  the  same  as  in  case  of  Multinomial  Laplace  Smoothing,  but \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nwith   a   slight   change   in   the   formula,   which   is   given   as, \\xa0\\n  ( W) ( C o u n t o f W o r d )  ( D o c u m e n t s i n C l a s s 2) P = +α/ + \\xa0\\n\\xa0\\nVocabulary \\xa0 D0 \\xa0 D1 \\xa0 D2 \\xa0 Total \\xa0P(W|C) \\xa0D3 \\xa0 D4 \\xa0 Total \\xa0P(W|C) \\xa0\\n0 \\xa0 cinema \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/5 \\xa0 1 \\xa0 0 \\xa0 1+1 \\xa0 2/4 \\xa0\\n1 \\xa0 depends \\xa0 0 \\xa0 1 \\xa0 0 \\xa0 1+1 \\xa0 2/5 \\xa0 0 \\xa0 1 \\xa0 1+1 \\xa0 2/4 \\xa0\\n2 \\xa0 educational \\xa0 1 \\xa0 1 \\xa0 1 \\xa0 3+1 \\xa0 4/5 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/4 \\xa0\\n3 \\xa0 ethics \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 2+1 \\xa0 3/5 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/4 \\xa0\\n4 \\xa0 good \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/5 \\xa0 0 \\xa0 1 \\xa0 1+1 \\xa0 2/4 \\xa0\\n5 \\xa0 great \\xa0 1 \\xa0 0 \\xa0 1 \\xa0 2+1 \\xa0 3/5 \\xa0 1 \\xa0 0 \\xa0 1+1 \\xa0 2/4 \\xa0\\n6 \\xa0 greatness \\xa0 0 \\xa0 1 \\xa0 1 \\xa0 2+1 \\xa0 3/5 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/4 \\xa0\\n7 \\xa0 institution \\xa0 1 \\xa0 0 \\xa0 0 \\xa0 1+1 \\xa0 2/5 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/4 \\xa0\\n8 \\xa0 movie \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/5 \\xa0 0 \\xa0 1 \\xa0 1+1 \\xa0 2/4 \\xa0\\n9 \\xa0 titanic \\xa0 0 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/5 \\xa0 1 \\xa0 0 \\xa0 1+1 \\xa0 2/4 \\xa0\\n10 \\xa0 story \\xa0 0 \\xa0 0 \\xa0 1 \\xa0 1+1 \\xa0 2/5 \\xa0 0 \\xa0 1 \\xa0 1+1 \\xa0 2/4 \\xa0\\n11 \\xa0 harvard \\xa0 1 \\xa0 0 \\xa0 0 \\xa0 1+1 \\xa0 2/5 \\xa0 0 \\xa0 0 \\xa0 0+1 \\xa0 1/4 \\xa0\\n\\xa0\\nNow  using  the  new  modiﬁed  BOW  table  and  the  Bernoulli  Naive  Bayes  classiﬁer  let's  classify  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nsame   document. \\xa0\\n\\xa0\\nDocument   :   “Very   good   educational   institution” \\xa0\\nVocabulary \\xa0Class   :   education \\xa0 Class   :   cinema \\xa0 Result \\xa0\\nP(W|C) \\xa0 Likelihood \\xa0 P(W|C) \\xa0 Likelihood \\xa0 The   word   Very   is \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   59 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 58}),\n",
              " Document(page_content=\"0 \\xa0 cinema \\xa0 0 \\xa0 1/5 \\xa0 (0x1/5)+(1-0)(1-1/5) \\xa0 2/4 \\xa0(0x2/4)+(1-0)(1-2/4) \\xa0\\nnot   there   in   the \\xa0\\nvocabulary   so   it \\xa0\\nhas   been   ignored. \\xa0\\n\\xa0\\nAs \\xa0\\nP(education   | \\xa0\\ndocument) \\xa0\\xa0\\n> \\xa0\\n  P(cinema   | \\xa0\\ndocument) \\xa0\\n\\xa0\\nThus,   it   can   be \\xa0\\nconcluded   that \\xa0\\n  the   document \\xa0\\nbelongs   to \\xa0\\neducation \\xa0\\n\\xa01 \\xa0 depends \\xa0 0 \\xa0 2/5 \\xa0(0x2/5)+(1-0)(1-2/5) \\xa02/4 \\xa0(0x2/4)+(1-0)(1-2/4) \\xa0\\n2 \\xa0educational \\xa01 \\xa0 4/5 \\xa0 (1x4/5)+(1-1)(1-4/5) \\xa0 1/4 \\xa0 (1x1/4)+(1-1)(1-1/4) \\xa0\\n3 \\xa0 ethics \\xa0 0 \\xa0 3/5 \\xa0(0x3/5)+(1-0)(1-3/5) \\xa0 1/4 \\xa0 (0x1/4)+(1-0)(1-1/4) \\xa0\\n4 \\xa0 good \\xa0 1 \\xa0 1/5 \\xa0 (1x1/5)+(1-1)(1-1/5) \\xa0 2/4 \\xa0 (1x2/4)+(1-1)(1-2/4) \\xa0\\n5 \\xa0 great \\xa0 0 \\xa0 3/5 \\xa0(0x3/5)+(1-0)(1-3/5) \\xa02/4 \\xa0(0x2/4)+(1-0)(1-2/4) \\xa0\\n6 \\xa0greatness \\xa00 \\xa0 3/5 \\xa0(0x3/5)+(1-0)(1-3/5) \\xa0 1/4 \\xa0 (0x1/4)+(1-0)(1-1/4) \\xa0\\n7 \\xa0institution \\xa01 \\xa0 2/5 \\xa0 (1x2/5)+(1-1)(1-2/5) \\xa0 1/4 \\xa0 (1x1/4)+(1-1)(1-1/4) \\xa0\\n8 \\xa0 movie \\xa0 0 \\xa0 1/5 \\xa0 (0x1/5)+(1-0)(1-1/5) \\xa0 2/4 \\xa0(0x2/4)+(1-0)(1-2/4) \\xa0\\n9 \\xa0 titanic \\xa0 0 \\xa0 1/5 \\xa0 (0x1/5)+(1-0)(1-1/5) \\xa0 2/4 \\xa0(0x2/4)+(1-0)(1-2/4) \\xa0\\n10 \\xa0 story \\xa0 0 \\xa0 2/5 \\xa0(0x2/5)+(1-0)(1-2/5) \\xa02/4 \\xa0(0x2/4)+(1-0)(1-2/4) \\xa0\\n11 \\xa0 harvard \\xa0 0 \\xa0 2/5 \\xa0(0x2/5)+(1-0)(1-2/5) \\xa01/4 \\xa0(0x1/4)+(1-0)(1-1/42) \\xa0\\n\\xa0Prior   :   P(education)   =   3/5 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\nLikelihood   x   Prior   =   0.00027 \\xa0Prior   :   P(cinema)   =   2/5 \\xa0\\n\\xa0\\nPosterity   : \\xa0\\xa0\\nLikelihood   x   Prior   =   0.00018 \\xa0\\n\\xa0\\nGaussian   Naive   Bayes   Classiﬁer \\xa0\\nIt  is  used  to  classify  the  continuous  data.  While  dealing  with  continuous  data,  a  typical  assumption  is \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nthat  the  continuous  values  associated  with  each  class  are  distributed  according  to  a  normal  (or \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nGaussian)  distribution.  For  example,  suppose  the  training  data  contains  a  continuous  attribute  and \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 x\\xa0\\xa0\\nthe  data  is  segmented  into  classes.  If  is  the  mean  and  is  the  variance  for  values  in \\xa0 \\xa0\\xa0 \\xa0 \\xa0 K\\xa0 \\xa0\\xa0μk\\xa0\\xa0\\xa0 \\xa0 \\xa0σk\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 x\\xa0\\nassociated  with  class ,  then  for  an  observation  the  probability  distribution  of  given  a  class \\xa0 \\xa0 \\xa0 Ck\\xa0 \\xa0\\xa0\\xa0 \\xa0 v\\xa0\\xa0 \\xa0 \\xa0\\xa0 v\\xa0 \\xa0\\xa0 \\xa0\\n  is   given   by, Ck \\xa0\\n\\xa0\\nPython   Code   -   Naive   Bayes \\xa0\\nDocument   Processing  \\n>>   train_docs[ 'Class' ]   =   train_docs.Class.map({ 'class_1' :0,    'class_2' :1})  \\n>>   train_array   =   train_docs.values  \\n>>   X_train   =   train_array[:,0]  \\n>>   y_train   =   train_array[:,1].astype( 'int' )  \\n \\nBag   of   Words   REpresentation  \\n>>    from    sklearn.feature_extraction.text    import    CountVectorizer  \\n>>   vec   =   CountVectorizer(stop_words= 'english' )  \\n>>   vec.fit(X_train)  \\n>>   vocabulary   =   vec.vocabulary_  \\n>>   features   =   vec.get_feature_names()  \\n>>   X_transformed   =   vec.transform(X_train)  \\n#   Compressed   sparse   matrix  \\n>>   matrix   =   X_transformed.toarray()  \\n#   Sparse   matrix  \\n>>   pd.DataFrame(X_transformed.toarray(),   columns=features)  \\n#   Converting   matrix   to   dataframe  \\n \\nTrain   Model  \\n>>    from    sklearn.naive_bayes    import    MultinomialNB  \\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   60 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 59}),\n",
              " Document(page_content='#   Multinomial   Naive   Bayes  \\n>>   model   =   MultinomialNB()  \\n>>   model.fit(X_transformed,   y_train)  \\n \\n>>    from    sklearn.naive_bayes    import    BernoulliNB  \\n#   Bernoulli   Naive   Bayes  \\n>>   model   =   BernoulliNB()  \\n>>   model.fit(X_train_transformed,y_train)  \\n \\nPredict  \\n>>   test_docs[ \\'Class\\' ]   =   test_docs.Class.map({ \\'class_1\\' :0,    \\'class_2\\' :1})  \\n>>   test_array   =   test_docs.values  \\n>>   X_test   =   test_array[:,0]  \\n>>   y_test   =   test_array[:,1]  \\n>>   X_test_transformed   =   vec.transform(X_test)  \\n>>   X_test   =   X_test_transformed.toarray()  \\n>>   y_pred_class   =   model.predict(X_test_tranformed)  \\n#   Predict   Class  \\n>>   y_pred_proba   =model.predict_proba(X_test_tranformed)  \\n#   Predict   probability  \\n \\nAnalyze   Model  \\n>>    from    sklearn    import    metrics  \\n>>   print( \"PRECISION   SCORE   :\" ,metrics.precision_score(y_test,   y_pred_class))  \\n>>   print( \"RECALL   SCORE   :\" ,   metrics.recall_score(y_test,   y_pred_class))  \\n>>   print( \"F1   SCORE   :\" ,metrics.f1_score(y_test,   y_pred_class)) \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   61 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 60}),\n",
              " Document(page_content='4.3.   SUPPORT   VECTOR   MACHINES \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nSUPPORT   VECTOR   MACHINES \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   62 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 61}),\n",
              " Document(page_content='4.3.1.   MAXIMAL   MARGIN   CLASSIFIER \\xa0\\nThe  well-known  classiﬁcation  models,  i.e.  logistic  regression  and  naive  Bayes  though  widely  used  as \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ndefault  prediction  and  text  classiﬁcation,  they  ﬁnd  limited  use  in  more  complex  classiﬁcation \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nproblems,  such  as  image  classiﬁcation.  But  Support  Vector  Machine  models  are  more  capable  of \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\ndealing  with  such  complex  problems,  where  models  such  as  logistic  regression  typically  fail.  SVMs \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nare  mostly  used  for  classiﬁcation  tasks,  but  they  can  also  be  used  for  regression.  SVMs  belong  to \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nthe  class  of  linear  machine  learning  models  (logistic  regression  is  also  a  linear  model).  The  model \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nalso   requires   all   its   attributes   to   be   numeric. \\xa0\\nHyperplanes \\xa0\\nEssentially,  it  is  a  boundary  which  separates  the  data  set  into  its  classes.  It  could  be  lines,  2D  planes, \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nor  even  n-dimensional  planes.  In  general,  the  hyperplane  in  2D  can  be  represented  by  a  line, \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nwhereas   a   hyperplane   in   3D   can   be   represented   by   a   plane   as   shown   in   the   ﬁgure   below. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe  dimension  of  a  hyperplane  is  always  equal  to  the .  If  the  value  of  any \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 u m b e r o f f e a t u r e s n −1\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\npoint  is  positive  it  would  mean  that  the  set  of  values  of  the  features  is  in  one  class;  however,  if  it  is \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\nnegative  then  it  belongs  to  the  other  class.  A  value  of  zero  would  imply  that  the  point  lies  on  the \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\nhyperplane. \\xa0\\nLinear   Discriminator \\xa0\\nThe   generalized   equation   of   the   hyperplane   is   given   as, \\xa0\\n\\xa0\\nMaximal   Margin   Classiﬁer \\xa0\\nThere  can  be  multiple  possible  hyperplanes,  which  perfectly  divide  any  two  classes.  But  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\noptimum  hyperplane  is  the  one  that  maintains  the  largest  possible  equal  distance  from  the  nearest \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\npoints  of  both  the  classes.  This  is  also  referred  to  as  a  maximal  margin  classiﬁer.  One  can  think  of \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe   margin   as   a   band   that   the   hyperplane   has   on   both   its   sides. \\xa0\\n\\xa0\\nThe   distance   of   any   data   point   from   a   hyperplane   is   given   by, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   63 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 62}),\n",
              " Document(page_content='If  the  training  data  is  linearly  separable,  then  two  parallel  hyperplanes  can  be  selected,  which \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nseparate  the  two  classes  of  data,  such  that  the  distance  between  them  is  as  large  as  possible.  The \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nregion  bounded  by  these  two  hyperplanes  is  called  the  margin ,  and  the  Maximum  Margin \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 M\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nHyperplane  is  the  hyperplane  that  lies  halfway  between  these  two  parallel  hyperplanes.  Also  it \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nneeds  to  be  ensured  that  each  data  point  must  lie  on  the  correct  side  of  the  margin.  This  can  be \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nmathematically   expressed   as, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nwhere,     is   the   label   of   the     datapoint   (such   as   Class_1   (+1)   &   Class_2   (-1)) yi it h\\xa0\\n\\xa0\\nThus,   the   ﬁnal   optimisation   problem   becomes, \\xa0\\n\\xa0\\nThe   following   ﬁgure   explains   it   geometrically. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nMaximal  Margin  Classiﬁer  is  possible  only  on  datasets  which  are  perfectly  linearly  separable,  so  it \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nhas  a  rather  limited  applicability.  Thus,  in  order  to  classify  data  points  which  are  partially \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nintermingled   one   cannot   use   this   classiﬁer. \\xa0\\nSupport   Vectors \\xa0\\nOne  can  observe  in  the  above  geometric  diagram,  that  the  Maximum  Margin  Hyperplane  is \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ncompletely  determined  by  the ,  which  lie  nearest  to  it.  These ’s  are  called  support  vectors.  Thus, \\xa0 \\xa0\\xa0 xi\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0  xi\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nthe  support  vectors  are  the  points  that  lie  close  to  the  hyperplane  and  are  the  only  points  that  are \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nused   in   constructing   the   hyperplane. \\xa0\\n4.3.2.   SOFT   MARGIN   CLASSIFIER \\xa0\\nThe  hyperplane  that  allows  certain  points  to  be  deliberately  misclassiﬁed  is  called  the  Soft  Margin \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nClassiﬁer  (also  called  Support  Vector  Classiﬁer).  Similar  to  the  Maximal  Margin  Classiﬁer,  the  Soft \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nMargin  Classiﬁer  also  maximises  the  margin,  but  at  the  same  time  allows  some  points  to  be \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nmisclassiﬁed. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   64 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 63}),\n",
              " Document(page_content='Slack   Variable \\xa0\\nIn  order  to  allow  certain  points  to  be  deliberately  misclassiﬁed,  one  has  to  include  a  new  variable \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ncalled  as  slack  variable .  A  slack  variable  is  used  to  control  the  misclassiﬁcations.  It  tells  where \\xa0\\xa0 \\xa0 \\xa0ε\\xa0\\xa0 \\xa0 \\xa0ε\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nan  observation  is  located  relative  to  the  margin  and  hyperplane.  The  following  ﬁgure  explains  it \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ngeometrically. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n1)For  data  points  which  are  at  a  distance  of  more  than  the  margin ,  i.e.  at  a  safe  distance \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 M\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\nfrom   the   hyperplane,   the   value   of   the   slack   variable   is   0. \\xa0\\n2)For  data  points  correctly  classiﬁed  but  falling  inside  the  margin (or  violating  the  margin), \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 M \\xa0 \\xa0\\xa0 \\xa0\\nthe   value   of   the   slack   variable   lies   between   0   and   1. \\xa0\\xa0\\n3)For  data  points  incorrectly  classiﬁed  (i.e.  it  violating  the  hyperplane),  the  value  of  slack \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nvariable   is   greater   than   1. \\xa0\\n\\xa0\\nThus,  the  slack  variable  takes  a  value  between  0  to  inﬁnity.  Lower  values  of  slack  are  better  than \\xa0\\xa0 \\xa0 \\xa0ε\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nhigher  values  (  correct  classiﬁcation,  incorrect  classiﬁcation,  classiﬁes  correctly \\xa0 \\xa0ε=0\\xa0 \\xa0 \\xa0ε>1\\xa0 \\xa0 \\xa00<ε<1\\xa0 \\xa0 \\xa0\\nbut   violates   the   margin). \\xa0\\xa0\\nSoft   Margin   Classiﬁer \\xa0\\nIt   has   already   been   proven   that   the   Maximal   Margin   Classiﬁer   can   be   expressed   as, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nIf  the  above  condition  is  imposed  on  any  model,  then  it  implies  that  each  point  in  the  dataset  has  to \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nbe  at  least  a  distance  of  away  from  the  hyperplane.  But  unfortunately,  hardly  any  available  real \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 M\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ndatasets   are   so   easily   and   perfectly   separable. \\xa0\\xa0\\n\\xa0\\nThus,  to  relax  the  constraint,  one  has  to  include  the  slack  variable  for  each  datapoint ,  modifying \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0εi\\xa0\\xa0 \\xa0 \\xa0 i\\xa0 \\xa0\\nthe   above   expression   to, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nWith   the   inclusion   of   slack   variable   ,   the   optimisation   problem   now   becomes, ε \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   65 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 64}),\n",
              " Document(page_content=\"\\xa0\\nCost   of   Misclassiﬁcation   ‘C’ \\xa0\\nIn  the  equation ,  given  above  is  the  cost  of  misclassiﬁcation  and  is  equal  to  the  sum  of  all \\xa0\\xa0 \\xa0εΣi< C\\xa0 \\xa0 \\xa0 C\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nthe  values  of  slack  variables,  i.e  it  represents  the  cost  of  violations  to  the  margin  and  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nhyperplane.  Using  the  notion  of  the  slack  variable,  one  can  easily  compare  any  number  of  Support \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nVector  Classiﬁers.  The  cost  of  misclassiﬁcation  can  be  measured  for  the  hyperplanes  of  all  the \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 C\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\nclassiﬁers,   and   the   one   with   the   least   sum   of   epsilons   be   chosen   as   the   best   ﬁt   classiﬁer. \\xa0\\n\\xa0\\nWhen  is  large,  the  slack  variables  can  be  large,  i.e.  larger  number  of  data  points  are  allowed  to  be \\xa0 C\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nmisclassiﬁed  or  to  violate  the  margin.  Thus,  one  gets  a  hyperplane  where  the  margin  is  wide  and \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nmisclassiﬁcations  are  allowed.  These  models  tend  to  be  more  ﬂexible,  more  generalisable,  and  less \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nlikely  to  overﬁt  (high  bias).  On  the  other  hand,  when  is  small,  the  individual  slack  variables  are \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 C\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nforced  to  be  small,  i.e.  not  many  data  points  are  allowed  to  fall  on  the  wrong  side  of  the  margin  or \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nthe  hyperplane.  Thus,  the  margin  is  narrow  and  there  are  few  misclassiﬁcations.  These  models  tend \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nto   be   less   ﬂexible,   less   generalisable,   and   more   likely   to   overﬁt   (high   variance). \\xa0\\nPython   Code   -   SVM \\xa0\\nSVM  \\n>>    from    sklearn.svm    import    SVC  \\n>>   model   =   SVC(C   =   1)  \\n#   Hyperparameter   C   used   in   the   SVM   formulation   (in   theory)   and   the   C   in   the   SVC()  \\nfunctions   are   the   inverse   of   each   other.   C   is   analogous   to   the   penalty   imposed   for  \\nmisclassification,   i.e.   a   higher   C   will   force   the   model   to   classify   most   (training)  \\ndata   points   correctly   (and   thus,   overfit).  \\n>>   model.fit(X_train,   y_train)  \\n \\nGrid   Search   SVM  \\n>>   hyper_params   =   [{ 'C' :   [parameters]}]  \\n>>   model   =   SVC()  \\n>>   model_cv   =   GridSearchCV(estimator=model,   param_grid=hyper_params,   cv=folds,  \\n                            scoring= 'accuracy' ,   verbose=1,   return_train_score= True )  \\n>>   model_cv.fit(X_train,   y_train)  \\n>>   scores   =   pd.DataFrame(model_cv.cv_results_)  \\n \\n4.3.3.   KERNELS \\xa0\\nSometimes  it  is  not  possible  to  imagine  a  linear  hyperplane  that  can  separate  the  classes  reasonably \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nwell.  So,  one  needs  to  tweak  the  linear  SVM  model  and  enable  it  to  incorporate  nonlinearity  in  some \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nway.  The  Kernels  serve  this  purpose,  i.e  they  enable  the  linear  SVM  model  to  separate  non  linearly \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nseparable  data  points.  The  Kernels  do  not  change  the  linearity  of  the  SVM  model,  rather  these  are \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ntoppings   over   the   linear   SVM   model,   which   somehow   enables   the   model   to   separate   nonlinear   data. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   66 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 65}),\n",
              " Document(page_content=\"Feature   Transformation \\xa0\\n\\xa0\\nA  nonlinear  boundary  can  be  transformed  into  a  linear  boundary  by  applying  certain  functions  to  the \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\noriginal  attributes  to  create  new  features.  The  original  space  (X,  Y)  is  called  the  original  attribute \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nspace,   and   the   transformed   space   (X’,   Y’)   is   called   the   feature   space. \\xa0\\xa0\\n\\xa0\\nThe  preceding  diagram  represents  one  such  transformation.  Now  in  the  above  example,  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ntransformation  was  very  neat  as  the  shape  of  the  separator  was  known  to  be  a  circle.  But  in  real \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nscenario  it  is  going  to  be  very  tough  to  guess  the  shape  (functional  form)  of  the  separator,  other  than \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nthat  it  is  linear  or  nonlinear.  Thus,  one  has  to  eventually  go  for  a  degree  polynomial  transformation \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 n\\xa0 \\xa0 \\xa0 \\xa0\\nfor     number   of   attributes. m \\xa0\\xa0\\n\\xa0\\nFor  example  let’s  consider  a  2  degree  polynomial  transformation  of  3  attributes.  Solving  this,  one \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nends   up   with   10   features   in   the   new   feature   space   as   shown   below. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThus,  it  can  be  concluded  that  as  the  number  of  attributes  increases,  the  number  of  dimensions  in \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe   transformed   feature   space   also   increases   exponentially. \\xa0\\nThe   Kernel   Trick \\xa0\\nDue  to  the  exponential  rise  in  the  number  of  the  dimensions  during  feature  transformation,  the \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nmodelling  (i.e.  the  learning  process)  becomes  computationally  expensive.  This  problem  is  solved  by \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe  use  of  the  Kernel  trick.  The  kernels  don't  do  the  feature  transformation  explicitly  (which  is  a \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\ncomputationally  diﬃcult  task,  as  discussed  above),  rather  they  use  a  mathematical  hack  to  do  this \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\nimplicitly. \\xa0\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   67 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 66}),\n",
              " Document(page_content='It  is  a  mathematical  fact  (Dual  Space  Optimisation  Theory)  that  for  all  the  linear  models  in  order  to \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nﬁnd  a  best  ﬁt  model,  the  learning  algorithm  only  requires  the  pairwise  dot  product  of  the \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nobservations   ,   rather   than   individual   data   points   or   . x. x) (iT \\nj xi xj\\xa0\\xa0\\nReplacing  these  attribute  vectors  with  their  corresponding  feature  vectors  gives  the  function  as \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 K\\xa0\\xa0\\ndeﬁned   below, \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThis  function  is  known  as  the  Kernel  function.  Thus,  one  just  needs  to  know  only  the  Kernel \\xa0 \\xa0 K\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nfunction  and  not  the  mapping  function  itself,  for  transforming  the  non-linear  attributes  into  linear \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nfeatures.   The   beneﬁts   of   this   implicit   transformation   are   as   below. \\xa0\\n\\xa0\\n1)Manually  ﬁnding  the  mathematical  transformation  is  now  not  required  to  convert  a  nonlinear \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nto   a   linear   feature   space. \\xa0\\n\\xa0\\n2)Computationally   heavy   transformations   are   no   longer   required. \\xa0\\n\\xa0\\nSome   of   the   most   popular   types   of   kernel   functions   are   given   below. \\xa0\\n\\xa0\\n1)Linear  Kernel  :  This  is  the  same  as  the  support  vector  classiﬁer,  or  the  hyperplane,  without \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nany   transformation   at   all. \\xa0\\xa0\\n\\xa0\\n2)Polynomial   Kernel   :   It   is   capable   of   creating   nonlinear,   polynomial   decision   boundaries. \\xa0\\n\\xa0\\n3)Radial  Basis  Function  Kernel  (RBF)  :  This  is  the  most  complex  one,  which  is  capable  of \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ntransforming  highly  nonlinear  feature  spaces  to  linear  ones.  It  is  even  capable  of  creating \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nelliptical   (i.e.   enclosed)   decision   boundaries. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThus,  one  can  think  of  a  kernel  as  a  black  box.  The  original  attributes  are  passed  into  the  black  box, \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nand  it  returns  the  transformed  attributes  (in  a  higher  dimensional  feature  space).  The  SVM  algorithm \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nis  then  shown  only  the  transformed,  linear  feature  space,  where  it  builds  the  linear  classiﬁer  as \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nusual. \\xa0\\nChoosing   a   Kernel   Function \\xa0\\nChoosing  the  appropriate  kernel  is  very  important  for  building  a  model  of  optimum  complexity.  If  the \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nkernel  is  highly  nonlinear,  the  model  is  likely  to  overﬁt.  On  the  other  hand,  if  the  kernel  is  too  simple, \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nthen  it  may  not  ﬁt  the  training  data  well.  Usually,  it  is  diﬃcult  to  choose  the  appropriate  kernel  by \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nvisualising  the  data  or  using  exploratory  analysis.  Thus,  cross-validation  (or  hit-and-trial,  if  2-3  types \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nof   kernels   are   chosen)   is   often   a   good   strategy. \\xa0\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   68 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 67}),\n",
              " Document(page_content='For  the  polynomial,  rbf  and  sigmoid  kernels,  the  hyperparameter  gamma  controls  the  amount  of \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nnon-linearity  in  the  model.  As  gamma  increases,  the  model  becomes  more  non-linear,  and  thus \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmodel  complexity  increases.  High  values  of  gamma  lead  to  overﬁtting  (especially  for  higher  values \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nof   C). \\xa0\\nPython   Code   -   Kernel   SVM \\xa0\\nKernel   SVM  \\n>>    from    sklearn.svm    import    SVC  \\n>>   model   =   SVC(C=1,   kernel= \\'linear\\' )  \\n#   Kernels   can   be   any   one   of    \\'linear\\',   \\'poly\\',   \\'rbf\\',   \\'sigmoid\\'   or    \\'precomputed\\'.  \\n>>   model.fit(X_train,   y_train)  \\n \\nGrid   Search   Kernel   SVM  \\n>>   hyper_params   =   [{ \\'C\\' :[parameters],    \\'gamma\\' :[parameters]}]  \\n>>   model   =   SVC(kernel= \"rbf\" )  \\n#   hyperparameter   gamma   is   available   for   \\'poly\\',   \\'rbf\\'   and   \\'sigmoid\\'   kernel  \\n>>   model_cv   =   GridSearchCV(estimator=model,   param_grid=hyper_params,   cv=folds,  \\n                            scoring= \\'accuracy\\' ,   verbose=1,   return_train_score= True )  \\n>>   model_cv.fit(X_train,   y_train)  \\n>>   scores   =   pd.DataFrame(model_cv.cv_results_) \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   69 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 68}),\n",
              " Document(page_content='4.4.   TREE   MODELS \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nTREE   MODELS \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   70 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 69}),\n",
              " Document(page_content='4.4.1.   DECISION   TREES \\xa0\\nWith  high  interpretability  and  an  intuitive  algorithm,  decision  trees  mimic  the  human  decision  making \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nprocess  and  excel  in  dealing  with  categorical  data.  Unlike  other  algorithms  like  logistic  regression  or \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nSVMs,  decision  trees  do  not  ﬁnd  a  linear  relationship  between  the  independent  and  the  target \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nvariable,  rather,  they  can  be  used  to  model  highly  nonlinear  data.  With  decision  trees,  one  can  easily \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nexplain  all  the  factors  leading  to  a  particular  decision/prediction.  Hence,  they  are  easily  understood \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nby   business   people. \\xa0\\nDecision   Trees \\xa0\\nA  decision  tree  uses  a  tree-like  model  (resembling  an  upside-down  tree)  to  make  predictions.  It  is \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nalso  very  similar  to  the  decision  making  in  real  life  (asking  a  series  of  questions  in  a  nested \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nif-then-else  structure  to  arrive  at  a  decision).  A  decision  tree  splits  the  data  into  multiple  sets.  Then, \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\neach   of   these   sets   is   further   split   into   subsets   to   arrive   at   a   decision. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nIf  a  test  splits  the  data  into  two  partitions  it  is  called  a  binary  decision  tree  and  if  it  splits  the  data  into \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nmore  than  two  partitions,  it  is  called  a  multiway  decision  tree.  The  decision  trees  are  easy  to \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ninterpret   (i.e.   almost   always,   one   can   identify   the   various   factors   that   lead   to   the   decision). \\xa0\\nRegression   with   Decision   Trees \\xa0\\nThere  are  cases  where  one  cannot  directly  apply  linear  regression  to  solve  a  regression  problem. \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nThe  linear  regression  ﬁts  only  one  model  to  the  entire  data  set.  But  there  may  be  cases  where  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nrequirement  might  be  to  split  the  data  set  into  multiple  subsets  and  then  apply  the  linear  regression \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nto  each  subset  separately.  This  can  be  done  by  using  the  decision  tree  to  split  the  data  into  multiple \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nsubsets  ﬁrst.  Then  a  linear  regression  can  be  applied  to  each  leaf.  The  diﬀerence  between  decision \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ntree  classiﬁcation  and  decision  tree  regression  is  that  in  regression,  each  leaf  represents  a  linear \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nregression   model,   as   opposed   to   a   class   label. \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   71 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 70}),\n",
              " Document(page_content='Python   Code   -   Decision   Tree   Classiﬁcation \\xa0\\nCreate   Train   Data  \\n>>   X   =   df.drop( \\'dependent_variable_column\\' ,axis=1)  \\n>>   y   =   df[ \\'dependent_variable_column\\' ]  \\n \\nTrain   Model  \\n>>    from    sklearn.tree    import    DecisionTreeClassifier  \\n>>   model   =   DecisionTreeClassifier(max_depth=level_of_depth)  \\n>>   model.fit(X,   y)  \\n \\nVisualize   Decision   Tree   Structure  \\n>>    from    IPython.display    import    Image   \\n>>    from    sklearn.externals.six    import    StringIO   \\n>>    from    sklearn.tree    import    export_graphviz  \\n>>    import    pydotplus,   graphviz  \\n>>   features   =   list(X.columns[0:])  \\n>>   dot_data   =   StringIO()   \\n>>   export_graphviz(model,   out_file=dot_data,   feature_names=features,  \\n                    filled= True ,   rounded= True )  \\n>>   graph   =   pydotplus.graph_from_dot_data(dot_data.getvalue())   \\n>>   graph.write_pdf( \"decision_tree_structure.pdf\" )  \\n#   Creates   the   pdf   with   the   tree   in   the   current   directory  \\n \\n4.4.2.   ALGORITHMS   FOR   DECISION   TREE   CONSTRUCTION \\xa0\\nHomogeneity \\xa0\\nA  partition  which  contains  data  points  with  all  the  labels  identical  (i.e.  label ),  then  the  entire \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 X\\xa0 \\xa0\\xa0 \\xa0\\npartition  can  be  classiﬁed  as  label  (i.e.  homogeneous  data  set).  But,  in  real  world  data  sets,  one \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 X\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nnever  gets  a  completely  homogeneous  data  set  (or  even  nodes  after  splitting).  Thus,  one  always \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ntries  the  best  to  split  the  nodes  such  that  the  resulting  nodes  are  as  homogenous  as  possible  (i.e. \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe  generated  partitions  result  in  homogeneous  data  points).  For  classiﬁcation  tasks,  a  data  set  is \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\ncompletely  homogeneous  if  it  contains  only  a  single  class  label.  The  following  ﬁgure  shows  the \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ngeneric   algorithm   followed   by   the   decision   trees. \\xa0\\n\\xa0\\n\\xa0\\nThe  algorithm  goes  on  step  by  step,  picking  an  attribute  and  splitting  the  data  such  that  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nhomogeneity  increases  after  every  split.  Usually,  the  attribute  that  results  in  the  maximum  increase \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nin  homogeneity  is  chosen  for  splitting.  A  split  that  gives  a  homogenous  subset  is  much  more \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ndesirable  than  the  one  that  results  in  a  50-50  distribution  (in  the  case  of  two  labels).  The  splitting  is \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nstopped  when  the  resulting  leaves  are  suﬃciently  homogenous.  Here,  suﬃciently  homogeneous \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmeans  that  most  of  the  data  points  in  the  set  should  belong  to  the  same  class  label.  Thus,  the \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ndecision  tree  construction  problem  gets  narrowed  down  to,  splitting  of  data  set  such  that  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nhomogeneity   of   the   resulting   partitions   is   maximum. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   72 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 71}),\n",
              " Document(page_content='Gini   Index \\xa0\\nGini  index  is  a  measure  of  homogeneity  that  measures  the  degree  or  probability  of  a  particular \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nvariable  being  correctly  classiﬁed  when  it  is  randomly  chosen,  i.e.  if  all  the  elements  belong  to  a \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nsingle  class,  then  it  can  be  called  pure.  The  degree  of  Gini  index  varies  between  0  and  1,  where  0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\ndenotes  that  the  elements  are  randomly  distributed  across  various  classes  and  1  denotes  that  all \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nelements  belong  to  a  certain  class.  A  Gini  Index  of  0.5  denotes  equally  distributed  elements  into \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nsome   classes.   Thus,   the   higher   the   homogeneity,   the   higher   the   Gini   index. \\xa0\\xa0\\n\\xa0\\nGini   index   of   a   partition     with     labels   is   given   by, D n \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nWhile  calculating  the  Gini  index  of  an  attribute  after  splitting,  one  has  to  multiply  the  Gini  index  of  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\npartition  with  the  fraction  of  data  points  of  the  respective  partition.  Gini  index  for  an  attribute \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 A\\xa0\\nhaving     partitions   is   given   by, n \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nBelow  is  an  example  where  one  has  to  make  a  choice  between  two  attributes  for  splitting,  Age  and \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nGender.  On  computing,  it  was  found  that  Gender  is  a  better  attribute  to  split  on  as  it  yields  a  higher \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\nvalue  of  Gini  index  as  compared  to  Age.  This  means  that  gender  gives  a  better  split  that  helps  in \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ndistinguishing  between  football  players  and  non-football  players.  This  is  intuitive  as  well,  splitting  on \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\ngender  is  expected  to  be  more  informative  than  age  because  football  is  usually  more  popular \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\namong   males.   Thus,   one   can   split   the   dataset   on   the   attribute   Gender. \\xa0\\n\\xa0\\n\\xa0\\nEntropy \\xa0\\nThe  idea  here  is  to  use  the  notion  of  entropy  (lack  of  order  or  predictability)  for  measuring  the \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nhomogeneity.  Entropy  quantiﬁes  the  degree  of  disorder  in  the  data.  Similar  to  the  Gini  index,  its \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nvalue  also  varies  from  0  to  1.  If  a  data  set  is  completely  homogenous,  then  the  entropy  of  such  a \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\ndata  set  is  0,  i.e.  there’s  no  disorder.  The  lower  the  entropy  (or  higher  the  Gini  index),  the  lesser  the \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ndisorder,  and  the  greater  homogeneity.  One  needs  to  note  that  Entropy  is  a  measure  of \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\ndisorderness,   while   the   Gini   index   is   a   measure   of   homogeneity   in   the   data   set. \\xa0\\n\\xa0\\nEntropy   of   partition     with     labels   is   given   by, D n \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   73 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 72}),\n",
              " Document(page_content='While  calculating  the  Entropy  of  an  attribute  after  splitting,  one  has  to  multiply  the  Entropy  of  the \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\npartition  with  the  fraction  of  data  points  of  the  respective  partition.  Entropy  for  an  attribute  having \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 A\\xa0 \\xa0\\n  partitions   is   given   by, n \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nBelow  is  the  same  example  used  for  Gini  index,  where  one  has  to  make  a  choice  between  two \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nattributes  for  splitting,  Age  and  Gender.  On  computing,  it  was  found  that  Gender  is  a  better  attribute \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nto  split  on  as  it  yields  a  lower  value  of  Entropy  as  compared  to  Age.  Thus,  one  can  split  the  dataset \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\non   the   attribute   Gender. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nInformation   Gain \\xa0\\nInformation  gain  is  another  measure  of  homogeneity  which  is  based  on  the  decrease  in  entropy \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\nafter  a  data  set  is  split  on  an  attribute.  It  measures  by  how  much  entropy  has  decreased  between \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nthe  parent  set  and  the  partitions  obtained  after  splitting.  Information  gain  of  an  attribute  with  a \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 A\\xa0 \\xa0\\xa0\\npartition     is   given   by, D \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nAs  the  information  gain  is  equal  to  the  entropy  change  from  the  parent  set  to  the  partitions,  it  is \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\nmaximum  when  the  entropy  of  the  parent  set  minus  the  entropy  of  the  partitions  is  maximum.  Thus, \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nwhile   splitting   one   should   choose   an   attribute   such   that   the   information   gained   is   maximum. \\xa0\\nChi-Square \\xa0\\nIt  measures  the  signiﬁcance  of  the  relationship  between  sub-nodes  and  parent  node.  Chi-Square  of \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nan   attribute     with   a   partition     is   given   by, A D \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   74 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 73}),\n",
              " Document(page_content='Below  is  the  same  example  used  for  Gini  index,  where  one  has  to  make  a  choice  between  two \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nattributes  for  splitting,  Age  and  Gender.  On  computing,  it  was  found  that  Gender  is  a  better  attribute \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nto  split  on  as  it  yields  a  higher  Chi-squared  value  as  compared  to  Age.  Thus,  one  can  split  the \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ndataset   on   the   attribute   Gender. \\xa0\\n\\xa0\\n\\xa0\\nR-squared   Splitting \\xa0\\nSo  far  the  splitting  has  been  done  only  on  discrete  target  variables.  But,  the  splitting  can  also  be \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\ndone  for  continuous  output  variables  too.  One  can  calculate  the  R-squared  values  of  the  data  sets \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\n(before  and  after  splitting)  in  a  similar  manner  to  what  is  done  for  linear  regression  models.  The  data \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nis  split  such  that  the  R-squared  of  the  partitions  obtained  after  splitting  is  greater  than  that  of  the \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\noriginal  or  parent  data  set.  In  other  words,  the  ﬁt  of  the  model  should  be  as  good  as  possible  after \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nsplitting. \\xa0\\xa0\\n\\xa0\\n\\xa0\\nGeneric   Algorithm   for   Decision   Tree   Construction \\xa0\\nBelow   are   the   following   steps   involved   in   decision   tree   construction. \\xa0\\n\\xa0\\n1)The   decision   tree   ﬁrst   decides   on   an   attribute   to   split   on. \\xa0\\n\\xa0\\n2)For  the  selection  of  the  attribute,  it  measures  the  homogeneity  of  the  nodes  before  and  after \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nthe  split  using  various  measures  such  as  Gini  Index,  Entropy,  Information  Gain,  Chi-square, \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nR-squared   (for   regression   models)   etc. \\xa0\\n\\xa0\\n3)The   attribute   with   maximum   homogeneous   data   set   is   then   selected   for   splitting. \\xa0\\n4)This   whole   cycle   is   repeated   till   one   gets   a   suﬃciently   homogeneous   data   set. \\xa0\\n\\xa0\\nThere   are   two   widely   used   Decision   Tree   algorithms   for   diﬀerent   applications, \\xa0\\n\\xa0\\n1)CART  (Classiﬁcation  and  Regression  Trees)  :  It  creates  a  binary  tree  (a  tree  with  a  maximum \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nof  two  child  nodes  for  any  node  in  the  tree)  using  measures  such  as  Gini  Index,  Entropy, \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   75 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 74}),\n",
              " Document(page_content='Information  Gain  or  R-squared.  With  CART  it  is  not  always  appropriate  to  visualise  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nimportant  features  in  a  dataset  as  the  binary  trees  tend  to  be  much  deeper  and  more \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ncomplex  than  a  non-binary  tree  (a  tree  which  can  have  more  than  two  child  nodes  for  any \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nnode   in   the   tree).   It   is   best   suited   for   prediction   (supervised   learning)   tasks. \\xa0\\n\\xa0\\n2)CHAID  (Chi-square  Automatic  Interaction  Detection)  :  It  creates  non-binary  trees  using  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nmeasure  Chi-square.  CHAID  trees  tend  to  be  shallower  and  easier  to  look  at  and  understand \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nthe   important   drivers   (features)   in   a   business   problem. \\xa0\\n\\xa0\\n4.4.3.   TRUNCATION   AND   PRUNING \\xa0\\nAdvantages   of   Decision   Trees \\xa0\\n1)Predictions   made   by   a   decision   tree   are   easily   interpretable. \\xa0\\n\\xa0\\n2)A  decision  tree  does  not  assume  anything  speciﬁc  about  the  nature  of  the  attributes  in  a \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\ndata  set  (as  is  done  by  Linear/Logistic  regression  and  SVM).  It  can  seamlessly  handle  all \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nkinds   of   data   may   it   be   numeric,   categorical,   string,   boolean   etc. \\xa0\\n\\xa0\\n3)It   does   not   require   normalisation   since   it   has   to   only   compare   the   values   within   an   attribute. \\xa0\\n\\xa0\\n4)Decision  trees  often  give  an  idea  of  the  relative  importance  (importance  of  attribute \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ndecreases  as  one  travels  from  parent  node  towards  leaf)  of  the  explanatory  attributes  that \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nare   used   for   prediction. \\xa0\\nDisadvantages   of   Decision   Trees \\xa0\\n1)Decision  trees  tend  to  overﬁt  the  data.  If  allowed  to  grow  without  any  check  on  its \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\ncomplexity,  a  tree  will  keep  splitting  till  it  has  correctly  classiﬁed  all  the  data  points  in  the \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\ntraining   set   i.e   each   leaf   will   denote   a   datapoint. \\xa0\\n\\xa0\\n2)Decision  trees  tend  to  be  very  unstable,  which  is  an  implication  of  overﬁtting.  A  few  changes \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nin   the   data   can   change   a   tree   considerably. \\xa0\\n\\xa0\\nAs  can  be  understood  from  above,  the  Decision  Trees  have  a  strong  tendency  to  overﬁt  the  data.  So \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nfor  practical  purposes  one  has  to  incorporate  certain  regularization  measures  to  ensure  that  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\ndecision  tree  built  does  not  become  more  complex  than  is  necessary  and  starts  to  overﬁt.  Below  are \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nthe   two   ways   of   regularization   of   decision   trees. \\xa0\\n\\xa0\\n1)Truncation  of  the  decision  tree  during  the  training  (growing)  process,  prevents  the  tree  from \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ndegenerating  into  being  complex  (i.e  with  one  leaf  for  every  data  point  in  the  training \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\ndataset).  Various  decision  tree  stopping  criterion  can  be  used  to  decide  the  truncation  of  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\ntree   while   performing   this   pre-pruning   process. \\xa0\\n\\xa0\\n2)Pruning  of  the  decision  tree  in  a  bottom-up  fashion  starting  from  the  leaves,  after  having \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nallowed  the  tree  to  grow  to  any  complexity,  also  prevents  the  tree  from  degenerating  into  a \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\ncomplex  one.  This  post-processing  step  is  more  commonly  used  to  avoid  overﬁtting  in \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\npractical   implementations. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   76 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 75}),\n",
              " Document(page_content='\\xa0\\nTree   Truncation \\xa0\\nThere  are  several  ways  to  truncate  the  decision  trees  before  they  start  to  overﬁt.  Some  of  these  are \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nlisted   below. \\xa0\\n\\xa0\\n1)Minimum  Size  of  the  Partition  for  a  Split  :  Stop  partitioning  further  when  the  current  partition \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nis   small   enough. \\xa0\\n\\xa0\\n2)Minimum  Change  in  Homogeneity  Measure  :  Do  not  partition  further  when  even  the  best \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nsplit  causes  an  insigniﬁcant  change  in  the  purity  measure  (diﬀerence  between  the  current \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\npurity   and   the   purity   of   the   partitions   created   by   the   split). \\xa0\\n\\xa0\\n3)Limit  on  Tree  Depth  :  If  the  current  node  is  farther  away  from  the  root  than  a  threshold,  then \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nstop   partitioning   further. \\xa0\\n\\xa0\\n4)Minimum  Size  of  the  Partition  at  a  Leaf  :  If  any  of  partitions  after  a  split  has  fewer  than  this \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nthreshold   minimum,   then   do   not   consider   the   split. \\xa0\\n\\xa0\\n5)Maximum  Number  of  Leaves  in  the  Tree  :  If  the  current  number  of  the  bottom-most  nodes  in \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nthe   tree   exceeds   this   limit   then   stop   partitioning. \\xa0\\nTree   Pruning \\xa0\\nOne  of  the  most  popular  approaches  to  pruning  called  the  Reduced  Error  Pruning,  typically  uses  the \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nvalidation  set  (a  set  of  labelled  data  points  kept  aside  from  the  original  training  dataset)  for  pruning. \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nBelow   is   the   algorithm   for   the   approach. \\xa0\\n\\xa0\\n1)Starting   at   the   leaves,   each   node   test   (non-leaf)   node   is   considered   for   pruning. \\xa0\\n\\xa0\\n2)Pruning  is  done  for  a  node,  by  removing  the  entire  subtree  below  the  node,  making  it  a  leaf. \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nThe  leaf  is  then  assigned  the  label  of  the  majority  class  (or  the  average  of  the  values  in  case \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\nit   is   regression)   among   the   training   data   points   that   passed   through   that   node. \\xa0\\n\\xa0\\n3)Pruning  of  a  node  is  done  only  if  the  decision  tree  obtained  after  the  pruning  has  an \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\naccuracy  that  is  no  worse  on  the  validation  dataset  than  the  tree  prior  to  pruning.  This \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nensures  that  the  parts  of  the  tree  which  got  added  due  to  accidental  irregularities  in  the  data \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nare   removed,   as   these   irregularities   are   not   likely   to   repeat. \\xa0\\n\\xa0\\n4)Pruning  is  stopped  when  the  removal  of  a  node  (any  of  the  remaining  nodes),  decreases  the \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\naccuracy   of   the   model   in   the   validation   dataset. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   77 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 76}),\n",
              " Document(page_content=\"Python   Code   -   Decision   Tree   Regularization \\xa0\\nLabel   Encoding   Categorical   Variables  \\n>>    from    sklearn    import    preprocessing  \\n>>   le   =   preprocessing.LabelEncoder()  \\n>>   df   =   df.apply(le.fit_transform)  \\n#   Decision   trees   can   handle   categorical   variables.   However,   label   encoding   is   still  \\ndone   to   get   a   standard   format   for   sklearn   to   build   the   tree.  \\n \\nCreate   Train   Data  \\n>>   X   =   df.drop( 'dependent_variable_column' ,axis=1)  \\n>>   y   =   df[ 'dependent_variable_column' ]   \\n \\nGrid   Search  \\n>>    from    sklearn.tree    import    DecisionTreeClassifier  \\n>>   hyper_params   =   [{ 'max_depth'    :   parameters,  \\n                      'min_samples_leaf'    :   parameters,   \\n                      'min_samples_split'    :   parameters,  \\n                      'max_features'    :   parameters,  \\n                      'max_leaf_nodes'    :   parameters,  \\n                      'criterion'    :   [ 'entropy' ,    'gini' ]}]  \\n \\n#   max_depth:   maximum   depth   of   the   tree.  \\n#   min_samples_leaf:   minimum   number   of   samples   required   to   be   at   a   leaf   node  \\n#   min_samples_split:   minimum   no.   of   samples   reqd  \\n#   max_features:   no.   of   features   to   consider   when   looking   for   the   best   split  \\n#   max_leaf_nodes:   maximum   number   of   possible   leaf   nodes  \\n#   criterion:   function   to   measure   the   quality   of   a   split  \\n>>   model   =   DecisionTreeClassifier()  \\n>>   model_cv   =   GridSearchCV(estimator=model,   param_grid=hyper_params,  \\n                            cv=folds,   verbose=1)  \\n>>   model_cv.fit(X_train,   y_train)  \\n>>   scores   =   pd.DataFrame(model_cv.cv_results_)  \\n>>   scores.best_score_  \\n>>   scores.best_estimator_  \\n \\nTrain   Model  \\n>>   model_final   =   DecisionTreeClassifier(max_depth=best_parameter,  \\n                                    min_samples_leaf=best_parameter,  \\n                                    min_samples_split=best_parameter,  \\n                                    max_features=best_parameter,  \\n                                    max_leaf_nodes=best_parameter,  \\n                                    criterion=best_parameter)  \\n>>   model_final.fit(X,   y)  \\n>>   model_final.score(X_test,y_test)  \\n \\nVisualize   Decision   Tree   Structure  \\n>>    from    IPython.display    import    Image   \\n>>    from    sklearn.externals.six    import    StringIO   \\n>>    from    sklearn.tree    import    export_graphviz  \\n>>    import    pydotplus,   graphviz  \\n>>   features   =   list(X.columns[0:])  \\n>>   dot_data   =   StringIO()   \\n>>   export_graphviz(model_final,   out_file=dot_data,  \\n                    feature_names=features,   filled= True ,   rounded= True )  \\n>>   graph   =   pydotplus.graph_from_dot_data(dot_data.getvalue())   \\n>>   Image(graph.create_png())  \\n \\n \\nTest   Model  \\n>>    from    sklearn.metrics    import    classification_report,   confusion_matrix  \\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   78 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 77}),\n",
              " Document(page_content='>>   y_pred   =   model_final.predict(X_test)  \\n>>   classification_report(y_test,   y_pred)  \\n>>   confusion_matrix(y_test,y_pred)  \\n \\n4.4.4.   ENSEMBLES \\xa0\\nEnsembles  are  a  group  of  things  viewed  as  a  whole  rather  than  individually.  In  ensembles,  a \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ncollection  of  models  is  used  to  make  predictions,  rather  than  individual  models.  The  random  forest  is \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nan  ensemble  made  by  the  combination  of  a  large  number  of  decision  trees.  In  principle,  ensembles \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ncan  be  made  by  combining  all  types  of  models  such  as  logistic  regression,  neural  network  or  a  few \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\ndecision   trees   all   working   in   unison. \\xa0\\n\\xa0\\nEnsembles  of  models  are  somewhat  analogous  to  teams  of  individual  players  with  diﬀerent \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nacceptable  (at  least  better  than  a  regular  person)  skill  sets.  For  an  ensemble  to  work,  each  model  of \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nthe   ensemble   should   comply   with   the   following   conditions. \\xa0\\n\\xa0\\n1)Each  model  should  be  diverse  :  Diversity  ensures  that  the  models  serve  complementary \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\npurposes,  which  means  that  the  individual  models  make  predictions  independent  of  each \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nother.  The  advantages  of  diversity  are  diﬀerent  depending  on  the  type  of  ensemble.  For \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nexample,  in  a  random  forest  even  if  some  trees  overﬁt,  the  other  trees  in  the  ensemble  will \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nneutralise  the  eﬀect.  Also  the  independence  among  the  trees  results  in  a  lower  variance  of \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nthe   ensemble   compared   to   a   single   tree. \\xa0\\n\\xa0\\n2)Each  model  should  be  acceptable  :  Acceptability  implies  that  each  model  should  at  least  be \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nbetter   than   a   random   model. \\xa0\\xa0\\n\\xa0\\nFollowing  is  an  example  of  how  the  ensemble  works.  Considered  here  is  a  binary  classiﬁcation \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nproblem  where  the  response  variable  is  either  a  0  or  1.  There  is  an  ensemble  of  three  models,  where \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\neach  model  has  an  accuracy  of  0.7  (70  %).  The  following  table  shows  all  the  possible  cases  that  can \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\noccur   while   classifying   a   test   data   point   as   1   or   0. \\xa0\\n\\xa0\\nCase \\xa0Result   of   each   Model \\xa0Result   of \\xa0\\nEnsemble \\xa0Probability \\xa0\\nModel   1 \\xa0 Model   2 \\xa0 Model   3 \\xa0\\n1 \\xa0Correct \\xa0 Correct \\xa0 Correct \\xa0 Correct \\xa0 0.7   x   0.7   x   0.7   =   0.343 \\xa0\\n2 \\xa0Incorrect \\xa0 Correct \\xa0 Correct \\xa0 Correct \\xa0 0.3   x   0.7   x   0.7   =   0.147 \\xa0\\n3 \\xa0Correct \\xa0 Incorrect \\xa0 Correct \\xa0 Correct \\xa0 0.7   x   0.3   x   0.7   =   0.147 \\xa0\\n4 \\xa0Correct \\xa0 Correct \\xa0 Incorrect \\xa0 Correct \\xa0 0.7   x   0.7   x   0.3   =   0.147 \\xa0\\n5 \\xa0Incorrect \\xa0 Incorrect \\xa0 Correct \\xa0 Incorrect \\xa0 0.3   x   0.3   x   0.7   =   0.063 \\xa0\\n6 \\xa0Incorrect \\xa0 Correct \\xa0 Incorrect \\xa0 Incorrect \\xa0 0.3   x   0.7   x   0.3   =   0.063 \\xa0\\n7 \\xa0Correct \\xa0 Incorrect \\xa0 Incorrect \\xa0 Incorrect \\xa0 0.7   x   0.3   x   0.3   =   0.063 \\xa0\\n8 \\xa0Incorrect \\xa0 Incorrect \\xa0 Incorrect \\xa0 Incorrect \\xa0 0.3   x   0.3   x   0.3   =   0.027 \\xa0\\n\\xa0\\nAs   can   be   seen   from   the   preceding   table, \\xa0\\nProbability   of   the   ensemble   being   correct   =   0.343   +   0.147   +   0.147   +   0.147   =   0.784 \\xa0\\nProbability   of   the   ensemble   being   wrong   =   0.027   +   0.063   +   0.063   +   0.063   =   0.216 \\xa0\\n\\xa0\\nThus,  one  can  see  that  an  ensemble  of  just  three  models,  gives  a  boost  to  the  accuracy  from  70%  to \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\n78.4%.  One  can  also  notice  that  the  ensemble  has  a  higher  probability  of  being  correct  and  a  lower \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nprobability  of  being  wrong  than  any  of  the  individual  models  (0.78  >  0.700  and  0.216  <  0.300).  In  this \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nway,  one  can  also  calculate  the  probabilities  of  the  ensemble  being  correct  and  incorrect  with  5,  10, \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\n100,  1000  and  even  a  million  individual  models.  The  diﬀerence  in  probabilities  keeps  on  increasing \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   79 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 78}),\n",
              " Document(page_content='with  an  increase  in  the  number  of  models,  thus  improving  the  overall  performance  of  the  ensemble. \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nIn   general,   the   more   the   number   of   models,   the   higher   the   accuracy   of   an   ensemble   is. \\xa0\\n\\xa0\\nAn  ensemble  makes  a  decision  by  taking  the  majority  vote.  It  means  that  if  there  are  models  in \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 n\\xa0 \\xa0\\xa0\\nthe  ensemble,  and  more  than  half  of  them  give  the  correct  answer,  the  ensemble  will  make  a  correct \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\ndecision  else  it  will  make  a  wrong  decision.  This  majority  voting  method  performs  better  on  unseen \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ndata   than   any   of   the   individual     models   as, n \\xa0\\n\\xa0\\n1)Firstly,  as  each  of  the  individual  models  is  acceptable  (i.e.  the  probability  of  each  model \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nbeing  wrong  is  less  than  0.5),  one  can  easily  show  that  the  probability  of  the  ensemble  being \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nwrong   (i.e.   the   majority   vote   going   wrong)   will   be   far   less   than   that   of   any   individual   model. \\xa0\\n\\xa0\\n2)Secondly,  the  ensembles  avoid  getting  misled  by  the  assumptions  made  by  individual \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nmodels.  For  example,  ensembles  (particularly  random  forests)  successfully  reduce  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nproblem  of  overﬁtting  as  the  chances  are  extremely  low  that  more  than  half  of  the  models \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\nhave   overﬁtted.   Ensembles   ensure   that   one   does   not   put   all   their   eggs   in   one   basket. \\xa0\\n\\xa0\\n4.4.5.   RANDOM   FORESTS   (BAGGING) \\xa0\\nRandom  Forests  are  an  ensemble  of  decision  trees.  The  great  thing  about  random  forests  is  that \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nthey  almost  always  outperform  a  decision  tree  in  terms  of  accuracy.  This  is  the  reason  why  it  is  one \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nof   the   most   popular   machine   learning   algorithms. \\xa0\\nBagging \\xa0\\nBagging  or  Bootstrapped  Aggregation  is  an  ensemble  method.  It  is  used  when  the  goal  is  to  reduce \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nthe  variance  of  a  decision  tree  classiﬁer.  Bootstrapping  means  to  create  several  random  subsets  of \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\ndata  (about  30-70%)  from  training  samples  chosen  randomly  with  replacement.  Each  collection  of \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nsubset  data  is  then  used  to  build  their  tree  using  a  random  sample  of  features  while  splitting  a  node \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\n(random  choice  of  attributes  ensure  that  the  prominent  features  do  not  appear  in  every  tree,  thus \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nensuring  diversity).  Aggregation  implies  combining  the  results  of  diﬀerent  models  present  in  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nensemble,  resulting  in  an  ensemble  of  diﬀerent  models.  Average  of  all  the  predictions  from  diﬀerent \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\ntrees  are  then  used  which  is  more  robust  than  a  single  decision  tree  classiﬁer.  It  needs  to  be  kept  in \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nmind  that  bagging  is  just  a  sampling  technique  and  is  not  speciﬁc  to  random  forests.  Below  is  the \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nalgorithm   for   the   same. \\xa0\\n\\xa0\\n1)Consider  there  are  observations  and  features  in  a  training  data  set.  A  sample  from  the \\xa0 \\xa0\\xa0 n\\xa0 \\xa0 \\xa0 k\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\ntraining   data   set   is   taken   randomly   with   replacement. \\xa0\\n2)A  subset  of  features  are  selected  randomly  and  whichever  feature  gives  the  best  split  is \\xa0 \\xa0\\xa0 k\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nused   to   split   the   node   iteratively. \\xa0\\xa0\\n3)The  tree  is  grown  to  the  largest  and  not  pruned  (as  may  be  done  in  constructing  a  normal \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\ntree   classiﬁer). \\xa0\\n4)The  above  steps  are  repeated  times,  to  construct  trees  in  the  forest.  As  each  tree  is \\xa0 \\xa0 \\xa0\\xa0 \\xa0 N\\xa0 \\xa0\\xa0 \\xa0 N\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nconstructed   independently,   it   is   possible   to   construct   each   tree   in   parallel. \\xa0\\n5)The  ﬁnal  prediction  is  given  based  on  the  aggregation  (majority  score)  of  predictions  from \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nthe   ensemble   of   the     number   of   trees. N \\xa0\\n\\xa0\\nFor  measuring  the  accuracy  of  the  model  the  OOB  (Out-Of-Bag)  error  can  be  used.  For  each \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nbootstrap  sample,  there  is  one  third  of  data  which  is  not  used  in  the  creation  of  a  tree  (i.e.  it  was  out \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nof  the  sample).  This  data  is  referred  to  as  the  out  of  bag  data.  In  order  to  get  an  unbiased  measure \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\nof   the   accuracy   of   the   model   over   test   data,   out   of   bag   error   is   used. \\xa0\\nOOB   (Out-Of-Bag)   Error \\xa0\\nOne  needs  to  always  avoid  violating  the  fundamental  tenet  of  learning,  that  is  not  to  test  a  model  on \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\nwhat  it  has  been  trained  on.  The  OOB  error  uses  the  above  principle.  It  is  calculated  by  using  each \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   80 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 79}),\n",
              " Document(page_content='observation  of  the  dataset  as  a  test  observation.  Since,  each  tree  is  built  on  a  random \\xa0 ni\\xa0\\xa0 \\xa0 \\xa0 n\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nbootstrap  sample  which  is  about  30-70%  of  the  dataset,  for  every  observation  there  will  be  those \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 ni\\xa0 \\xa0\\xa0\\xa0 \\xa0\\ntrees  which  will  not  have  used  the  observation  in  their  bootstrap  sample.  Thus,  this  observation \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 ni\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\n can  be  used  as  a  test  observation  by  such  trees.  All  such  trees  then  predict  on  this  observation ni\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\n and  the  ﬁnal  OOB  error  is  calculated  by  aggregating  the  error  on  each  observation .  The  OOB ni\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 ni\\xa0 \\xa0 \\xa0\\nerror  is  as  good  as  the  cross  validation  error.  In  fact,  it  has  been  proven  that  using  an  OOB  estimate \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nis  as  accurate  as  using  a  test  data  set  of  a  size  equal  to  the  training  set.  Thus,  the  OOB  error \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ncompletely   omits   the   need   for   set-aside   test   data. \\xa0\\nAdvantages   of   Bagging \\xa0\\nSome   of   the   advantages   of   random   forests   are   given   below. \\xa0\\n\\xa0\\n1)Higher  resolutions  (a  smoother  decision  boundary)  in  the  feature  space  due  to  trees  being \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nindependent  of  each  other.  Trees  are  independent  of  each  other  due  to  the  diversity  created \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nin  each  tree  by  the  use  of  random  subsets  of  features  (i.e.  not  all  attributes  are  considered \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\nwhile   making   each   tree). \\xa0\\n\\xa0\\n2)Reduction  in  overﬁtting  as  the  chances  are  extremely  low  that  more  than  half  of  the  models \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\nhave   overﬁtted   in   the   ensemble. \\xa0\\n\\xa0\\n3)Increased  stability  as  the  ﬁnal  prediction  is  given  by  the  aggregation  of  a  large  number  of \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\ntrees.  The  decision  made  by  a  single  tree  (on  unseen  data)  depends  highly  on  the  training \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\ndata  since  trees  are  unstable.  In  a  forest,  even  if  a  few  trees  are  unstable,  averaging  out  their \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ndecisions  decreases  the  mistakes  made  because  of  these  few  unstable  trees.  A  random \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nforest   usually   always   has   a   lower   model   variance   than   an   ordinary   individual   tree. \\xa0\\n\\xa0\\n4)Immunity  from  the  curse  of  dimensionality.  Since  each  tree  does  not  consider  all  the \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nfeatures,  the  feature  space  reduces.  This  makes  the  algorithm  immune  to  a  large  feature \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nspace  causing  computational  and  complexity  issues.  This  in  turn  also  allows  it  to  handle \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\nlarge   number   of   input   variables   eﬀectively   without   the   necessity   of   variable   deletion. \\xa0\\n\\xa0\\n5)Maintains  accuracy  for  missing  data  by  having  an  eﬀective  method  for  estimating  missing \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ndata   even   when   a   large   proportion   of   the   data   are   missing. \\xa0\\n\\xa0\\n6)Allows  parallelizability,  as  each  tree  can  be  built  separately  owing  to  the  fact  that  each  of  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\ntrees  are  independently  built  on  diﬀerent  data  and  attributes.  This  allows  one  to  make  full \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nuse   of   the   multi-core   CPU   to   build   random   forests. \\xa0\\n\\xa0\\n7)Uses  all  the  data  to  train  the  model.  There  is  no  need  for  the  data  to  be  split  into  training  and \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nvalidation  samples  as  one  can  calculate  the  OOB  (Out-of-Bag)  error  using  the  training  set \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nwhich   gives   a   really   good   estimate   of   the   performance   of   the   forest   on   unseen   data. \\xa0\\n\\xa0\\nEven  though  bagging  has  many  advantages,  at  times  it  may  not  give  precise  values  for  the \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nclassiﬁcation  and  regression  model.  It  is  because  the  ﬁnal  prediction  is  based  on  the  mean \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\npredictions   from   subset   trees. \\xa0\\nTime   taken   to   build   a   forest \\xa0\\nThe  time  taken  to  construct  a  forest  of  trees,  on  a  dataset  which  has  features  and \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 S\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 k\\xa0 \\xa0 \\xa0 n\\xa0\\nobservations   depend   on   the   below   factors. \\xa0\\n\\xa0\\n1)Number  of  Trees  :  The  time  is  directly  proportional  to  the  number  of  trees.  The  time  required \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nto  build  each  tree  is  proportional  to  the  time  spent  in  creating  the  levels  of  trees.  But \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 o g( n) l \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthis   time   can   be   reduced   by   creating   the   trees   in   parallel. \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   81 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 80}),\n",
              " Document(page_content=\"2)Size  of  Bootstrap  Sample  :  Generally  the  size  of  a  bootstrap  sample  is  30-70%  of .  The \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 n\\xa0 \\xa0\\nsmaller   the   size   the   faster   it   takes   to   create   a   forest. \\xa0\\n\\xa0\\n3)Size  of  Subset  of  Features  for  splitting  a  Node  :  Usually,  this  is  taken  as  in  case  of \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0  √ k\\xa0\\xa0 \\xa0\\xa0\\nclassiﬁcation   and     in   case   of   regression. 3 k/ \\xa0\\nPython   Code   -   Random   Forest \\xa0\\nGrid   Search  \\n>>    from    sklearn.ensemble    import    RandomForestClassifier  \\n>>   hyper_params   =   [{ 'n_estimators'    :   parameters,  \\n                      'max_depth'    :   parameters,  \\n                      'min_samples_leaf'    :   parameters,   \\n                      'min_samples_split'    :   parameters,  \\n                      'max_features'    :   parameters,  \\n                      'max_leaf_nodes'    :   parameters,  \\n                      'criterion'    :   [ 'entropy' ,    'gini' ]}]  \\n#   n_estimators:   number   of   trees   in   the   forest  \\n#   max_depth:   maximum   depth   of   the   tree  \\n#   min_samples_leaf:   minimum   number   of   samples   required   to   be   at   a   leaf   node  \\n#   min_samples_split:   minimum   no.   of   samples   reqd  \\n#   max_features:   no.   of   features   to   consider   when   looking   for   the   best   split  \\n#   max_leaf_nodes:   maximum   number   of   possible   leaf   nodes  \\n#   criterion:   function   to   measure   the   quality   of   a   split  \\n>>   model   =   RandomForestClassifier()  \\n>>   model_cv   =   GridSearchCV(estimator=model,   param_grid=hyper_params,  \\n               cv=folds,   n_jobs=-1,   verbose=1)  \\n>>   model_cv.fit(X_train,   y_train)  \\n>>   model_cv.best_score_  \\n>>   model_cv.best_params_  \\n \\nTrain   Model  \\n>>   model_final   =   RandomForestClassifier(bootstrap= True ,  \\n                                         n_estimators=best_parameter,  \\n                                         max_depth=best_parameter,  \\n                                         min_samples_leaf=best_parameter,  \\n                                         min_samples_split=best_parameter,  \\n                                         max_features=best_parameter,  \\n                                         max_leaf_nodes=best_parameter,  \\n                                         criterion=best_parameter)  \\n>>   model_final.fit(X,   y)  \\n>>   model_final.score(X_test,y_test)  \\n \\nTest   Model  \\n>>    from    sklearn.metrics    import    classification_report,   confusion_matrix  \\n>>   y_pred   =   model_final.predict(X_test)  \\n>>   classification_report(y_test,   y_pred)  \\n>>   confusion_matrix(y_test,y_pred) \\xa0\\n\\xa0\\n4.4.6.   RANDOM   FORESTS   (BOOSTING) \\xa0\\nOptional   section   not   yet   compiled. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   82 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 81}),\n",
              " Document(page_content='4.5.   CONSIDERATIONS   FOR   MODEL   SELECTION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nCONSIDERATIONS   FOR   MODEL \\xa0\\nSELECTION \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   83 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 82}),\n",
              " Document(page_content='4.5.1.   CONSIDERATIONS   FOR   MODEL   SELECTION \\xa0\\nWhen  it  comes  to  the  selection  of  a  suitable  model,  there  are  a  lot  of  algorithms  to  choose  from. \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nOne  could  always  apply  all  the  models  and  then  compare  their  results.  But  it  is  neither  always \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nfeasible  to  apply  all  the  models  nor  is  there  enough  time  to  try  all  the  available  options.  Thus,  there \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nneeds  to  be  some  guiding  principles  behind  the  choice  of  models  rather  than  to  just  use  the \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nhit-and-trial   approach. \\xa0\\nComparing   Diﬀerent   Machine   Learning   Models \\xa0\\nThe  business  problem  being  considered  here  is  to  determine  the  gender  of  each  online  consumer \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nand  the  age  group  to  which  they  belong  for  an  ecommerce  company.  The  problem  is  being  solved \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nusing  Logistic  Regression,  Decision  Trees  and  SVM.  The  following  ﬁgure  shows  the  distribution  of \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe   consumer. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe   following   ﬁgures   show   the   performance   of   the   various   models   used   over   the   data. \\xa0\\n\\xa0\\n\\xa0\\n \\xa0\\n \\xa0\\n  Logistic   Regression   :   The \\xa0\\ndiagonal   line   is   clearly   not   able \\xa0\\nto   diﬀerentiate   the   two   classes, \\xa0\\nwith   a   lot   of   misclassiﬁcations. \\xa0Decision   Trees/Random   Forests   : \\xa0\\nIt   does   better   diﬀerentiation,   but \\xa0\\ncan   potentially   overﬁt   with \\xa0\\nincrease   in   the   trees. \\xa0SVM   :    It   does   a   really   good   job \\xa0\\nof    diﬀerentiating   the   two \\xa0\\nclasses   almost   perfectly. \\xa0\\n\\xa0\\nThe   following   gives   the   comparison   between   the   three   algorithms. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   84 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 83}),\n",
              " Document(page_content='Advantages \\xa0 Disadvantages \\xa0\\nLogistic   Regression \\xa0\\nIt  has  a  widespread  industry  use  and  can  be \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\neﬃciently   implemented   across   diﬀerent   tools. \\xa0\\n\\xa0\\nIt  oﬀers  convenient  probability  scores  for \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\noutputs. \\xa0\\n\\xa0\\nIt  addresses  the  issue  of  multicollinearity  in  the \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\ndata   using   regularisation. \\xa0It  does  not  perform  well  when  the  features  space \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nis  too  large  or  when  there  are  a  lot  of  categorical \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\nvariables   in   the   data. \\xa0\\n\\xa0\\nIt  requires  the  nonlinear  features  to  be \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ntransformed   to   linear   features   for   eﬃciency. \\xa0\\n\\xa0\\nIt  relies  on  entire  data  i.e.  even  a  small  change  in \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nthe   data,   changes   the   model. \\xa0\\nDecision   Trees \\xa0\\nIt  is  easy  to  interpret  because  of  the  intuitive \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ndecision   rules. \\xa0\\n\\xa0\\nIt  can  handle  nonlinear  features  well  and  also \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ntakes  into  account  the  interaction  between  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nvariables. \\xa0It  is  highly  biased  towards  the  training  set  and \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\noverﬁts   it   more   often   than   not. \\xa0\\n\\xa0\\nIt  does  not  have  a  meaningful  probabilistic  output \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nscore   as   the   output. \\xa0\\n\\xa0\\nSplitting  with  multiple  linear  decision  boundaries \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nis   not   always   eﬃcient. \\xa0\\n\\xa0\\nIt  is  not  possible  to  predict  beyond  the  range  of \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nthe  response  variable  in  the  training  data  in  a \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nregression   problem. \\xa0\\nRandom   Forests \\xa0\\nIt  gives  a  good  estimate  of  model  performance \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\non   unseen   data   using   the   OOB   error. \\xa0\\n\\xa0\\nIt  does  not  require  pruning  of  trees  and  hardly \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\never   overﬁts   the   training   data. \\xa0\\n\\xa0\\nIt  is  not  aﬀected  by  outliers  because  of  the \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\naggregation   strategy. \\xa0It  too  has  the  problem  of  not  predicting  beyond \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nthe  range  of  the  response  variable  (being \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nderived   from   trees). \\xa0\\n\\xa0\\nIt  often  does  not  predict  the  extreme  values \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nbecause   of   the   aggregation   strategy. \\xa0\\nSupport   Vector   Machines \\xa0\\nIt   can   handle   large   feature   space. \\xa0\\n\\xa0\\nIt   can   handle   nonlinear   feature   interaction. \\xa0\\n\\xa0\\nIt  does  not  rely  on  the  entire  dimensionality  of \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nthe   data   for   the   transformation. \\xa0It  is  not  eﬃcient  in  terms  of  computational  cost \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nwhen   the   number   of   observations   is   large. \\xa0\\n\\xa0\\nIt  is  tricky  and  time-consuming  to  ﬁnd  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nappropriate   kernel   for   a   given   data. \\xa0\\n\\xa0\\nEnd-to-End   Modelling \\xa0\\nOne  can  easily  get  overwhelmed  by  the  choices  of  the  algorithms  available  for  classiﬁcation.  The \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nfollowing   steps   provide   a   more   general   rule   followed   while   going   about   modelling   data. \\xa0\\n\\xa0\\n1.Modelling  is  usually  started  oﬀ  with  Logistic  Regression.  This  serves  two  very  important \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\npurposes, \\xa0\\na.It   acts   as   a   baseline   (benchmark)   model. \\xa0\\nb.It   gives   an   idea   about   the   important   variables. \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   85 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 84}),\n",
              " Document(page_content='2.Next,  the  Decision  Trees  are  used  for  modelling  and  the  performances  are  compared.  While \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nbuilding  a  Decision  Tree,  one  should  choose  the  appropriate  method,  CART  for  predicting \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nand  CHAID  for  a  driver  analysis.  If  the  performance  is  not  up  to  the  standards,  then  the \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nDecision  Trees  are  used  to  model  again,  but  with  only  the  important  variables  identiﬁed  in \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nthe   Logistic   Regression   model   and   re-evaluated. \\xa0\\n\\xa0\\xa0\\n3.Finally,  if  the  performance  is  still  not  to  the  mark,  then  one  can  go  for  more  complex  models \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nsuch  as  Random  Forests  and  SVM’s,  keeping  the  time  and  resource  constraints  in  mind. \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nThese  models  being  computationally  expensive  and  time  consuming  are  usually  avoided  or \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nalways   used   at   last. \\xa0\\n\\xa0\\nThe   following   ﬂowchart   gives   the   steps   the   end   to   end   modelling. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   86 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 85}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n5.   CLUSTERING \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   87 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 86}),\n",
              " Document(page_content='5.1.   CLUSTERING \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nCLUSTERING \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   88 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 87}),\n",
              " Document(page_content='5.1.1.   CLUSTERING \\xa0\\nAll  the  algorithms  discussed  till  now  (such  as  Linear  Regression,  Logistic  Regression,  Naive  Bayes, \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nSVM,  Decision  Tree  etc.),  belong  to  the  Supervised  machine  learning  algorithms,  which  make  use  of \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nlabelled  data  to  make  predictions.  Next  to  be  discussed  are  Unsupervised  machine  learning \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nalgorithms.  In  Unsupervised  learning,  prediction  is  of  no  importance  as  there  are  no  target  or \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\noutcome  variables.  Rather,  the  objective  is  to  discover  interesting  patterns  in  the  data.  Clustering  is \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\nan  example  of  Unsupervised  machine  learning  technique.  It  is  used  to  place  the  data  elements  into \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nrelated   groups   without   any   prior   knowledge   of   the   group   deﬁnitions. \\xa0\\nClustering \\xa0\\nClustering  refers  to  a  very  broad  set  of  techniques  for  ﬁnding  subgroups  or  segments  in  a  data  set. \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nClustering  techniques  use  the  raw  data  to  form  clusters  based  on  common  factors  among  various \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ndata   points.   The   following   ﬁgure   shows   the   diﬀerence   between   clustering   and   classiﬁcation. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nFor   successful   segmentation, \\xa0\\xa0\\n\\xa0\\n1)The  segments  formed  need  to  be  stable  (i.e.  the  same  data  point  should  not  fall  under \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\ndiﬀerent   segments   upon   segmenting   the   data   on   the   same   criteria). \\xa0\\n2)The  segments  should  have  intra-segment  homogeneity  (i.e.  the  behaviour  in  a  segment \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nneeds   to   be   similar). \\xa0\\xa0\\n3)The  segments  should  have  inter-segment  heterogeneity  (i.e.  the  behaviour  between \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nsegments   needs   to   be   diﬀerent). \\xa0\\n\\xa0\\nThere   are   mainly   three   types   of   segmentation   used   for   customer   segmentation. \\xa0\\n\\xa0\\n1)Behavioural  segmentation  :  Segmentation  is  based  on  the  actual  behavioural  patterns \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ndisplayed   by   the   person. \\xa0\\n2)Attitudinal  segmentation  :  Segmentation  is  based  on  the  beliefs  or  intentions  of  the  person, \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nwhich   may   not   translate   into   similar   action. \\xa0\\n3)Demographic  segmentation  :  Segmentation  is  based  on  the  person’s  proﬁle  and  uses \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ninformation   such   as   age,   gender,   residence   locality,   income,   etc. \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   89 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 88}),\n",
              " Document(page_content='Behavioural   Segmentation \\xa0\\nThe   commonly   used   customer   behavioural   segmentation   techniques   are   as   follows. \\xa0\\n\\xa0\\nRFM \\xa0 RPI \\xa0 CDJ \\xa0\\nR   :   Recency   (Time   since   last \\xa0\\npurchase/visit) \\xa0\\nF   :   Frequency   (Total   number   of \\xa0\\npurchases/visits) \\xa0\\nM   :   Monetary   (Total   revenue \\xa0\\ngenerated) \\xa0R   :   Relationship   (Past   interaction) \\xa0\\nP   :   Persona   (Type   of   customer) \\xa0\\nI   :   Intent   (Intention   at   the   time   of \\xa0\\npurchase) \\xa0CDJ   :   Consumer   Decision \\xa0\\nJourney \\xa0\\n\\xa0\\nBased   on   the   customer’s   life \\xa0\\njourney   with   the   product. \\xa0\\n\\xa0\\n5.1.2.   K-MEANS   CLUSTERING \\xa0\\nK-means  clustering  is  a  simple  yet  elegant  approach  for  partitioning  a  data  set  into  K  distinct, \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nnon-overlapping  clusters.  In  order  to  perform  K-means  clustering,  one  must  ﬁrst  specify  the  desired \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nnumber  of  clusters  K  and  then  the  K-means  algorithm  assigns  each  observation  to  exactly  one  of  the \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nK   clusters. \\xa0\\nK-Means   Algorithm \\xa0\\nFor   a   dataset   where, \\xa0\\n  is   the   total   no   of   observations N \\xa0\\n  is   the   total   number   of   desired   clusters K \\xa0\\n  denotes   the   set   containing   the   observations   in     cluster Ck kt h\\xa0\\n\\xa0\\nThe   following   algorithm   is   applied   to   create     clusters. K \\xa0\\n\\xa0\\n1)Initialisation  :  The  cluster  centers  for  each  of  the  clusters  are  randomly  picked.  These  can \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 K\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\neither   be   from   any   of   the     observations   or   totally   a   diﬀerent   point. N \\xa0\\n\\xa0\\n2)Assignment  :  Each  observation  is  assigned  to  the  cluster  whose  cluster  center  is  the \\xa0\\xa0 \\xa0 \\xa0 n\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nclosest   to   it.   The   closest   cluster   center   is   found   using   the   squared   Euclidean   distance. \\xa0\\xa0\\nThe   equation   for   the   assignment   step   is   as   follows. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe   observations   are   to   be   assigned   in   such   a   way   that   they   satisfy   the   below   conditions. \\xa0\\n\\xa0\\na)  i.e.  each  observation  belongs  to  at  least  one  of  the 1,,..,} C1 ⋃ C2 ⋃... Ck={2. N\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 K\\xa0\\nclusters. \\xa0\\n\\xa0\\nb) i.e.   no   observation   belongs   to   more   than   one   cluster.  w h e r e k= Ck ⋂ Ck ′= ϕ / k ′ \\xa0\\n\\xa0\\n3)Optimisation  :  For  each  of  the  clusters,  the  cluster  center  is  computed  such  that  the \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 K\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 kt h\\xa0\\ncluster  center  is  the  vector  of  the  feature  means  for  the  observations  in  the  cluster. \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 p\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 kt h\\xa0 \\xa0\\nThe   equation   for   the   optimisation   step   is   as   follows. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   90 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 89}),\n",
              " Document(page_content='4)Iteration  :  The  process  of  assignment  and  optimisation  is  repeated  until  there  is  no  change  in \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nthe   clusters   or   possibly   until   the   algorithm   converges. \\xa0\\n\\xa0\\nThe   following   ﬁgures   are   a   graphical   representation   of   the   K-Means   algorithm. \\xa0\\n\\xa0\\n\\xa0\\n \\xa0\\n \\xa0\\n \\xa0\\n \\xa0\\n \\xa0\\nInitialization \\xa0\\n\\xa0\\nK-means   algorithm \\xa0\\nwith   K=3.   The   3 \\xa0\\ncluster   center   are \\xa0\\nrandomly   selected \\xa0Iteration   1,   Step   1 \\xa0\\n\\xa0\\nEach   observation \\xa0\\nis   assigned   to   the \\xa0\\nnearest   cluster \\xa0\\ncenter \\xa0Iteration   1,   Step   2 \\xa0\\n\\xa0\\nCluster   centers \\xa0\\nare   computed \\xa0\\nusing   squared \\xa0\\neuclidean   distance \\xa0Iteration   2,   Step   1 \\xa0\\n\\xa0\\nEach   observation \\xa0\\nis   assigned   to    the \\xa0\\nnearest   cluster \\xa0\\ncenter \\xa0Iteration   2,   Step   2 \\xa0\\n\\xa0\\nCluster   centers \\xa0\\nare   again \\xa0\\ncomputed    leading \\xa0\\nto   new   centers \\xa0Final   Result \\xa0\\n\\xa0\\nAfter   N   iterations \\xa0\\nthe   result   is   found \\xa0\\nwhen   the   clusters \\xa0\\nstop   changing \\xa0\\n\\xa0\\nCost   Function \\xa0\\nIn  the  K-Means  algorithm  it  is  known  that  the  centroids  being  computed  are  the  cluster  means  for \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\neach  feature  and  are  the  constants  that  minimize  the  sum  of  squared  deviations.  Added  to  this,  the \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nreallocation  of  the  observations  help  in  improving  the  above  result  until  the  result  no  longer  changes \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\n(i.e   optimum   point   is   reached).   Thus,   giving   the   cost   function. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nOptimizing   Cost   Function \\xa0\\nThe  K-Means  algorithm  repeatedly  minimize  the  function  concerning  cluster  assignment  given \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 Ck\\xa0 \\xa0μ\\xa0\\nand  then  minimize  the  function  concerning  given  the  cluster  assignment  using  a  coordinate \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0μ\\xa0 \\xa0\\xa0 \\xa0 \\xa0 Ck\\xa0 \\xa0\\xa0 \\xa0\\ndescent  method.  The  K-Means  cost  function  is  usually  a  non  convex  function  (i.e.  local  minima  is  not \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nequal  to  the  global  minima).  So,  the  coordinate  descent  is  not  guaranteed  to  converge  to  the  global \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nminimum  and  can  rather  converge  to  a  local  minima.  Thus,  the  ﬁnal  results  can  be  highly  dependent \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\non  the  initial  cluster  assignment  of  each  observation.  For  this  reason,  it  is  important  to  run  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\nalgorithm  multiple  times  with  diﬀerent  initial  conﬁgurations  to  check  for  the  robustness  of  the  ﬁnal \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nresult. \\xa0\\nK-Means++   Algorithm \\xa0\\nIn  the  K-Means  algorithm  during  the  initialisation  step  the  cluster  centers  are  selected  randomly.  This \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nmay  lead  to  the  algorithm  converging  at  a  local  minima.  So,  one  can  choose  the  cluster  centers \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nsmartly  during  the  initialisation  to  achieve  the  global  minima  using  the  K-Means++  algorithm. \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nK-Means++  is  just  an  initialisation  procedure  for  K-Means  for  picking  the  initial  cluster  centers  using \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nan   algorithm   that   tries   to   initialise   the   cluster   centers   that   are   far   apart   from   each   other. \\xa0\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   91 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 90}),\n",
              " Document(page_content='\\xa0\\nThe   following   algorithm   is   applied   to   select   the   cluster   centers. \\xa0\\n\\xa0\\n1)One   of   the   data   points   is   randomly   chosen   as   a   cluster   center. \\xa0\\n\\xa0\\n2)For  each  data  point ,  the  distance  is  computed.  It  is  the  distance  between  and  the \\xa0 \\xa0 \\xa0 \\xa0 xi\\xa0\\xa0 \\xa0 di\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 xi\\xa0 \\xa0\\xa0\\nnearest   center   that   had   already   been   chosen. \\xa0\\n\\xa0\\n3)The  next  cluster  center  is  chosen  using  the  weighted  probability  distribution  where  a  point \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 x\\xa0\\nis   chosen   with   probability   proportional   to   . di2\\xa0\\n\\xa0\\n4)Steps   2   and   3   are   repeated   until     cluster   centers   have   been   chosen. K \\xa0\\n\\xa0\\nThe   following   ﬁgure   is   a   graphical   representation   of   the   K-Means++   algorithm. \\xa0\\n\\xa0\\n \\xa0\\n \\xa0\\n\\xa0\\nRandomly   chosen   cluster   center \\xa0\\nout   of   the   12   data   points. \\xa0Data   point   1   is   the   least   likely   next   cluster   center   as   proportional \\xa0\\ndistance   is   the   smallest   for   this   data   point. di2\\xa0\\nDatapoint   2   is   the   most   likely   next   cluster   center   as   proportional \\xa0\\ndistance   is   the   largest   for   this   data   point. di2\\xa0\\n\\xa0\\nPractical   Considerations   for   K-Means   Clustering \\xa0\\nSome   of   the   points   to   be   considered   while   implementing   the   K-Means   algorithm   are   as   follows. \\xa0\\n\\xa0\\n1)The  number  of  clusters  (value  of )  into  which  the  data  points  are  to  be  divided  has  to  be \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 K\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\npredetermined. \\xa0\\n\\xa0\\n2)Since  the  distance  metric  used  in  the  clustering  process  is  the  squared  Euclidean  distance \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nbetween  the  data  points,  it  is  important  to  ensure  that  the  attributes  with  a  larger  range  of \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nvalues  do  not  out-weight  the  attributes  with  smaller  range.  Thus,  scaling  down  of  all \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\nattributes  to  the  same  normal  scale  helps  in  this  process.  Scaling  helps  in  making  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nattributes   unit-free   and   uniform. \\xa0\\n\\xa0\\n3)The  K-Means  algorithm  cannot  be  used  when  dealing  with  categorical  data  as  the  concept  of \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\ndistance   for   categorical   data   doesn’t   make   much   sense. \\xa0\\n\\xa0\\n4)The  choice  of  the  initial  cluster  centres  can  have  an  impact  on  the  ﬁnal  cluster  formation.  The \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nfollowing  ﬁgure  shows  three  cases  (for  the  same  dataset)  with  diﬀerent  set  of  initial  cluster \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\ncenters   giving   diﬀerent   clusters   at   the   end. \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   92 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 91}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\n5)Since  the  K-Means  algorithm  tries  to  allocate  each  of  the  data  points  to  one  of  the  clusters, \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\noutliers  have  a  serious  impact  on  the  performance  of  the  algorithm  and  prevent  optimal \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nclustering. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n6)There  is  a  chance  that  the  algorithm  may  not  converge  in  the  given  number  of  iterations,  so \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\none   needs   to   always   check   for   convergence. \\xa0\\nChoosing   Number   of   Clusters   ‘K’ \\xa0\\nChoosing  the  value  of  in  the  K-Means  algorithm  is  of  utmost  importance  as  lower  values  of  can \\xa0\\xa0 \\xa0\\xa0 K\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 K\\xa0\\xa0\\ncause  the  clusters  not  to  be  cohesive  enough  and  higher  values  of  can  lead  to  clusters  not \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 K\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ndissimilar   enough.   The   same   is   depicted   in   the   following   ﬁgure. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThere  are  a  number  of  pointers  available  that  can  help  in  deciding  the  number  of  for  a  K-means \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 K\\xa0\\xa0\\xa0 \\xa0\\nalgorithm. \\xa0\\nSilhouette   Analysis \\xa0\\nSilhouette  analysis  is  an  approach  to  choosing  the  value  of  for  the  K-Means  algorithm.  The \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 K\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nSilhouette  Analysis  or  Silhouette  Coeﬃcient  is  a  measure  of  how  similar  a  data  point  is  to  its  own \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\ncluster   (cohesion)   compared   to   other   clusters   (separation). \\xa0\\n\\xa0\\nFor   a   given   dataset   where, \\xa0\\n  is   the   average   distance   from   own   cluster ai \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   93 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 92}),\n",
              " Document(page_content='  is   the   average   distance   from   the   nearest   neighbour   cluster bi \\xa0\\n\\xa0\\nSilhouette   Value   is   given   by, \\xa0\\n\\xa0\\n\\xa0\\nThe  value  of  varies  between  -1  and  1.  For  to  be  close  to  1,  it  is  required  for .  As \\xa0 \\xa0\\xa0( i) S\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0( i) S\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0( i) ( i) a ≪ b\\xa0\\xa0\\n is  a  measure  of  dissimilarity  of  with  its  own  cluster,  a  small  value  of  means  it  is  well ( i) a\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 xi\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0( i) a\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nmatched.  Furthermore,  a  large  value  of  implies  that  is  badly  matched  to  its  neighbouring \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0( i) b\\xa0 \\xa0 \\xa0 xi\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\ncluster.  Thus,  values  close  to  1  means  that  the  data  is  appropriately  clustered.  If  is  close  to \\xa0 \\xa0( i) S\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0( i) S\\xa0\\xa0 \\xa0\\xa0\\n-1,  it  implies  that  would  be  more  appropriate  if  it  were  clustered  in  its  neighbouring  cluster.  An \\xa0\\xa0 \\xa0 \\xa0 xi\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\n  value   close   to   0   means   that     is   on   the   border   of   two   natural   clusters. ( i) S xi \\xa0\\xa0\\n\\xa0\\nThe  average  over  all  the  points  of  a  cluster  measures  how  tightly  grouped  all  the  points  in  the \\xa0 \\xa0( i) S\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\ncluster  are.  Thus  the  average  over  all  data  of  the  entire  dataset  is  a  measure  of  how \\xa0 \\xa0 \\xa0 \\xa0 \\xa0( i) S\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\nappropriately  the  data  has  been  clustered.  If  there  are  too  many  or  too  few  clusters,  some  of  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\nclusters  will  typically  display  much  narrower  silhouettes  than  the  rest.  Thus  silhouette  plots  and \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\naverages  are  used  to  determine  the  natural  number  of  clusters  within  a  dataset  as  shown  in  the \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nfollowing   ﬁgure. \\xa0\\n\\xa0\\n\\xa0\\nElbow   Curve   Method \\xa0\\nElbow  method  is  another  approach  for  ﬁnding  the  appropriate  number  of  clusters  in  a  dataset  by \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\ninterpreting  and  validating  the  consistency  within  a  cluster.  The  idea  of  the  elbow  method  is  to  run \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\nK-Means  clustering  on  the  dataset  for  a  range  of  values  of  while  calculating  the  Sum  of  Squared \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 K\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nErrors  (SSE)  for  each  value  of .  On  plotting  a  line  chart  of  the  SSE  for  each  value  of ,  an  arm  like \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 K\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 K\\xa0\\xa0\\xa0\\xa0\\nplot  is  formed  as  shown  in  the  following  ﬁgure.  The  elbow  on  the  arm  is  the  value  of ,  that  is  the \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 K\\xa0\\xa0\\xa0\\xa0\\nbest. \\xa0\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   94 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 93}),\n",
              " Document(page_content=\"\\xa0\\n\\xa0\\nThe  intention  is  to  always  have  a  small  SSE,  but  as  the  SSE  tends  to  decrease  towards  0  with  the \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nincrease  in  (the  SSE  is  0  when  is  equal  to  the  number  of  data  points  in  the  dataset,  because \\xa0\\xa0 K\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 K\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nthen  each  data  point  is  its  own  cluster,  and  there  is  no  error  between  it  and  the  center  of  its  cluster). \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nThus,  the  goal  is  to  choose  a  small  value  of  which  still  has  a  low  SSE,  and  the  elbow  usually \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 K\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nrepresents  the  point  from  where  the  returns  start  diminishing  with  increasing  values  of .  However, \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 K\\xa0 \\xa0\\nthe  elbow  method  doesn't  always  work  well,  especially  if  the  data  is  not  very  clustered.  In  such \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ncases  one  can  try  the  Silhouette  analysis,  or  even  reevaluate  whether  clustering  is  the  right  thing  to \\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\ndo. \\xa0\\xa0\\nCluster   Tendency \\xa0\\nBefore  applying  the  clustering  algorithm  to  any  given  dataset,  it  is  important  to  check  whether  the \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\ngiven  data  has  some  meaningful  clusters  or  not  (i.e  the  data  is  not  random).  This  process  of \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nevaluating  the  data  for  its  feasibility  for  clustering  is  known  as  the  Clustering  Tendency.  The \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nclustering  algorithms  will  return  clusters  even  when  the  datasets  do  not  have  any  meaningful \\xa0 \\xa0\\xa0 \\xa0 K\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nclusters.  So,  one  should  always  check  the  clustering  tendency  before  proceeding  for  clustering.  To \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ncheck   the   cluster   tendency,   one   can   use   the   Hopkins   test. \\xa0\\nHopkins   Test \\xa0\\xa0\\nThe  Hopkins  test  is  used  to  assess  the  clustering  tendency  of  a  data  set  by  measuring  the \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nprobability  that  a  given  data  set  is  generated  by  a  uniform  data  distribution.  In  other  words,  it  tests \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nthe   spatial   randomness   of   the   data. \\xa0\\n\\xa0\\nFor   a   dataset   where, \\xa0\\n  is   a   set   of     data   points X n \\xa0\\n  is   a   member   of   a   random   sample   (without   replacement)   of     data   points. pi  m ≪ n \\xa0\\n  is   the   distance   of   ,   from   its   nearest   neighbour   in   xi  pi ∈ X X\\xa0\\nis   a   set   of     uniformly   randomly   distributed   data   points   (similar   to   ) Y m X\\xa0\\n  is   a   member   of   a   data   set   qi Y\\xa0\\n  is   the   distance   of     from   its   nearest   neighbour   in   yi , qi ∈ Y X\\xa0\\n\\xa0\\nHopkins   Test   is   given   by, \\xa0\\n\\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   95 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 94}),\n",
              " Document(page_content=\"The  value  of  varies  between  0  and  1.  If  were  uniformly  distributed,  then  and  would \\xa0 \\xa0\\xa0 H\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 X\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 xΣi\\xa0 \\xa0 yΣi\\xa0 \\xa0\\nbe  close  to  each  other,  and  thus  would  be  about  0.5.  However,  if  were  to  have  clusters,  then \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 H\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 X\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nthe  distances  for  artiﬁcial  points  would  be  substantially  larger  than  for  the  real  ones ,  thus \\xa0 \\xa0\\xa0 \\xa0 \\xa0 yΣi\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 xΣi\\xa0 \\xa0\\nincreasing  the  value  of .  A  value  for  between  0.01  to  0.3  indicates  the  data  is  regularly  spaced, \\xa0\\xa0 \\xa0\\xa0 H\\xa0\\xa0 \\xa0\\xa0 H\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\naround  0.5  indicates  the  data  is  random  and  more  than  0.7  indicates  a  clustering  tendency  at  90% \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nconﬁdence   level.   The   following   ﬁgure   shows   the   clustering   tendency   for   various   Hopkins   factor. \\xa0\\n\\xa0\\n\\xa0\\nPython   Code   -   K-Means   Clustering \\xa0\\nBehavioural   Segmentation   (RFM)  \\n>>   data   =   RFM[[ 'recency_col' , 'frequency_col' , 'monetary_col' ]]  \\n>>   Q1   =   data[ 'rfm_col' ].quantile(0.25)  \\n>>   Q3   =   data[ 'rfm_col' ].quantile(0.75)  \\n>>   IQR   =   Q3   -   Q1  \\n>>   data   =   data[(data[ 'rfm_col' ]   >=   Q1-1.5*IQR)   &   \\n                (data[ 'rfm_col' ]   <=   Q3+1.5*IQR)]  \\n>>   data   =   pd.DataFrame(standard_scaler.fit_transform(data))  \\n>>   data.columns   =   [ 'recency_col' , 'frequency_col' , 'monetary_col' ]  \\n \\nCluster   Tendency   Analysis   using   Hopkins   Statistics  \\n>>    from    math    import    isnan  \\n>>    from    random    import    sample  \\n>>    from    numpy.random    import    uniform  \\n>>    from    sklearn.neighbors    import    NearestNeighbors  \\n>>    def     hopkins (X):  \\n>>       d   =   X.shape[1]  \\n>>       n   =   len(X)  \\n>>       m   =   int(0.1   *   n)   \\n>>       nbrs   =   NearestNeighbors(n_neighbors=1).fit(X.values)  \\n>>       rand_X   =   sample(range(0,   n,   1),   m)  \\n>>       yi   =   []  \\n>>       xi   =   []  \\n>>        for    j    in    range(0,   m):  \\n>>           y_dist,   _   =   nbrs.kneighbors(uniform(np.amin(X,   axis=0),   \\n                        np.amax(X,   axis=0),   d).reshape(1,   -1),   2,  \\n                        return_distance= True )  \\n>>           yi.append(y_dist[0][1])  \\n>>           x_dist,   _   =   nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1,-1),  \\n                                        2,   return_distance= True )  \\n>>           xi.append(x_dist[0][1])   \\n>>       H   =   sum(yi)   /   (sum(yi)   +   sum(xi))  \\n>>        if    isnan(H):  \\n>>           print(yi,   xi)  \\n>>           H   =   0  \\n>>        return    H  \\n>>   hopkins(data)  \\n \\n \\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   96 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 95}),\n",
              " Document(page_content=\"Choosing   K   using   Silhouette   Analysis  \\n>>    from    sklearn.cluster    import    KMeans  \\n>>    from    sklearn.metrics    import    silhouette_score  \\n>>   sse   =   []  \\n>>    for    k    in    range(min_K,   max_K):  \\n>>       kmeans   =   KMeans(n_clusters=k).fit(data)  \\n>>       sse.append([k,   silhouette_score(data,   kmeans.labels_)])  \\n>>   plt.plot(pd.DataFrame(sse)[0],   pd.DataFrame(sse)[1]);  \\n \\nChoosing   K   using   Elbow   Curve  \\n>>   ssd   =   []  \\n>>    for    k    in    range(min_K,   max_K):  \\n>>       kmeans   =   KMeans(n_clusters   =   k,   max_iter=50)  \\n>>       kmeans.fit(data)  \\n>>       ssd.append(kmeans.inertia_)  \\n>>   plt.plot(ssd)  \\n \\nBuild   Model  \\n>>   model   =   KMeans(n_clusters   =   best_k,   max_iter=50)  \\n>>   model.fit(data)  \\n \\nAnalyse   Model  \\n>>   data.index   =   pd.RangeIndex(len(data.index))  \\n>>   data   =   pd.concat([data,   pd.Series(model.labels_)],   axis=1)  \\n>>   data.columns   =   [ 'id' , 'recency_col' , 'frequency_col' , 'monetary_col' , 'cluster_id' ]  \\n>>   data[ 'recency_col' ]   =   data[ 'recency_col' ].dt.days  \\n>>   cluster_R   =   pd.DataFrame(data.groupby([ 'cluster_id' ]).recency_col.mean())  \\n>>   cluster_F   =   pd.DataFrame(data.groupby([ 'cluster_id' ]).frequency_col.mean())  \\n>>   cluster_M   =   pd.DataFrame(data.groupby([ 'cluster_id' ]).monetary_col.mean())  \\n>>   data_plot   =   pd.concat([pd.Series(range(k)),cluster_R,cluster_F,   cluster_M],axis=1)  \\n>>   data_plot.columns   =   [ 'cluster_id','recency_col' , 'frequency_col' , 'monetary_col' ]  \\n>>   sns.barplot(x=data_plot[ 'cluster_id' ],   y=data_plot[ 'rfm_col' ])  \\n \\n5.1.3.   HIERARCHICAL   CLUSTERING \\xa0\\nHierarchical  clustering  is  yet  another  algorithm  for  unsupervised  clustering.  Here,  instead  of \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\npre-deﬁning  the  number  of  clusters,  one  has  to  ﬁrst  visually  describe  the  similarity  or  dissimilarity \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nbetween  the  diﬀerent  data  points  and  then  decide  the  appropriate  number  of  clusters  on  the  basis \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nof  these  similarities  or  dissimilarities.  The  output  of  the  hierarchical  clustering  algorithm  resembles \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nan   inverted   tree-shaped   structure,   called   the   dendrogram. \\xa0\\nHierarchical   Clustering   Algorithm \\xa0\\nIn  hierarchical  clustering,  the  data  is  not  partitioned  into  a  particular  cluster  in  a  single  step.  Instead, \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\na  series  of  partitions/merges  take  place,  which  may  run  from  a  single  cluster  containing  all  objects  to \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\n  clusters   with   each   cluster   containing   a   single   object. n \\xa0\\n\\xa0\\nThe   following   algorithm   is   applied   to   create   hierarchical   clusters   for   a   dataset   with     items. n \\xa0\\n\\xa0\\n1)Initialisation  :  The  distance  (similarity)  matrix  is  calculated,  which  gives  the  distance  of \\xa0\\xa0 \\xa0 n× n\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\neach   data   point   from   the   other. \\xa0\\n\\xa0\\n2)Assignment  :  The  clustering  is  started  by  assigning  each  item  to  its  own  cluster,  such  that  if \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nthere  are  items,  then  the  total  number  of  clusters  created  is  also ,  with  each  cluster \\xa0\\xa0 n\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 n\\xa0 \\xa0 \\xa0 \\xa0\\ncontaining   just   one   item. \\xa0\\n\\xa0\\n3)Optimisation  :  The  closest  (most  similar)  pair  of  clusters  is  searched  for  using  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 n× n\\xa0\\ndistance  matrix.  This  pair  is  then  merged  into  a  single  cluster,  so  that  now  there  is  one  less \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ncluster   (i.e   ). n−1\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   97 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 96}),\n",
              " Document(page_content='\\xa0\\n4)Computation  :  The  distances  (similarities)  between  the  new  cluster  and  each  of  the  old \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0\\xa0\\nclusters   are   computed. \\xa0\\n\\xa0\\n5)Iteration  :  The  process  of  optimisation  and  computation  are  repeated  until  all  the  items  are \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nclustered   into   a   single   cluster   of   size   . n\\xa0\\n\\xa0\\nThe   following   ﬁgures   are   a   graphical   representation   of   the   hierarchical   clustering   algorithm. \\xa0\\n\\xa0\\n\\xa0\\n \\xa0\\n \\xa0\\n \\xa0\\n1.   Assignment \\xa0 2.   Optimisation \\xa0 3.   Computation \\xa0 4.   Final   Result \\xa0\\n\\xa0\\nThus,  at  the  end  a  dendrogram  is  formed,  that  shows  which  data  points  are  grouped  together  in \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nwhich  cluster  at  what  distance.  One  can  look  at  what  stage  an  element  is  joining  a  cluster  and  hence \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nsee  how  similar  or  dissimilar  it  is  to  the  rest  of  the  cluster  (if  it  joins  at  the  higher  height,  it  is  quite \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\ndiﬀerent  from  the  rest  of  the  group).  The  following  ﬁgure  shows  a  dendrogram,  that  starts  with  all  the \\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0\\ndata  points  as  separate  clusters  and  indicates  at  what  level  of  dissimilarity  any  two  clusters  were \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\njoined. \\xa0\\n\\xa0\\n\\xa0\\nThe  y-axis  of  the  dendrogram  is  some  measure  of  dissimilarity  or  distance  at  which  clusters  join.  In \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nthe  dendrogram  given,  samples  5  and  7  are  the  most  similar  and  so  join  to  form  the  ﬁrst  cluster, \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\nfollowed  by  samples  1  and  10.  The  last  two  clusters  to  fuse  together  to  form  the  ﬁnal  single  cluster \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nare   10-9-1-5-7-8   and   2-3-6-4.   There   are   two   types   of   hierarchical   clustering   algorithms. \\xa0\\n\\xa0\\n1)Agglomerative  :  It  is  a  bottom-up  algorithm  which  starts  with  distinct  clusters  and \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 n\\xa0 \\xa0 \\xa0 \\xa0\\niteratively  merge  (or  agglomerate)  pairs  of  clusters  (which  have  the  smallest  dissimilarity) \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nuntil   all   clusters   have   been   merged   into   a   single   cluster. \\xa0\\n\\xa0\\n2)Divisive  :  It  is  a  top-down  algorithm  which  proceeds  by  splitting  (or  dividing)  a  single  cluster \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\n(resulting  in  the  biggest  dissimilarity)  recursively  until  distinct  clusters  are  reached.  Even \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 n\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nthough  divisive  algorithms  are  more  complex  than  agglomerative  algorithms,  they  produce \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmore  accurate  hierarchies  in  certain  scenarios.  The  bottom-up  algorithms  make  clustering \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   98 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 97}),\n",
              " Document(page_content='decisions  based  on  local  patterns  without  initially  taking  into  account  the  global  distribution, \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nwhich  cannot  be  undone.  Whereas,  top-down  algorithms  beneﬁt  from  having  the  complete \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\ninformation   about   the   global   distribution   while   partitioning. \\xa0\\nLinkages \\xa0\\nIn  a  clustering  algorithm,  before  any  clustering  is  performed,  it  is  required  to  determine  the  proximity \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nmatrix  (displaying  the  distance  between  each  cluster)  using  a  distance  function.  This  proximity  or \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nlinkage   between   any   two   clusters   can   be   measured   by   the   following   three   distance   functions. \\xa0\\n\\xa0\\nSingle   Linkage \\xa0 Complete   Linkage \\xa0 Average   Linkage \\xa0\\nThe  distance  between  two \\xa0 \\xa0 \\xa0 \\xa0\\nclusters  is  deﬁned  as  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nshortest  distance  between  two \\xa0 \\xa0 \\xa0 \\xa0\\npoints   in   each   cluster. \\xa0The  distance  between  two \\xa0 \\xa0 \\xa0 \\xa0\\nclusters  is  deﬁned  as  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nlongest  distance  between  two \\xa0 \\xa0 \\xa0 \\xa0\\npoints   in   each   cluster. \\xa0The  distance  between  two \\xa0 \\xa0 \\xa0 \\xa0\\nclusters  is  deﬁned  as  the \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\naverage  distance  between \\xa0 \\xa0 \\xa0\\neach  point  in  one  cluster  to \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\nevery   point   in   the   other   cluster. \\xa0\\n\\xa0\\n \\xa0\\n \\xa0\\n\\xa0\\n \\xa0\\n\\xa0\\nIt  suﬀers  from  chaining.  In  order \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nto  merge  two  groups,  only  one \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\npair  of  points  is  required  to  be \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\nnear  to  each  other,  irrespective \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nof  the  location  of  all  other \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\npoints.  Therefore  clusters  can \\xa0 \\xa0 \\xa0 \\xa0\\nbe  too  spread  out,  and  not \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ncompact   enough. \\xa0It  suﬀers  from  crowding.  Its \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nscore  is  based  on  the  worst \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\ncase  dissimilarity  between \\xa0 \\xa0 \\xa0\\npairs,  making  a  point  to  be \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0\\ncloser  to  points  in  other  clusters \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\nthan  to  points  in  its  own  cluster. \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nThus,  clusters  are  compact,  but \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nnot   far   enough   apart. \\xa0It  tries  to  strike  a  balance \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nbetween  the  two,  by  using \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nmean  pairwise  dissimilarity.  So \\xa0 \\xa0 \\xa0\\xa0\\nthe  clusters  tend  to  be \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nrelatively  compact  as  well  as \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nrelatively   far   apart. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nThe  type  of  linkage  to  be  used  is  usually  decided  after  having  a  look  at  the  data.  One  convenient \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nway  to  decide  is  by  looking  at  how  the  dendrogram  looks.  Usually,  single  linkage  types  produce \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\ndendrograms  which  are  not  structured  properly,  whereas  complete  or  average  linkage  produce \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\ndendrograms   which   have   a   proper   tree-like   structure. \\xa0\\nDendrogram \\xa0\\nDetermining  the  number  of  groups  in  a  cluster  analysis  is  usually  the  primary  goal.  To  determine  the \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\ncutting   section,   various   methods   can   be   used. \\xa0\\n\\xa0\\xa0\\n1)By  means  of  observation  or  experience  based  on  the  knowledge  of  the  researcher.  For \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\nexample,   based   on   appearance   diﬀerences,   clusters   can   be   divided   into   groups. \\xa0\\n2)Using  statistical  conventions,  the  dendrogram  can  be  cut  where  the  diﬀerence  is  most \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nsigniﬁcant.  Another  technique  is  to  use  the  square  root  of  the  number  of  observations.  Yet \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\nanother   technique   is   to   use   at   least   70%   of   the   distance   between   the   two   groups. \\xa0\\n3)Using   the   function   discrimination   and   classiﬁcation   based   on   discrimination   function. \\xa0\\n\\xa0\\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   99 \\xa0\\n\\xa0', metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 98}),\n",
              " Document(page_content=\"Thus,  the  use  of  the  method  for  cutting  a  dendrogram  depends  on  the  purpose.  Typically,  one  can \\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nlook  for  natural  groupings  deﬁned  by  long  stems.  Once  the  appropriate  level  is  decided  upon,  one \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\ncan  obtain  the  clusters  by  cutting  the  dendrogram  at  that  appropriate  level.  The  number  of  vertical \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\nlines  intersecting  the  cutting  line  represents  the  number  of  clusters.  The  following  ﬁgures  show  the \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\nvarious   clusters   obtained   at   various   levels   of   dissimilarity. \\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\nOne  of  the  major  advantages  of  Hierarchical  clustering  is  that  the  number  of  clusters  is  not \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 K\\xa0\\xa0\\xa0\\nrequired  to  be  pre-deﬁned  as  is  the  case  in  K-Means  clustering.  The  Hierarchical  clusterings  usually \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\nproduce  better  clusters,  but  are  time-consuming  and  computationally  intensive,  due  to  which  the \\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\nK-Means   clusterings   are   generally   used   for   larger   datasets. \\xa0\\n\\xa0\\nWhile  performing  a  clustering  activity,  it  is  always  advisable  to  ﬁrst  use  the  Hierarchical  clustering \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\n(dendrograms)  to  visualize  the  clusters  from  a  business  standpoint  and  determine  the  right  value  of \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0\\n.  Then  calculate  the  centroids  of  these  clusters  using  the  mean  of  each  Hierarchical  cluster. K\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 K\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\nAnd  ﬁnally  use  K-Means  clustering  with  being  the  number  of  clusters  and  the  computed  centroids \\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0 K\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0 \\xa0\\nas  the  seeds  for  these  clusters  (helps  the  algorithm  to  converge  faster  than  in  case  of  random \\xa0\\xa0 \\xa0\\xa0 \\xa0 K\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nseed   selection). \\xa0\\nPython   Code   -   Hierarchical   Clustering \\xa0\\nBehavioural   Segmentation   (RFM)  \\n>>   data   =   RFM[[ 'recency_col' , 'frequency_col' , 'monetary_col' ]]  \\n>>   Q1   =   data[ 'rfm_col' ].quantile(0.25)  \\n>>   Q3   =   data[ 'rfm_col' ].quantile(0.75)  \\n>>   IQR   =   Q3   -   Q1  \\n>>   data   =   data[(data[ 'rfm_col' ]   >=   Q1-1.5*IQR)   &   \\n                (data[ 'rfm_col' ]   <=   Q3+1.5*IQR)]  \\n>>   data   =   pd.DataFrame(standard_scaler.fit_transform(data))  \\n>>   data.columns   =   [ 'recency_col' , 'frequency_col' , 'monetary_col' ]  \\n \\nBuild   Model  \\n>>    from    scipy.cluster.hierarchy    import    linkage  \\n>>    from    scipy.cluster.hierarchy    import    dendrogram  \\n>>   model   =   linkage(data,   method   =    'single' ,   metric= 'euclidean' )  \\n#   method   can   be   any   of   the   values   single,   complete   or   average  \\n>>   dendrogram(model)  \\n>>   plt.show()  \\n \\nCut   Dendrogram  \\n>>    from    scipy.cluster.hierarchy    import    cut_tree  \\n>>   clusterCut   =   pd.Series(cut_tree(model,   n_clusters   =   5).reshape(-1,))  \\n \\nAnalyse   Model  \\n>>   data   =   pd.concat([RFM,   clusterCut],   axis=1)  \\nNotes   by   Aniket   Sahoo   -   Part   II   (Machine   Learning)   -   100 \\xa0\\n\\xa0\", metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 99})]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "Sl6_VDYulc-U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size =500, chunk_overlap=20)\n",
        "text_chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "YtgWUnspmaXL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Wf2-mXSnoHG",
        "outputId": "2b9a5338-9967-4fb0-f9fd-86d0e478b959"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "673"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name =\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589,
          "referenced_widgets": [
            "cb8e472370e345b9b2a833d03f69f789",
            "09567ad1314141bf8eab4275f770107f",
            "397fe42b9f384cc587ca2f78a96743ab",
            "6f8c15e60a7040ee8ea830ffaf94c593",
            "faf9c60def3a47ee81d2dc78a4402a91",
            "397bfe3379134a0e9ad1c66012cbf349",
            "6f2372f49171442381ad5ff28e112dd2",
            "10aa8151120e42b399c3958dd02784d5",
            "f8446bb9f13d4fdc80e2127f887191a9",
            "2a5b17b01cb6410c89870c2790fd0e85",
            "7506a00e8f9340e6b181043182c502a6",
            "5227030ca1864775b5f6dcc21044cdc3",
            "c3f7b24ee5c147b39a1b5122781a309d",
            "26618df17faf4d35a7a6a1fa97a8b9df",
            "aa3d4310e7ef4ea89de53bda4a74b0c7",
            "3d30882baad04d358c83a00ed8d008d7",
            "d294acf8df1a458783f31d1226ed8b44",
            "0792e429d9484f9fb3a9e2e472bc02e3",
            "2db0606d1bea4459a186baa294b7bcc7",
            "ac638640f91e4800822956b87d3239ea",
            "778b277d133e4cf4ba8ae4da65901638",
            "0e956086344d400a85f814081651aee2",
            "df52491bd986457aa27bb4100ade5677",
            "c7255ea98dcb4d3e8bd6bf5a50555b74",
            "978b4aba613a4cf387946ca0a3d7cff8",
            "03caf456fb564346b45a4a8a70f8e06f",
            "f4febbfff90945f192a90c419462b19d",
            "15db6466426b4e3ba638e60ff411c543",
            "cdc694e4d0134c7f9dab1422ea34d020",
            "98865fda13c14944a971de0de60a8097",
            "a02292c8c604481487309b5c3a6c4676",
            "88ff52bafcdd4cd288ecf48882b02b8f",
            "05d62dbb8c694b21ab445eb38afd4436",
            "e0cb73e4be69443bb931dcb0aca6def9",
            "5c9469a9650946aea4b3e1dd813774b1",
            "007385dd81034d92ab6f76f76400ced3",
            "66c2e309fd1e4ee0a1cb77d130e3511c",
            "af7a72d87ec040709a036da0c1571c39",
            "c53b884d783a4a27871ce278ed92dbfc",
            "f57901b44fba4880bfa45f20003f37ed",
            "01bc79bc8ad24235923639301068702e",
            "8406c6f322224d2790dbf14407f13a0b",
            "2771448e7ded4f8f9e6a0d331ea63780",
            "0417e52414c64222808c1b903173acf0",
            "c30cf6da10e24c30a65674142f88dee4",
            "24d6b291db8446e0b778c61995b17457",
            "7621752382864507a30ddef4a75c8e77",
            "512d71ccc9544cccac2b5a8d2705e23e",
            "65ffc66fd4e14f2aacdfd2707de67df4",
            "19bc829741a24ca9ab5fadb98013e3fc",
            "1cad8ee2fd6f4be59fb4f52e8e624db8",
            "318316a94fd44803a8435c8da0c392ca",
            "1c3b231767124a19af231b3bcb61b6ee",
            "18f62ac8ca534be08dac1ab16bf67c93",
            "8ff1dd7b752c4443895c509e4ecc2720",
            "d2e663e45b6041a9ad0b5fca4ef9f261",
            "51edd47fc6214398aaccd87352e4dc36",
            "e54e2a224f97442ea4afc01120a18809",
            "91901bf8621a4b45b52fa5bff3cf3e7b",
            "0075ac14bc95458ea53d0a948a3fb1dc",
            "3317f7e2deb84095b6434be4c1588b6a",
            "47193aecd54549be926312fc710c5789",
            "c23b783f1b804d0caaba96feb796f426",
            "f1a04e49577f411d9d4b622c2efddd5c",
            "71591a3ef5f04259952e00bd8da46a71",
            "88127338b82d411e908ed32e99070090",
            "9a3e6f3d48b04b25989b5e62e4ab5304",
            "d64cf7495e46437187e4402d2ca3082f",
            "cef50578436743028fdc32524a0bbd50",
            "32831d329bd04b8abe70e64bb5750a54",
            "7caf9eea6db946b69482f3d3ca10a69c",
            "c0d7962454a54451be55ef971fdbfcea",
            "6d0a079eee8a4c509971f3dc5e5f4a62",
            "a427911a681e4460b69cb4774f18ea82",
            "5ea85e1ca1664d059ec9988ebf425cb9",
            "b5dd4be2075f41dd9cf9cd47591fde85",
            "f9877b76aab6471086a057ebc979c5fe",
            "b5cfe3eb788949f58193274c6e2fad5e",
            "431e38e75a1341019d1e92ade09d5f1e",
            "1362fcb72b1844a495a222f7701f90c7",
            "4d22fbed7f0b44a7afd698ad7069e291",
            "7e108204cfad4c3e8c2308dc96bd3592",
            "e0d44655a91a410e8554781d320d6367",
            "09edb0c6326c4f46b67f1425cccd8545",
            "7fed6b61a97d4fa3891d36aefafbaf14",
            "2c3e7040e19940bea5983bf7c4f9f4c3",
            "03637d303d99421aaaa6d4f6b01a4a08",
            "975b5406be4647279fed3efcca42842c",
            "6bb84579724e4432b8292a1f6ac1707e",
            "9cd684600fac46c99b4c9cce4cddac0a",
            "31a7db3a2c7b445089c7a2a63b93d3d0",
            "1c79e0f887104e2eb63ddcef2d8260f3",
            "7252e51231a14db299c583e2ef079657",
            "82beeabe720e4104a4910acaf5e6c0d6",
            "4bbeb15f2e85415faa1fa54dd98e791b",
            "4c0af801c7744939b574edd057b5f94e",
            "4b2a3b5eee3c4334942b0f0cdd66df94",
            "ccd9ff1c36c745edb729786061f71a4b",
            "628726826b5643f7a95a935e8b4c03e3",
            "4c867c221a77468c9ef731308c248c26",
            "d5bdfff3d37442378b5b8f56ff526d37",
            "4a1c2d7ced3a41109f327dea519d1b09",
            "274cace8f8fd43b399833ee2ed6fbd85",
            "cd03ad445d6c4718800a907629e2f12f",
            "80213c5ca7e54e5e8e107679210e45a9",
            "f8dc458322764e8bb7951d3222a84ca2",
            "d56bc2a4f6f94911868e0aebd26dd1f8",
            "90ce4b826aae487bb2032efcb7c37c8a",
            "cae44044b1054e478d344c81de79ea48",
            "c8004e5534ab45c989cd081e1d89e18b",
            "bf1599f745154dbba949e6b8e2148441",
            "894d5099e86f4389919259043b5e10a0",
            "d1beb4e9db0e44b0aefdf55083ca06ae",
            "d4711df62b824bf3b42f339b0c3b1b7d",
            "c2059a826c6c46038236119d54b5c1eb",
            "2f1a5fda4b594287a1c7394932c596f4",
            "f5f456b369af4099839cf14b428dc424",
            "bb945aeafc284173a5969ace0d693f0c",
            "df4683d4fda4459c8088ec64532de9b2",
            "8bed34ffd6c54b6b95ca5d6f2e3d2472",
            "9f5f82bcb0574dfe94fe11274ed92ab9",
            "2efeb87f56d34be09b9cfa27e7dc8418",
            "db76ea05231b41c2a6f9a3080740698f",
            "c4fcf186b0f5429aab295acc71083c4c",
            "915ebac976bc4bdd820cd5389f345196",
            "8016cba202e049ce991204430135b94b",
            "741ef82a94994200b1cb52cecb8814a7",
            "af4be8fbbb0b48459aca42c5de95c150",
            "d3a90d3933dc498e8e60765fdf67f124",
            "530785cedf714e2cab8126ee841a7413",
            "d58b54cf12d84e84a18c492864c030de",
            "3e05badb35a149bead679f2c256a92a7",
            "d013cbfb43324f0488f798a1b2f2ec3b",
            "79370d11935242c1a7716d7742f62976",
            "7c656bc8412d4debb4f1042312050306",
            "ba5dda4601834cb9b3736e4d7ced1342",
            "1ed9fd650b524c659311c6e043893f56",
            "f6e40c7b28b147869de7a6d82b3f2b68",
            "b20aec39c5a141f083ab501a1f976abb",
            "00f45eab4a354b9ab45ab71c3c5b87d1",
            "951140055c624ddfb0db286dc0a03c83",
            "09a467655d454e2bb56061faed4552c3",
            "a077f58b612048a898775597324a7ef4",
            "7165f70572db4e9188f6695a095e8813",
            "69992c3c479a48e190864630bc2f72bc",
            "fe3590caf6a2470c9af82274432c4306",
            "8eae510886f6435e8646dee360c7d81b",
            "b71458cfa8294d30a7ba45dce142be48",
            "80ee952c19d340b791cf4eb7b45a829f",
            "ff29225b5d4641e5a2f77afaaafb2967",
            "2733eaee805d4ea589bbf6f608ebf03b",
            "8ba83936aca6462abf110cf78f4da9fb",
            "4fbc7fe87b3e44a295e6e1fe233bf670",
            "1e2e0708a7e74c3b953125d04196c999"
          ]
        },
        "id": "IpY4FbilnsG_",
        "outputId": "7e0083c7-424c-47fa-88c4-f03e5d1ce4e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb8e472370e345b9b2a833d03f69f789"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5227030ca1864775b5f6dcc21044cdc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df52491bd986457aa27bb4100ade5677"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0cb73e4be69443bb931dcb0aca6def9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c30cf6da10e24c30a65674142f88dee4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2e663e45b6041a9ad0b5fca4ef9f261"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a3e6f3d48b04b25989b5e62e4ab5304"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5cfe3eb788949f58193274c6e2fad5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bb84579724e4432b8292a1f6ac1707e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c867c221a77468c9ef731308c248c26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf1599f745154dbba949e6b8e2148441"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2efeb87f56d34be09b9cfa27e7dc8418"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d013cbfb43324f0488f798a1b2f2ec3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7165f70572db4e9188f6695a095e8813"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "vectorstore = FAISS.from_documents(text_chunks, embeddings)"
      ],
      "metadata": {
        "id": "zdqDQmIgoPWK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is k means clustering\"\n",
        "\n",
        "docs = vectorstore.similarity_search(query,k=3)"
      ],
      "metadata": {
        "id": "AkADUFQuogNW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvU-2sdZouqY",
        "outputId": "77e9a195-da27-44d9-fe8b-70c800d01414"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='purchase) \\xa0CDJ   :   Consumer   Decision \\xa0\\nJourney \\xa0\\n\\xa0\\nBased   on   the   customer’s   life \\xa0\\njourney   with   the   product. \\xa0\\n\\xa0\\n5.1.2.   K-MEANS   CLUSTERING \\xa0\\nK-means  clustering  is  a  simple  yet  elegant  approach  for  partitioning  a  data  set  into  K  distinct, \\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0 \\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\xa0 \\xa0\\nnon-overlapping  clusters.  In  order  to  perform  K-means  clustering,  one  must  ﬁrst  specify  the  desired' metadata={'source': 'pdfs/2  Machine Learning Notes.pdf', 'page': 89}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('openai_api_key')"
      ],
      "metadata": {
        "id": "IMw5VNq-pE4O"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
      ],
      "metadata": {
        "id": "vaQJfm1xovxY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI()"
      ],
      "metadata": {
        "id": "fKuId5kfpJ85"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa = RetrievalQA.from_chain_type(llm=llm , chain_type='stuff', retriever = vectorstore.as_retriever())"
      ],
      "metadata": {
        "id": "-oBPE5q1pMRj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa.run(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YSSijzGpvep",
        "outputId": "26fbf7eb-fd90-497f-f384-c342ff8837a0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " K-means clustering is a method used for partitioning a dataset into K distinct, non-overlapping clusters. It involves selecting the desired number of clusters (K) and assigning each observation to exactly one of those clusters. The algorithm works by calculating the squared Euclidean distance between each data point and the centroid of each cluster, and then assigning the data point to the cluster with the nearest centroid. The value of K is determined beforehand and can greatly impact the quality of the resulting clusters. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NHtSkZYxp1KK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}