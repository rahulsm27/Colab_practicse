{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVuc/7JXGMvwDEujVYq/Hz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulsm27/Colab_practicse/blob/main/HDFS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmNXMu8R6zUG",
        "outputId": "0b95510e-d970-4c07-88d4-2088c1549c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-26 13:21:30--  https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 730107476 (696M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.6.tar.gz’\n",
            "\n",
            "hadoop-3.3.6.tar.gz 100%[===================>] 696.28M  25.4MB/s    in 28s     \n",
            "\n",
            "2024-03-26 13:21:59 (24.5 MB/s) - ‘hadoop-3.3.6.tar.gz’ saved [730107476/730107476]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf hadoop-3.3.6.tar.gz"
      ],
      "metadata": {
        "id": "bHa1wCgY69c3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To find the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91fK88qd7BhI",
        "outputId": "bdb935ca-de0d-4a3f-b2c1-5ab0d6d89cde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Running Hadoop\n",
        "!hadoop-3.3.6/bin/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTb023wb7D9K",
        "outputId": "d98f1b65-19c7-4d66-a11c-acf0b90cb8f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in worker mode\n",
            "hosts filename                   list of hosts to use in worker mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN applications, not this\n",
            "              command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       S3 Commands\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp /usr/local/hadoop-3.3.0/etc/hadoop/*.xml ~/input\n",
        "!hadoop-3.3.6/bin/hadoop jar hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
        "-input ratings.txt -output output12 \\\n",
        "-file mapper.py -mapper 'python mapper.py' \\\n",
        "-file reducer.py -reducer 'python reducer.py'\n",
        "#!cat /output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SSj7lET7IxU",
        "outputId": "01a23afd-b403-4f9a-990a-09387be8773f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-26 13:24:08,444 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [mapper.py, reducer.py] [] /tmp/streamjob6086342935373207008.jar tmpDir=null\n",
            "2024-03-26 13:24:09,369 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-26 13:24:09,609 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-26 13:24:09,609 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-26 13:24:09,640 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-26 13:24:09,918 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-26 13:24:09,952 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-26 13:24:10,444 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local865646808_0001\n",
            "2024-03-26 13:24:10,445 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-26 13:24:10,910 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local865646808_0001_ff7ef174-57ed-412e-a4df-2c53e8c720c9/mapper.py\n",
            "2024-03-26 13:24:10,969 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local865646808_0001_c2b8bfb8-8ed3-4c2c-b003-227a236d8d2d/reducer.py\n",
            "2024-03-26 13:24:11,067 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-26 13:24:11,070 INFO mapreduce.Job: Running job: job_local865646808_0001\n",
            "2024-03-26 13:24:11,076 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-26 13:24:11,079 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-26 13:24:11,091 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-26 13:24:11,091 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-26 13:24:11,178 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-26 13:24:11,183 INFO mapred.LocalJobRunner: Starting task: attempt_local865646808_0001_m_000000_0\n",
            "2024-03-26 13:24:11,258 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-26 13:24:11,258 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-26 13:24:11,288 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-26 13:24:11,302 INFO mapred.MapTask: Processing split: file:/content/ratings.txt:0+5211793\n",
            "2024-03-26 13:24:11,318 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-26 13:24:11,437 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-26 13:24:11,437 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-26 13:24:11,437 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-26 13:24:11,438 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-26 13:24:11,438 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-26 13:24:11,442 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-26 13:24:11,453 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n",
            "2024-03-26 13:24:11,465 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-26 13:24:11,467 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-26 13:24:11,468 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-26 13:24:11,469 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-26 13:24:11,470 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-26 13:24:11,471 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-26 13:24:11,475 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-26 13:24:11,476 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-26 13:24:11,476 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-26 13:24:11,477 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-26 13:24:11,477 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-26 13:24:11,478 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-26 13:24:11,518 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:11,519 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:11,521 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:11,534 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:11,613 INFO streaming.PipeMapRed: Records R/W=2653/1\n",
            "2024-03-26 13:24:11,728 INFO streaming.PipeMapRed: R/W/S=10000/18958/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:12,077 INFO mapreduce.Job: Job job_local865646808_0001 running in uber mode : false\n",
            "2024-03-26 13:24:12,079 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2024-03-26 13:24:12,537 INFO streaming.PipeMapRed: R/W/S=100000/203411/0 in:100000=100000/1 [rec/s] out:203411=203411/1 [rec/s]\n",
            "2024-03-26 13:24:12,554 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-26 13:24:12,555 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-26 13:24:12,559 INFO mapred.LocalJobRunner: \n",
            "2024-03-26 13:24:12,559 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-26 13:24:12,559 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-26 13:24:12,559 INFO mapred.MapTask: bufstart = 0; bufend = 13665178; bufvoid = 104857600\n",
            "2024-03-26 13:24:12,559 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 25380092(101520368); length = 834305/6553600\n",
            "2024-03-26 13:24:13,257 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-26 13:24:13,275 INFO mapred.Task: Task:attempt_local865646808_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-26 13:24:13,280 INFO mapred.LocalJobRunner: Records R/W=2653/1\n",
            "2024-03-26 13:24:13,281 INFO mapred.Task: Task 'attempt_local865646808_0001_m_000000_0' done.\n",
            "2024-03-26 13:24:13,292 INFO mapred.Task: Final Counters for attempt_local865646808_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=5216747\n",
            "\t\tFILE: Number of bytes written=14728040\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=100755\n",
            "\t\tMap output records=208577\n",
            "\t\tMap output bytes=13665178\n",
            "\t\tMap output materialized bytes=14082945\n",
            "\t\tInput split bytes=77\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=208577\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=5211793\n",
            "2024-03-26 13:24:13,292 INFO mapred.LocalJobRunner: Finishing task: attempt_local865646808_0001_m_000000_0\n",
            "2024-03-26 13:24:13,293 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-26 13:24:13,300 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-26 13:24:13,304 INFO mapred.LocalJobRunner: Starting task: attempt_local865646808_0001_r_000000_0\n",
            "2024-03-26 13:24:13,318 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-26 13:24:13,319 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-26 13:24:13,319 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-26 13:24:13,327 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2d0474e7\n",
            "2024-03-26 13:24:13,329 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-26 13:24:13,361 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-26 13:24:13,373 INFO reduce.EventFetcher: attempt_local865646808_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-26 13:24:13,458 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local865646808_0001_m_000000_0 decomp: 14082941 len: 14082945 to MEMORY\n",
            "2024-03-26 13:24:13,501 INFO reduce.InMemoryMapOutput: Read 14082941 bytes from map-output for attempt_local865646808_0001_m_000000_0\n",
            "2024-03-26 13:24:13,514 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 14082941, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->14082941\n",
            "2024-03-26 13:24:13,520 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-26 13:24:13,522 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-26 13:24:13,522 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-26 13:24:13,533 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-26 13:24:13,533 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 14082901 bytes\n",
            "2024-03-26 13:24:13,769 INFO reduce.MergeManagerImpl: Merged 1 segments, 14082941 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-26 13:24:13,769 INFO reduce.MergeManagerImpl: Merging 1 files, 14082945 bytes from disk\n",
            "2024-03-26 13:24:13,771 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-26 13:24:13,771 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-26 13:24:13,772 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 14082901 bytes\n",
            "2024-03-26 13:24:13,773 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-26 13:24:13,782 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
            "2024-03-26 13:24:13,787 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-03-26 13:24:13,791 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-03-26 13:24:13,817 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:13,818 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:13,820 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:13,833 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:13,916 INFO streaming.PipeMapRed: Records R/W=1944/1\n",
            "2024-03-26 13:24:13,994 INFO streaming.PipeMapRed: R/W/S=10000/7116/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:14,083 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-03-26 13:24:14,640 INFO streaming.PipeMapRed: R/W/S=100000/98255/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-26 13:24:15,064 INFO streaming.PipeMapRed: R/W/S=200000/197464/0 in:200000=200000/1 [rec/s] out:197464=197464/1 [rec/s]\n",
            "2024-03-26 13:24:15,101 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-26 13:24:15,102 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-26 13:24:15,104 INFO mapred.Task: Task:attempt_local865646808_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-26 13:24:15,106 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-26 13:24:15,106 INFO mapred.Task: Task attempt_local865646808_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-26 13:24:15,108 INFO output.FileOutputCommitter: Saved output of task 'attempt_local865646808_0001_r_000000_0' to file:/content/output12\n",
            "2024-03-26 13:24:15,117 INFO mapred.LocalJobRunner: Records R/W=1944/1 > reduce\n",
            "2024-03-26 13:24:15,117 INFO mapred.Task: Task 'attempt_local865646808_0001_r_000000_0' done.\n",
            "2024-03-26 13:24:15,118 INFO mapred.Task: Final Counters for attempt_local865646808_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=33382669\n",
            "\t\tFILE: Number of bytes written=37009132\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=54969\n",
            "\t\tReduce shuffle bytes=14082945\n",
            "\t\tReduce input records=208577\n",
            "\t\tReduce output records=208579\n",
            "\t\tSpilled Records=208577\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=331350016\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=8198147\n",
            "2024-03-26 13:24:15,118 INFO mapred.LocalJobRunner: Finishing task: attempt_local865646808_0001_r_000000_0\n",
            "2024-03-26 13:24:15,118 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-26 13:24:16,085 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-26 13:24:16,086 INFO mapreduce.Job: Job job_local865646808_0001 completed successfully\n",
            "2024-03-26 13:24:16,101 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=38599416\n",
            "\t\tFILE: Number of bytes written=51737172\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=100755\n",
            "\t\tMap output records=208577\n",
            "\t\tMap output bytes=13665178\n",
            "\t\tMap output materialized bytes=14082945\n",
            "\t\tInput split bytes=77\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=54969\n",
            "\t\tReduce shuffle bytes=14082945\n",
            "\t\tReduce input records=208577\n",
            "\t\tReduce output records=208579\n",
            "\t\tSpilled Records=417154\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=662700032\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=5211793\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=8198147\n",
            "2024-03-26 13:24:16,102 INFO streaming.StreamJob: Output directory: output12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Me3WnJ1z7mJo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}