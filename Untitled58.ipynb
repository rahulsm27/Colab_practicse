{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNucRXcnpTowVVQGm27bDki",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulsm27/Colab_practicse/blob/main/Untitled58.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install -q swig\n",
        "!pip install -q gymnasium[box2d]"
      ],
      "metadata": {
        "id": "lQOMeChdGkDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Yh3lXqeDF6bK",
        "outputId": "a86a7d33-733e-4b34-c000-047124467b15"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "DependencyNotInstalled",
          "evalue": "box2D is not installed, run `pip install gym[box2d]`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mBox2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     from Box2D.b2 import (\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-24eb917ded01>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbox2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipedal_walker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBipedalWalker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBipedalWalkerHardcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar_racing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarRacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlunar_lander\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLunarLander\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"box2D is not installed, run `pip install gym[box2d]`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import imageio\n",
        "from gym.envs import box2d\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "#### If you encounter error related to \"libiomp5md.dll\", you can try the code in the next line.\n",
        "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:  \", device)\n",
        "# ==================================================================\n",
        "#  Define Basic Classes\n",
        "#  - Finish get_state_values() function in class DQN\n",
        "#  - Finish forward() function and get_state_values() function in DuelingDQN class for bonus\n",
        "# ==================================================================\n",
        "\n",
        "#### Definition of Standard Experience Replay Buffer\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'terminated'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# class PriorityReplayMemory():\n",
        "#     def __init__(self, capacity):\n",
        "#         self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "#     def _getPriority(self, error):\n",
        "#         return (error + self.e) ** self.a\n",
        "\n",
        "#     def push(self, transistion):\n",
        "#         p = self._getPriority(error)\n",
        "#         self.memory.append(p, sample)\n",
        "\n",
        "class PrioritizedReplayMemory:\n",
        "    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_annealing_steps=1000):\n",
        "        self.capacity = capacity\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.beta_annealing_steps = beta_annealing_steps\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "        self.priorities = SumSegmentTree(capacity)\n",
        "        self.min_priorities = MinSegmentTree(capacity)\n",
        "        self.index = 0\n",
        "        self.experience_count = 0\n",
        "\n",
        "    def push(self, transition):\n",
        "        max_priority = self.priorities.max() if self.memory else 1.0\n",
        "        self.memory.append(transition)\n",
        "        self.priorities[self.index] = max_priority\n",
        "        self.min_priorities[self.index] = max_priority\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "        self.experience_count += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.memory) == self.capacity:\n",
        "            priorities = self.priorities\n",
        "        else:\n",
        "            priorities = self.priorities[:self.experience_count]\n",
        "\n",
        "        total_priority = priorities.sum()\n",
        "        batch_indices = []\n",
        "        segment = total_priority / batch_size\n",
        "        priorities = np.array(priorities)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            lower_bound = segment * i\n",
        "            upper_bound = segment * (i + 1)\n",
        "            sample_priority = random.uniform(lower_bound, upper_bound)\n",
        "            batch_indices.append(self.priorities.find_prefixsum_idx(sample_priority))\n",
        "\n",
        "        batch = [self.memory[idx] for idx in batch_indices]\n",
        "        weights = (self.capacity * np.power(priorities[batch_indices] / total_priority, -self.beta)).tolist()\n",
        "\n",
        "        return batch, batch_indices, weights\n",
        "\n",
        "    def update_priorities(self, batch_indices, priorities):\n",
        "        for idx, priority in zip(batch_indices, priorities):\n",
        "            self.priorities[idx] = priority\n",
        "            self.min_priorities[idx] = priority\n",
        "\n",
        "    def anneal_beta(self, current_step):\n",
        "        self.beta = min(1.0, self.beta + (1.0 - self.beta) * current_step / self.beta_annealing_steps)\n",
        "\n",
        "\n",
        "#### Definition of Standard DQN network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.L1 = nn.Linear(n_observations, 64)\n",
        "        self.L2 = nn.Linear(64, 64)\n",
        "        self.L3 = nn.Linear(64, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.L1(x))\n",
        "        x = F.relu(self.L2(x))\n",
        "        return self.L3(x)\n",
        "\n",
        "    def get_state_action_values(self, state_batch, action_batch):\n",
        "        q_values = self(state_batch)\n",
        "        row_index = torch.arange(0, state_batch.shape[0])\n",
        "        selected_actions_q_values = q_values[row_index, action_batch]\n",
        "        return selected_actions_q_values\n",
        "\n",
        "    def get_state_values(self, state_batch):\n",
        "        q_values = self(state_batch)\n",
        "        # Get the maximum Q value for each state\n",
        "        max_q_values, _ = q_values.max(dim=1)\n",
        "        return max_q_values\n",
        "\n",
        "#### Definition of Dueling Networkss\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        # self.feature_layer = nn.Linear(n_observations, 64)\n",
        "\n",
        "        # Value stream\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(n_observations, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # Advantage stream\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(n_observations, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        value = self.value_stream(x)\n",
        "        advantages = self.advantage_stream(x)\n",
        "\n",
        "        # Subtracting the mean of the advantages to make them have zero mean\n",
        "        # This ensures the Q values are properly centered around the value function\n",
        "        Q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
        "\n",
        "        return Q_values\n",
        "\n",
        "    # Get Q(s,a)\n",
        "    def get_state_action_values(self, state_batch, action_batch):\n",
        "        q_values = self(state_batch)\n",
        "        row_index = torch.arange(0, state_batch.shape[0])\n",
        "        selected_actions_q_values = q_values[row_index, action_batch]\n",
        "        return selected_actions_q_values\n",
        "\n",
        "    def get_state_values(self, state_batch):\n",
        "        # Forward pass through the network to get the Q values for each action\n",
        "        Q_values = self.forward(state_batch)\n",
        "\n",
        "        # Find the maximum Q value among all actions for each state\n",
        "        max_state_values, _ = Q_values.max(dim=1)\n",
        "\n",
        "        return max_state_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#=========================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ===================================\n",
        "#  Hyperparameters\n",
        "# ===================================\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.003\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.002\n",
        "LR = 1e-4\n",
        "# ==================================\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "#  Initialize Environment and Networks\n",
        "# =====================================================================\n",
        "env = gym.make('LunarLander-v2')\n",
        "#### Get the dimension of action and state\n",
        "n_actions = env.action_space.n\n",
        "n_observations = env.observation_space.shape[0]\n",
        "\n",
        "\n",
        "#### Initilize DQN/DDQN Networks and optimizer\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "\n",
        "#### Initilize Dueling Networks and optimizer\n",
        "policy_net_duel = DuelingDQN(n_observations, n_actions).to(device)\n",
        "target_net_duel = DuelingDQN(n_observations, n_actions).to(device)\n",
        "target_net_duel.load_state_dict(policy_net_duel.state_dict())\n",
        "## Only update the parameter for the policy network\n",
        "optimizer_duel = optim.AdamW(policy_net_duel.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "#### Initizalize Experience Replay Buffer\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "#### Initizalize Priority Experience Replay Buffer\n",
        "pmemory = PrioritizedReplayMemory(10000)\n",
        "\n",
        "# ======================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================================================================\n",
        "#  Define Main Algorithms\n",
        "#  - Finish the update of optimize_model_DQN() amd optimize_model_DDQN()\n",
        "#  - Finish the update of optimize_model_DN() for bonus\n",
        "# ===============================================================================================================\n",
        "\n",
        "#### Implementation of vanilla DQN\n",
        "def optimize_model_DQN():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    states, actions, rewards, next_states, terminateds = zip(*memory.sample(BATCH_SIZE))\n",
        "\n",
        "    # Convert to tensors\n",
        "    state_batch = torch.tensor(np.array(states), device=device, dtype=torch.float)\n",
        "    action_batch = torch.tensor(actions, device=device).unsqueeze(-1) # Actions are used as indices\n",
        "    reward_batch = torch.tensor(rewards, device=device, dtype=torch.float)\n",
        "    next_state_batch = torch.tensor(np.array(next_states), device=device, dtype=torch.float)\n",
        "    terminated_batch = torch.tensor(terminateds, device=device, dtype=torch.float)\n",
        "\n",
        "    # Compute Q(s, a)\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute max_a Q(s', a) for all next states.\n",
        "    next_state_values = target_net(next_state_batch).max(1)[0].detach()\n",
        "\n",
        "    # If next state is terminal, then there is no next Q value\n",
        "    next_state_values = next_state_values * (1 - terminated_batch)\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)  # Clipping gradients to avoid explosion\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "#### Implementation of Double DQN\n",
        "def optimize_model_DDQN():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    states, actions, rewards, next_states, terminateds = zip(*memory.sample(BATCH_SIZE))\n",
        "\n",
        "    # Convert to tensors\n",
        "    state_batch = torch.tensor(np.array(states), device=device, dtype=torch.float)\n",
        "    action_batch = torch.tensor(actions, device=device).unsqueeze(-1)  # Actions are used as indices\n",
        "    reward_batch = torch.tensor(rewards, device=device, dtype=torch.float)\n",
        "    next_state_batch = torch.tensor(np.array(next_states), device=device, dtype=torch.float)\n",
        "    terminated_batch = torch.tensor(terminateds, device=device, dtype=torch.float)\n",
        "\n",
        "    # Compute Q(s, a) using the policy network\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Select the best action in the next state using the policy network (action selection)\n",
        "    next_state_actions = policy_net(next_state_batch).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "    # Evaluate the selected action's Q value using the target network (action evaluation)\n",
        "    next_state_values = target_net(next_state_batch).gather(1, next_state_actions).squeeze(-1)\n",
        "\n",
        "    # If next state is terminal, then there is no next Q value\n",
        "    next_state_values = next_state_values * (1 - terminated_batch)\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "#### Implementation of Double DQN + Dueling Network\n",
        "def optimize_model_DN():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    # Extract transitions from the replay memory\n",
        "    states, actions, rewards, next_states, terminateds = zip(*memory.sample(BATCH_SIZE))\n",
        "\n",
        "    # Convert arrays into tensors\n",
        "    state_batch = torch.tensor(np.array(states), device=device, dtype=torch.float)\n",
        "    action_batch = torch.tensor(actions, device=device).unsqueeze(-1)\n",
        "    reward_batch = torch.tensor(rewards, device=device, dtype=torch.float)\n",
        "    next_state_batch = torch.tensor(np.array(next_states), device=device, dtype=torch.float)\n",
        "    terminated_batch = torch.tensor(terminateds, device=device, dtype=torch.float)\n",
        "\n",
        "    # Compute Q(s, a) using the policy network\n",
        "    state_action_values = policy_net_duel(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Determine the best action for each next state using the policy network\n",
        "    next_state_actions = policy_net_duel(next_state_batch).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "    # Evaluate the selected actions' Q values using the target network\n",
        "    next_state_values = target_net_duel(next_state_batch).gather(1, next_state_actions).squeeze(-1)\n",
        "\n",
        "    # Apply terminal mask to ensure no next state value is considered for terminal states\n",
        "    next_state_values = next_state_values * (1 - terminated_batch)\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer_duel.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_duel.step()\n",
        "# ==============================================================================================================================\n",
        "\n",
        "#### Implementation of Double DQN + Dueling Network + PriorityRelay\n",
        "def optimize_model_DNP():\n",
        "    if len(pmemory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    # Sample transitions from the prioritized experience replay memory\n",
        "    batch, batch_indices, weights = pmemory.sample(BATCH_SIZE)\n",
        "\n",
        "    # Extract transitions into separate arrays\n",
        "    states, actions, rewards, next_states, terminateds = zip(*batch)\n",
        "\n",
        "    # Convert arrays into tensors\n",
        "    state_batch = torch.tensor(np.array(states), device=device, dtype=torch.float)\n",
        "    action_batch = torch.tensor(actions, device=device).unsqueeze(-1)\n",
        "    reward_batch = torch.tensor(rewards, device=device, dtype=torch.float)\n",
        "    next_state_batch = torch.tensor(np.array(next_states), device=device, dtype=torch.float)\n",
        "    terminated_batch = torch.tensor(terminateds, device=device, dtype=torch.float)\n",
        "\n",
        "    # Compute Q(s, a) using the policy network\n",
        "    state_action_values = policy_net_duel(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Determine the best action for each next state using the policy network\n",
        "    next_state_actions = policy_net_duel(next_state_batch).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "    # Evaluate the selected actions' Q values using the target network\n",
        "    next_state_values = target_net_duel(next_state_batch).gather(1, next_state_actions).squeeze(-1)\n",
        "\n",
        "    # Apply terminal mask to ensure no next state value is considered for terminal states\n",
        "    next_state_values = next_state_values * (1 - terminated_batch)\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
        "\n",
        "    # Compute TD errors for prioritized experience replay\n",
        "    td_errors = (state_action_values - expected_state_action_values.unsqueeze(1)).abs().squeeze(-1)\n",
        "\n",
        "    # Update priorities in the prioritized experience replay memory\n",
        "    pmemory.update_priorities(batch_indices, td_errors.detach().cpu().numpy())\n",
        "\n",
        "    # Compute loss weighted by importance sampling weights\n",
        "    weighted_loss = (weights * F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1), reduction='none')).mean()\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer_duel.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    optimizer_duel.step()\n",
        "# ==============================================================================================================================\n",
        "\n",
        "\n",
        "# ===============================================================================================================\n",
        "#  Main Train Loop\n",
        "#  - Finish the epsilon greedy exploration\n",
        "# ===============================================================================================================\n",
        "\n",
        "#### Training Episodes\n",
        "NUM_EPISODES = 2000\n",
        "\n",
        "#### Training Loop. If the input algorithm == \"DQN\", it will utilize DQN to train.\n",
        "#### Similarly, if the input algorithm == \"DDQN\", it will utilize DDQN to train. If the input algorithm == \"DN\", it will utilize Dueling Networks to train\n",
        "def train_models(algorithm):\n",
        "    episode_returns = []  # To store returns from each episode\n",
        "\n",
        "    for iteration in range(NUM_EPISODES):\n",
        "        current_episode_return = 0  # Reset the return for the current episode\n",
        "        state, _ = env.reset()  # Reset the environment\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        while not (terminated or truncated):\n",
        "            # Epsilon-Greedy Exploration\n",
        "            eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * iteration / EPS_DECAY)\n",
        "            if random.random() > eps:\n",
        "                # Convert state to tensor\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    if algorithm in [\"DQN\", \"DDQN\"]:\n",
        "                        action_values = policy_net(state_tensor)\n",
        "                    elif algorithm == \"DN\":\n",
        "                        action_values = policy_net_duel(state_tensor)\n",
        "                    action = action_values.max(1)[1].item()  # Choose the action with the highest Q-value\n",
        "            else:\n",
        "                action = random.randrange(n_actions)  # Choose a random action\n",
        "\n",
        "            # Take action in the environment\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            if not algorithm == 'DNP':\n",
        "                memory.push(state, action, reward, next_state, terminated)  # Save the transition in memory\n",
        "            else:\n",
        "                pmemory.push(state, action, reward, next_state, terminated)\n",
        "\n",
        "            current_episode_return += reward\n",
        "\n",
        "            # Update the target network parameters\n",
        "            if algorithm in [\"DQN\", \"DDQN\"]:\n",
        "                for target_par0am, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
        "                    target_param.data.copy_(TAU * policy_param.data + (1.0 - TAU) * target_param.data)\n",
        "            elif algorithm == \"DN\":\n",
        "                for target_param, policy_param in zip(target_net_duel.parameters(), policy_net_duel.parameters()):\n",
        "                    target_param.data.copy_(TAU * policy_param.data + (1.0 - TAU) * target_param.data)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                if iteration % 20 == 0:\n",
        "                    print('Episode {},  score: {}'.format(iteration, current_episode_return))\n",
        "                episode_returns.append(current_episode_return)\n",
        "            else:\n",
        "                state = next_state  # Update the state\n",
        "\n",
        "            # Optimize the model according to the selected algorithm\n",
        "            if algorithm == \"DQN\":\n",
        "                optimize_model_DQN()\n",
        "            elif algorithm == \"DDQN\":\n",
        "                optimize_model_DDQN()\n",
        "            elif algorithm == \"DN\":\n",
        "                optimize_model_DN()\n",
        "            elif algorithm == \"DNP\":\n",
        "                optimize_model_DNP()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.title('Training with ' + algorithm)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    # plt.plot(episode_returns)\n",
        "\n",
        "    # Plot the full episode returns without markers\n",
        "    plt.plot(episode_returns, color='blue', alpha=0.5)\n",
        "\n",
        "    # Create a subset of episode_returns with only the points after every 20 episodes\n",
        "    marker_indices = [i for i in range(len(episode_returns)) if i % 20 == 0]\n",
        "    marker_returns = [episode_returns[i] for i in marker_indices]\n",
        "\n",
        "    # Plot the subset of episode returns with circular markers\n",
        "    plt.plot(marker_indices, marker_returns, 'o', markersize=5, color='orange')\n",
        "\n",
        "    plt.savefig(\"Training with \" + algorithm)\n",
        "    plt.show()\n",
        "\n",
        "# ===============================================================================================================\n",
        "#  Functions to generate videos for visulization\n",
        "#  (This part is only for playing. You do not need to upload any video for homework)\n",
        "#  To play with your own policy, you can save your policy network and put it into generate_videos(policy) as policy\n",
        "#  You also need to complete a line in generate_videos(policy) to play.\n",
        "#  This line is the same as the line you finishes in train_models(algorithm), choosing the best action based on your model.\n",
        "# ===============================================================================================================\n",
        "def generate_videos(policy):\n",
        "    env = gym.make('LunarLander-v2', render_mode = 'rgb_array')\n",
        "    n_actions = env.action_space.n\n",
        "    n_observations = env.observation_space.shape[0]\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    img = env.render()\n",
        "    frames.append(img)\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "        if policy == \"random\":\n",
        "            action = np.random.randint(0, n_actions)\n",
        "        else:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                # Example for using the DuelingDQN policy network\n",
        "                action_values = policy_net_duel(state_tensor)\n",
        "            action = action_values.max(1)[1].item()\n",
        "        next_state, _, terminated, truncated, _ = env.step(action)\n",
        "        img = env.render()\n",
        "        frames.append(img)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "        state = next_state\n",
        "    imageio.mimsave(f'lunar_lander_{policy}.mp4', frames, fps=60)\n",
        "# ===============================================================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_models(\"DNP\")\n",
        "generate_videos(\"DNP\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHnaPKAaF92Q",
        "outputId": "464ae3c1-d1f9-46de-c87e-c667b4283c30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4Qvm-p5GAXS",
        "outputId": "d7272a51-c486-41ea-f2b6-dcb212b1d08a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.* (from gym[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q swig\n",
        "!pip install -q gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgwYIe62GQUI",
        "outputId": "044181b1-c222-4472-a3b4-679bff9898cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "04v-tUMoGblA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}