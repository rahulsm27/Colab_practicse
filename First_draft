{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6649,"databundleVersionId":860670,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-13T12:09:54.844437Z","iopub.execute_input":"2024-04-13T12:09:54.845575Z","iopub.status.idle":"2024-04-13T12:09:54.885567Z","shell.execute_reply.started":"2024-04-13T12:09:54.845535Z","shell.execute_reply":"2024-04-13T12:09:54.884120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## STEP1 : Import Libraries and Load Dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\n\n\nfrom tqdm import tqdm\nimport gc\n\n\nfrom catboost import CatBoostRegressor\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:09:54.887577Z","iopub.execute_input":"2024-04-13T12:09:54.888328Z","iopub.status.idle":"2024-04-13T12:09:58.494603Z","shell.execute_reply.started":"2024-04-13T12:09:54.888294Z","shell.execute_reply":"2024-04-13T12:09:58.493151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the properties dataset\n# This file constains properties with their home features for their respective year\n# This cell takes time to execute\n\nprint(\"Loading Properties dataset..\")\nproperties2016 = pd.read_csv('/kaggle/input/zillow-prize-1/properties_2016.csv', low_memory = False)\nproperties2017 = pd.read_csv('/kaggle/input/zillow-prize-1/properties_2017.csv', low_memory = False)\nprint(\" Properties dataset loaded\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:09:58.496677Z","iopub.execute_input":"2024-04-13T12:09:58.497194Z","iopub.status.idle":"2024-04-13T12:11:14.486337Z","shell.execute_reply.started":"2024-04-13T12:09:58.497161Z","shell.execute_reply":"2024-04-13T12:11:14.484819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The file contain transaction details i.e. the date of sale and sale value\n#train_2016.csv - the training set with transactions from 1/1/2016 to 12/31/2016\n#train_2017.csv - the training set with transactions from 1/1/2017 to 9/15/2017\n\nprint('Loading transaction dataset..')\ntrain2016 = pd.read_csv('/kaggle/input/zillow-prize-1/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\ntrain2017 = pd.read_csv('/kaggle/input/zillow-prize-1/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\nprint('Transaction dataset loaded')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:11:14.488420Z","iopub.execute_input":"2024-04-13T12:11:14.488836Z","iopub.status.idle":"2024-04-13T12:11:14.729784Z","shell.execute_reply.started":"2024-04-13T12:11:14.488803Z","shell.execute_reply":"2024-04-13T12:11:14.728486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The processing of some of the algorithms can be made quick if data representation is made in int/float32 instead of int/float64.\nfor c, dtype in zip(properties2016.columns, properties2016.dtypes):\n    if dtype == np.float64:        \n        properties2016[c] = properties2016[c].astype(np.float32)\n    if dtype == np.int64:\n        properties2016[c] = properties2016[c].astype(np.int32)\n        \nfor c, dtype in zip(properties2017.columns, properties2017.dtypes):\n    if dtype == np.float64:        \n        properties2017[c] = properties2017[c].astype(np.float32)\n    if dtype == np.int64:\n        properties2017[c] = properties2017[c].astype(np.int32)\n\n\nfor column in train2016.columns:\n    if train2016[column].dtype == int:\n        train2016[column] = train2016[column].astype(np.int32)\n    if train2016[column].dtype == float:\n        train2016[column] = train2016[column].astype(np.float32)\n        \nfor column in train2017.columns:\n    if train2017[column].dtype == int:\n        train2017[column] = train2017[column].astype(np.int32)\n    if train2017[column].dtype == float:\n        train2017[column] = train2017[column].astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:12:56.943509Z","iopub.execute_input":"2024-04-13T12:12:56.943996Z","iopub.status.idle":"2024-04-13T12:12:57.576311Z","shell.execute_reply.started":"2024-04-13T12:12:56.943962Z","shell.execute_reply":"2024-04-13T12:12:57.575318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this competition, Zillow is asking us to predict the \nlog-error between their Zestimate and the actual sale price, \ngiven all the features of a home. The log error is defined as\n\n𝑙𝑜𝑔𝑒𝑟𝑟𝑜𝑟=𝑙𝑜𝑔(𝑍𝑒𝑠𝑡𝑖𝑚𝑎𝑡𝑒)−𝑙𝑜𝑔(𝑆𝑎𝑙𝑒𝑃𝑟𝑖𝑐𝑒)","metadata":{}},{"cell_type":"code","source":"\n# We need to predict 6 time points for all properties: October 2016 (201610), \n#November 2016 (201611), December 2016 (201612), October 2017 (201710),\n#November 2017 (201711), and December 2017 (201712).\n\nprint('Loading submission sample file')\nsample_submission = pd.read_csv('/kaggle/input/zillow-prize-1/sample_submission.csv', low_memory = False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:12:57.883135Z","iopub.execute_input":"2024-04-13T12:12:57.884332Z","iopub.status.idle":"2024-04-13T12:13:00.486596Z","shell.execute_reply.started":"2024-04-13T12:12:57.884284Z","shell.execute_reply":"2024-04-13T12:13:00.485479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:00.489157Z","iopub.execute_input":"2024-04-13T12:13:00.489522Z","iopub.status.idle":"2024-04-13T12:13:00.510273Z","shell.execute_reply.started":"2024-04-13T12:13:00.489494Z","shell.execute_reply":"2024-04-13T12:13:00.509069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sample_submission)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:00.512093Z","iopub.execute_input":"2024-04-13T12:13:00.512446Z","iopub.status.idle":"2024-04-13T12:13:00.519206Z","shell.execute_reply.started":"2024-04-13T12:13:00.512417Z","shell.execute_reply":"2024-04-13T12:13:00.518127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## STEP 2 : EDA &  Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Lets check the basic attributes of all the four datasets like \n#shape, number of null values and few records to get a sense of the data","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:00.521505Z","iopub.execute_input":"2024-04-13T12:13:00.522454Z","iopub.status.idle":"2024-04-13T12:13:00.531027Z","shell.execute_reply.started":"2024-04-13T12:13:00.522419Z","shell.execute_reply":"2024-04-13T12:13:00.529598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nproperties2016.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:00.532768Z","iopub.execute_input":"2024-04-13T12:13:00.533252Z","iopub.status.idle":"2024-04-13T12:13:00.624966Z","shell.execute_reply.started":"2024-04-13T12:13:00.533218Z","shell.execute_reply":"2024-04-13T12:13:00.622919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nproperties2016.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:00.659347Z","iopub.execute_input":"2024-04-13T12:13:00.660348Z","iopub.status.idle":"2024-04-13T12:13:00.672771Z","shell.execute_reply.started":"2024-04-13T12:13:00.660276Z","shell.execute_reply":"2024-04-13T12:13:00.670934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"properties2016.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:01.163466Z","iopub.execute_input":"2024-04-13T12:13:01.163923Z","iopub.status.idle":"2024-04-13T12:13:07.257625Z","shell.execute_reply.started":"2024-04-13T12:13:01.163881Z","shell.execute_reply":"2024-04-13T12:13:07.255614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(properties2016.isnull().sum()/len(properties2016)*100).sort_values()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:07.260748Z","iopub.execute_input":"2024-04-13T12:13:07.261255Z","iopub.status.idle":"2024-04-13T12:13:08.471608Z","shell.execute_reply.started":"2024-04-13T12:13:07.261219Z","shell.execute_reply":"2024-04-13T12:13:08.469885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LEts check the size of the dataset\nproperties2017.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:08.473027Z","iopub.execute_input":"2024-04-13T12:13:08.473507Z","iopub.status.idle":"2024-04-13T12:13:08.484170Z","shell.execute_reply.started":"2024-04-13T12:13:08.473472Z","shell.execute_reply":"2024-04-13T12:13:08.482495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"properties2017.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:08.487270Z","iopub.execute_input":"2024-04-13T12:13:08.487753Z","iopub.status.idle":"2024-04-13T12:13:14.719498Z","shell.execute_reply.started":"2024-04-13T12:13:08.487718Z","shell.execute_reply":"2024-04-13T12:13:14.717729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking percentage of null values\n(properties2017.isnull().sum()/len(properties2017) * 100).sort_values()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:14.721591Z","iopub.execute_input":"2024-04-13T12:13:14.723173Z","iopub.status.idle":"2024-04-13T12:13:15.950021Z","shell.execute_reply.started":"2024-04-13T12:13:14.723112Z","shell.execute_reply":"2024-04-13T12:13:15.947731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"properties2017[properties2017['longitude'].isnull()].head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:15.953794Z","iopub.execute_input":"2024-04-13T12:13:15.954446Z","iopub.status.idle":"2024-04-13T12:13:16.022395Z","shell.execute_reply.started":"2024-04-13T12:13:15.954397Z","shell.execute_reply":"2024-04-13T12:13:16.020496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2016.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:16.024015Z","iopub.execute_input":"2024-04-13T12:13:16.024438Z","iopub.status.idle":"2024-04-13T12:13:16.040589Z","shell.execute_reply.started":"2024-04-13T12:13:16.024403Z","shell.execute_reply":"2024-04-13T12:13:16.038591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train2016)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:16.043313Z","iopub.execute_input":"2024-04-13T12:13:16.043938Z","iopub.status.idle":"2024-04-13T12:13:16.053858Z","shell.execute_reply.started":"2024-04-13T12:13:16.043890Z","shell.execute_reply":"2024-04-13T12:13:16.052257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train2017.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:16.056326Z","iopub.execute_input":"2024-04-13T12:13:16.056969Z","iopub.status.idle":"2024-04-13T12:13:16.077161Z","shell.execute_reply.started":"2024-04-13T12:13:16.056919Z","shell.execute_reply":"2024-04-13T12:13:16.075054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train2017)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:16.084129Z","iopub.execute_input":"2024-04-13T12:13:16.085297Z","iopub.status.idle":"2024-04-13T12:13:16.095948Z","shell.execute_reply.started":"2024-04-13T12:13:16.085249Z","shell.execute_reply":"2024-04-13T12:13:16.094113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us add date features as it is important for analysis. \ndef add_date_features(df):\n    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n    df[\"transaction_quarter\"] = (df[\"transactiondate\"].dt.year - 2016)*4 +df[\"transactiondate\"].dt.quarter\n    df.drop([\"transactiondate\"], inplace=True, axis=1)\n    return df\n\ntrain2016 = add_date_features(train2016)\ntrain2017 = add_date_features(train2017)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:16.097897Z","iopub.execute_input":"2024-04-13T12:13:16.099067Z","iopub.status.idle":"2024-04-13T12:13:16.160847Z","shell.execute_reply.started":"2024-04-13T12:13:16.099014Z","shell.execute_reply":"2024-04-13T12:13:16.159305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets merge and prepare final dataset. We will perform any feature engineering on this","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:16.164945Z","iopub.execute_input":"2024-04-13T12:13:16.165366Z","iopub.status.idle":"2024-04-13T12:13:16.171506Z","shell.execute_reply.started":"2024-04-13T12:13:16.165334Z","shell.execute_reply":"2024-04-13T12:13:16.169835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Preparing final dataset')\ntrain2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\ntrain2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n\ntrain_df = pd.concat([train2016, train2017], axis = 0)\ntest_df = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')\n\n#del properties2016, properties2017, train2016, train2017\n#gc.collect();","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:16.173366Z","iopub.execute_input":"2024-04-13T12:13:16.173937Z","iopub.status.idle":"2024-04-13T12:13:21.113047Z","shell.execute_reply.started":"2024-04-13T12:13:16.173894Z","shell.execute_reply":"2024-04-13T12:13:21.111855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport ydata_profiling as yp \nprofile = yp.ProfileReport(train_df) ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:21.114332Z","iopub.execute_input":"2024-04-13T12:13:21.114677Z","iopub.status.idle":"2024-04-13T12:13:25.794173Z","shell.execute_reply.started":"2024-04-13T12:13:21.114648Z","shell.execute_reply":"2024-04-13T12:13:25.792932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## takes 10-12 minutes\n#profile.to_notebook_iframe() \n## Save to html\n#profile.to_file(\"report\")","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:25.795571Z","iopub.execute_input":"2024-04-13T12:13:25.796199Z","iopub.status.idle":"2024-04-13T12:13:25.800405Z","shell.execute_reply.started":"2024-04-13T12:13:25.796164Z","shell.execute_reply":"2024-04-13T12:13:25.799579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are lot of null values let us treate each of them\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:25.801565Z","iopub.execute_input":"2024-04-13T12:13:25.802226Z","iopub.status.idle":"2024-04-13T12:13:25.830792Z","shell.execute_reply.started":"2024-04-13T12:13:25.802193Z","shell.execute_reply":"2024-04-13T12:13:25.829710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#checking percentage of null values in final dataset\n(train_df.isnull().sum() / len(train_df) * 100).sort_values()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:25.834065Z","iopub.execute_input":"2024-04-13T12:13:25.834455Z","iopub.status.idle":"2024-04-13T12:13:25.912304Z","shell.execute_reply.started":"2024-04-13T12:13:25.834424Z","shell.execute_reply":"2024-04-13T12:13:25.911190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:25.913742Z","iopub.execute_input":"2024-04-13T12:13:25.914076Z","iopub.status.idle":"2024-04-13T12:13:25.920073Z","shell.execute_reply.started":"2024-04-13T12:13:25.914049Z","shell.execute_reply":"2024-04-13T12:13:25.918874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 'airconditioningtypeid'\nprint(f\"Column : {i}\")\nprint(\"*\"*50)\n\nprint(\"Value counts before missing imputation\")\nprint(train_df[i].value_counts())\n\n\ntrain_df[i].fillna(6.0,inplace=True) # filling with 'other' as per data dictionary 6 indicates other  \ntest_df[i].fillna(6.0,inplace=True)\n\nprint(\"*\"*50)\nprint(\"Value counts after missing imputation\")\nprint(train_df[i].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:25.921857Z","iopub.execute_input":"2024-04-13T12:13:25.922209Z","iopub.status.idle":"2024-04-13T12:13:25.955350Z","shell.execute_reply.started":"2024-04-13T12:13:25.922175Z","shell.execute_reply":"2024-04-13T12:13:25.954592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df['poolcnt'] == 1][['poolcnt','pooltypeid7','poolsizesum','pooltypeid10']].head(5)\n\n# we will keep only poolcnt column.Drop other columns related to pool. We will fill 0 for poolcnt columns wheren it is nan assuming no pools\ntrain_df['poolcnt'].fillna(0,inplace = True)\ntest_df['poolcnt'].fillna(0,inplace =True)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:25.956605Z","iopub.execute_input":"2024-04-13T12:13:25.957102Z","iopub.status.idle":"2024-04-13T12:13:25.988576Z","shell.execute_reply.started":"2024-04-13T12:13:25.957073Z","shell.execute_reply":"2024-04-13T12:13:25.987411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['taxdelinquencyflag'].value_counts())\nprint(train_df['taxdelinquencyyear'].value_counts().sum())\n\n# We observer that no of records in both the columns is same. This means the flag is 'Y' only when there is an overdue of tax. We can fill other rows with N for delinquency column and 0 for year column\n\ntrain_df['taxdelinquencyflag'].fillna('N',inplace=True)\ntest_df['taxdelinquencyflag'].fillna('N',inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:25.990331Z","iopub.execute_input":"2024-04-13T12:13:25.991437Z","iopub.status.idle":"2024-04-13T12:13:26.201678Z","shell.execute_reply.started":"2024-04-13T12:13:25.991381Z","shell.execute_reply":"2024-04-13T12:13:26.200656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no of missing values is same in garagecarcnt and sqft column. SO we will fill o in missing values assuming no garage\ntrain_df['garagecarcnt'].value_counts()\ntrain_df['garagecarcnt'].fillna(0,inplace = True)\ntrain_df['garagecarcnt'].fillna(0,inplace = True)\n\ntrain_df['garagetotalsqft'].fillna(0,inplace = True)\ntrain_df['garagetotalsqft'].fillna(0,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:26.203404Z","iopub.execute_input":"2024-04-13T12:13:26.204693Z","iopub.status.idle":"2024-04-13T12:13:26.219597Z","shell.execute_reply.started":"2024-04-13T12:13:26.204651Z","shell.execute_reply":"2024-04-13T12:13:26.218289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df['longitude'].isnull()].head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:26.221637Z","iopub.execute_input":"2024-04-13T12:13:26.222588Z","iopub.status.idle":"2024-04-13T12:13:26.286527Z","shell.execute_reply.started":"2024-04-13T12:13:26.222543Z","shell.execute_reply":"2024-04-13T12:13:26.284907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ni = 'fireplaceflag'\nprint(f\"Column : {i}\")\nprint(\"*\"*50)\n\nprint(\"Value counts before missing imputation\")\nprint(train_df[i].value_counts())\n\n# it means we only have data for true values. Data for false is missing. Therefore we can replace nan with false\n\ntrain_df[i].fillna(False,inplace=True) \ntest_df[i].fillna(False,inplace=True)\n\nprint(\"*\"*50)\nprint(\"Value counts after missing imputation\")\nprint(train_df[i].value_counts())","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:26.288775Z","iopub.execute_input":"2024-04-13T12:13:26.289382Z","iopub.status.idle":"2024-04-13T12:13:26.722462Z","shell.execute_reply.started":"2024-04-13T12:13:26.289333Z","shell.execute_reply":"2024-04-13T12:13:26.721298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping rows having all NA values except parcelid\ntrain_df.dropna(subset=[\"longitude\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:26.724263Z","iopub.execute_input":"2024-04-13T12:13:26.725010Z","iopub.status.idle":"2024-04-13T12:13:26.765353Z","shell.execute_reply.started":"2024-04-13T12:13:26.724968Z","shell.execute_reply":"2024-04-13T12:13:26.764363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_columns = list(train_df.columns[train_df.isnull().any()])\nprint(\"Total columns having null value \", len(null_columns))\nprint()\nprint(null_columns)\n\n\n# Drop columns having greter than 60 % missing values\nmissing_perc_thresh = 0.60\nexclude_missing = []\nnum_rows = train_df.shape[0]\nfor c in train_df.columns:\n    num_missing = train_df[c].isnull().sum()\n    if num_missing == 0:\n        continue\n    missing_frac = num_missing / float(num_rows)\n    if missing_frac > missing_perc_thresh:\n        exclude_missing.append(c)\nprint()\nprint(\"We exclude: %s\" % len(exclude_missing))\nprint(exclude_missing)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:26.766991Z","iopub.execute_input":"2024-04-13T12:13:26.767703Z","iopub.status.idle":"2024-04-13T12:13:26.924876Z","shell.execute_reply.started":"2024-04-13T12:13:26.767662Z","shell.execute_reply":"2024-04-13T12:13:26.923685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop(['propertycountylandusecode'],inplace= True, axis=1)\ntest_df.drop(['propertycountylandusecode'],inplace= True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:26.926591Z","iopub.execute_input":"2024-04-13T12:13:26.927307Z","iopub.status.idle":"2024-04-13T12:13:27.346587Z","shell.execute_reply.started":"2024-04-13T12:13:26.927266Z","shell.execute_reply":"2024-04-13T12:13:27.345303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Define training features !!\")\nexclude_other = ['parcelid', 'logerror','propertyzoningdesc']\ntrain_features = []\nfor c in train_df.columns:\n    if c not in exclude_missing and c not in exclude_other :\n        train_features.append(c)\nprint(\"We use these for training: %s\" % len(train_features))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.354412Z","iopub.execute_input":"2024-04-13T12:13:27.355116Z","iopub.status.idle":"2024-04-13T12:13:27.363163Z","shell.execute_reply.started":"2024-04-13T12:13:27.355083Z","shell.execute_reply":"2024-04-13T12:13:27.362343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking columsn that we plan to drop","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.364366Z","iopub.execute_input":"2024-04-13T12:13:27.365340Z","iopub.status.idle":"2024-04-13T12:13:27.373239Z","shell.execute_reply.started":"2024-04-13T12:13:27.365310Z","shell.execute_reply":"2024-04-13T12:13:27.372233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i='storytypeid'\nprint(train_df[i].value_counts())\n# We will drop this column as number of na is too high\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.374803Z","iopub.execute_input":"2024-04-13T12:13:27.375588Z","iopub.status.idle":"2024-04-13T12:13:27.387661Z","shell.execute_reply.started":"2024-04-13T12:13:27.375536Z","shell.execute_reply":"2024-04-13T12:13:27.386538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 'architecturalstyletypeid'\nprint(train_df[i].value_counts())\n# We will drop this colum as no of missing values is too high\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.389373Z","iopub.execute_input":"2024-04-13T12:13:27.389734Z","iopub.status.idle":"2024-04-13T12:13:27.400615Z","shell.execute_reply.started":"2024-04-13T12:13:27.389705Z","shell.execute_reply":"2024-04-13T12:13:27.399638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 'basementsqft'\n#print(train_df[i].value_counts())\n# We will drop this colum as no of missing values is too high\n# if i not in to_drop:\n#     to_drop.append(i)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.402598Z","iopub.execute_input":"2024-04-13T12:13:27.403961Z","iopub.status.idle":"2024-04-13T12:13:27.410553Z","shell.execute_reply.started":"2024-04-13T12:13:27.403918Z","shell.execute_reply":"2024-04-13T12:13:27.409724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 'buildingclasstypeid'\nprint(train_df[i].value_counts())\n# We will drop this colum as no of missing values is too high\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.412248Z","iopub.execute_input":"2024-04-13T12:13:27.412602Z","iopub.status.idle":"2024-04-13T12:13:27.425721Z","shell.execute_reply.started":"2024-04-13T12:13:27.412573Z","shell.execute_reply":"2024-04-13T12:13:27.424490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will drop taxlinquencyyear as we already have the flag column\n\ni ='taxdelinquencyyear'\nprint(train_df[i].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.427684Z","iopub.execute_input":"2024-04-13T12:13:27.428468Z","iopub.status.idle":"2024-04-13T12:13:27.440056Z","shell.execute_reply.started":"2024-04-13T12:13:27.428418Z","shell.execute_reply":"2024-04-13T12:13:27.438911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Overall assessment of condition of the building from best (lowest) to worst (highest)\ni = 'buildingqualitytypeid'\nprint(train_df[i].value_counts())\n# We will drop this colum as no of missing values is too high\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.441604Z","iopub.execute_input":"2024-04-13T12:13:27.442282Z","iopub.status.idle":"2024-04-13T12:13:27.454762Z","shell.execute_reply.started":"2024-04-13T12:13:27.442244Z","shell.execute_reply":"2024-04-13T12:13:27.453495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 'decktypeid'\nprint(train_df[i].value_counts())\n# We will drop this colum as no of missing values is too high\n# if i not in to_drop:\n#     to_drop.append(i)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.456595Z","iopub.execute_input":"2024-04-13T12:13:27.457313Z","iopub.status.idle":"2024-04-13T12:13:27.467462Z","shell.execute_reply.started":"2024-04-13T12:13:27.457274Z","shell.execute_reply":"2024-04-13T12:13:27.466351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 'storytypeid'\nprint(train_df[i].value_counts())\n# We will drop this colum as no of missing values is too high\n# if i not in to_drop:\n#     to_drop.append(i)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.468969Z","iopub.execute_input":"2024-04-13T12:13:27.469657Z","iopub.status.idle":"2024-04-13T12:13:27.479489Z","shell.execute_reply.started":"2024-04-13T12:13:27.469618Z","shell.execute_reply":"2024-04-13T12:13:27.478335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will drop all pool related column as we have pool cnt colun\n# We will drop all other area related column","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.480905Z","iopub.execute_input":"2024-04-13T12:13:27.481391Z","iopub.status.idle":"2024-04-13T12:13:27.486843Z","shell.execute_reply.started":"2024-04-13T12:13:27.481363Z","shell.execute_reply":"2024-04-13T12:13:27.485710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_df['propertycountylandusecode'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.488563Z","iopub.execute_input":"2024-04-13T12:13:27.489496Z","iopub.status.idle":"2024-04-13T12:13:27.498256Z","shell.execute_reply.started":"2024-04-13T12:13:27.489455Z","shell.execute_reply":"2024-04-13T12:13:27.497280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ulimit = np.percentile(train_df.logerror.values, 99) \nllimit = np.percentile(train_df.logerror.values, 1) \ntrain_df['logerror'].loc[train_df['logerror'] > ulimit] = ulimit\ntrain_df['logerror'].loc[train_df['logerror'] < llimit] = llimit\nsns.histplot(train2016['logerror'],bins=50)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:27.499499Z","iopub.execute_input":"2024-04-13T12:13:27.500510Z","iopub.status.idle":"2024-04-13T12:13:28.036635Z","shell.execute_reply.started":"2024-04-13T12:13:27.500467Z","shell.execute_reply":"2024-04-13T12:13:28.035406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12,12))\nsns.jointplot(x = train_df.latitude.values, y = train_df.longitude.values, size = 10)\nplt.ylabel('Longitude', fontsize = 12)\nplt.xlabel('Latitude', fontsize = 12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:28.038130Z","iopub.execute_input":"2024-04-13T12:13:28.038453Z","iopub.status.idle":"2024-04-13T12:13:34.250144Z","shell.execute_reply.started":"2024-04-13T12:13:28.038425Z","shell.execute_reply":"2024-04-13T12:13:34.249234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n#profile.to_file('eda_report.html')","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:34.251133Z","iopub.execute_input":"2024-04-13T12:13:34.251466Z","iopub.status.idle":"2024-04-13T12:13:34.256414Z","shell.execute_reply.started":"2024-04-13T12:13:34.251437Z","shell.execute_reply":"2024-04-13T12:13:34.255278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #life of property\n# df_train['N-life'] = 2018 - df_train['yearbuilt']\n\n# #error in calculation of the finished living area of home\n# df_train['N-LivingAreaError'] = df_train['calculatedfinishedsquarefeet']/df_train['finishedsquarefeet12']\n\n# #proportion of living area\n# df_train['N-LivingAreaProp'] = df_train['calculatedfinishedsquarefeet']/df_train['lotsizesquarefeet']\n# df_train['N-LivingAreaProp2'] = df_train['finishedsquarefeet12']/df_train['finishedsquarefeet15']\n\n# #Amout of extra space\n# df_train['N-ExtraSpace'] = df_train['lotsizesquarefeet'] - df_train['calculatedfinishedsquarefeet'] \n# df_train['N-ExtraSpace-2'] = df_train['finishedsquarefeet15'] - df_train['finishedsquarefeet12'] \n\n# #Total number of rooms\n# df_train['N-TotalRooms'] = df_train['bathroomcnt']*df_train['bedroomcnt']\n\n# #Average room size\n# df_train['N-AvRoomSize'] = df_train['calculatedfinishedsquarefeet']/df_train['roomcnt'] \n\n# # Number of Extra rooms\n# df_train['N-ExtraRooms'] = df_train['roomcnt'] - df_train['N-TotalRooms'] \n\n# #Ratio of the built structure value to land area\n# df_train['N-ValueProp'] = df_train['structuretaxvaluedollarcnt']/df_train['landtaxvaluedollarcnt']\n\n# #Does property have a garage, pool or hot tub and AC?\n# df_train['N-GarPoolAC'] = ((df_train['garagecarcnt']>0) & (df_train['pooltypeid10']>0) & (df_train['airconditioningtypeid']!=5))*1 \n\n# df_train[\"N-location\"] = df_train[\"latitude\"] + df_train[\"longitude\"]\n# df_train[\"N-location-2\"] = df_train[\"latitude\"]*df_train[\"longitude\"]\n# df_train[\"N-location-2round\"] = df_train[\"N-location-2\"].round(-4)\n\n# df_train[\"N-latitude-round\"] = df_train[\"latitude\"].round(-4)\n# df_train[\"N-longitude-round\"] = df_train[\"longitude\"].round(-4)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:34.257894Z","iopub.execute_input":"2024-04-13T12:13:34.258288Z","iopub.status.idle":"2024-04-13T12:13:34.268248Z","shell.execute_reply.started":"2024-04-13T12:13:34.258258Z","shell.execute_reply":"2024-04-13T12:13:34.267253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del num_rows, missing_perc_thresh\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:34.269482Z","iopub.execute_input":"2024-04-13T12:13:34.269913Z","iopub.status.idle":"2024-04-13T12:13:34.431610Z","shell.execute_reply.started":"2024-04-13T12:13:34.269881Z","shell.execute_reply":"2024-04-13T12:13:34.430276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## STEP 3 : Model training and Evaluation","metadata":{}},{"cell_type":"code","source":"print (\"Training time !!\")\nX_train = train_df[train_features]\ny_train = train_df.logerror\nprint(X_train.shape, y_train.shape)\n\ntest_df['transactiondate'] = pd.Timestamp('2016-12-01') \ntest_df = add_date_features(test_df)\nX_test = test_df[train_features]\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:34.433223Z","iopub.execute_input":"2024-04-13T12:13:34.433662Z","iopub.status.idle":"2024-04-13T12:13:35.543726Z","shell.execute_reply.started":"2024-04-13T12:13:34.433629Z","shell.execute_reply":"2024-04-13T12:13:35.542577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncat_columns = ['taxdelinquencyflag']\n\nfor i in cat_columns:\n    le = LabelEncoder()\n    X_train[i]= le.fit_transform(X_train[i])\n    X_test[i] = le.transform(X_test[i])\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:13:35.552199Z","iopub.execute_input":"2024-04-13T12:13:35.552531Z","iopub.status.idle":"2024-04-13T12:13:36.257639Z","shell.execute_reply.started":"2024-04-13T12:13:35.552503Z","shell.execute_reply":"2024-04-13T12:13:36.256455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:14:08.585971Z","iopub.execute_input":"2024-04-13T12:14:08.586670Z","iopub.status.idle":"2024-04-13T12:14:08.630816Z","shell.execute_reply.started":"2024-04-13T12:14:08.586636Z","shell.execute_reply":"2024-04-13T12:14:08.629319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_val, label=y_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:14:08.632189Z","iopub.execute_input":"2024-04-13T12:14:08.632532Z","iopub.status.idle":"2024-04-13T12:14:09.764107Z","shell.execute_reply.started":"2024-04-13T12:14:08.632502Z","shell.execute_reply":"2024-04-13T12:14:09.762912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    'objective':'reg:linear',\n    'verbose' : 0\n  \n}\n\nnum_boost_rounds=100\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(3,10)\n    for min_child_weight in range(2,8)\n]\n\n# Define initial best params and MAE\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    # Run CV\n    cv_results = xgb.cv(\n        dict(params,verbose=0),\n        dtrain,\n        num_boost_round=num_boost_rounds,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:58:38.586518Z","iopub.execute_input":"2024-04-13T12:58:38.586970Z","iopub.status.idle":"2024-04-13T13:01:19.472283Z","shell.execute_reply.started":"2024-04-13T12:58:38.586930Z","shell.execute_reply":"2024-04-13T13:01:19.471361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['max_depth'] = best_params[0]\nparams['min_child_weight'] = best_params[1]","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:06:41.692513Z","iopub.execute_input":"2024-04-13T13:06:41.693035Z","iopub.status.idle":"2024-04-13T13:06:41.699116Z","shell.execute_reply.started":"2024-04-13T13:06:41.693000Z","shell.execute_reply":"2024-04-13T13:06:41.697925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i/10. for i in range(7,11)]\n    for colsample in [i/10. for i in range(7,11)]\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_rounds,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=10\n    )\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (subsample,colsample)\nprint(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:07:11.416468Z","iopub.execute_input":"2024-04-13T13:07:11.416898Z","iopub.status.idle":"2024-04-13T13:08:21.240624Z","shell.execute_reply.started":"2024-04-13T13:07:11.416866Z","shell.execute_reply":"2024-04-13T13:08:21.239545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['subsample'] = best_params[0]\nparams['colsample_bytree'] =best_params[1]","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:08:21.245379Z","iopub.execute_input":"2024-04-13T13:08:21.248002Z","iopub.status.idle":"2024-04-13T13:08:21.254927Z","shell.execute_reply.started":"2024-04-13T13:08:21.247960Z","shell.execute_reply":"2024-04-13T13:08:21.253312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\n# This can take some time…\nmin_mae = float(\"Inf\")\nbest_params = None\nfor eta in [.5,.4,.3, .2, .1, .05,0.04,.03, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    params['eta'] = eta\n    # Run and time CV\n    cv_results = xgb.cv(params,\n            dtrain,\n            num_boost_round=num_boost_rounds,\n            seed=42,\n            nfold=5,\n            metrics=['mae'],\n            early_stopping_rounds=10)\n    # Update best score\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = eta\nprint(\"Best params: {}, MAE: {}\".format(best_params, min_mae))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:08:21.257094Z","iopub.execute_input":"2024-04-13T13:08:21.257943Z","iopub.status.idle":"2024-04-13T13:09:36.233624Z","shell.execute_reply.started":"2024-04-13T13:08:21.257908Z","shell.execute_reply":"2024-04-13T13:09:36.232707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['eta'] = best_params\nparams['eval_metric'] = 'mae'","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:14:19.711525Z","iopub.execute_input":"2024-04-13T13:14:19.712329Z","iopub.status.idle":"2024-04-13T13:14:19.717070Z","shell.execute_reply.started":"2024-04-13T13:14:19.712295Z","shell.execute_reply":"2024-04-13T13:14:19.715872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = xgb.train(dict(params, verbosity=0),\n                  dtrain,evals=[(dtest, \"Test\")],num_boost_round=500,early_stopping_rounds=30)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:14:26.031176Z","iopub.execute_input":"2024-04-13T13:14:26.031595Z","iopub.status.idle":"2024-04-13T13:14:28.389169Z","shell.execute_reply.started":"2024-04-13T13:14:26.031566Z","shell.execute_reply":"2024-04-13T13:14:28.388138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best MAE: {:.6f} in {} rounds\".format(model.best_score, model.best_iteration+1))","metadata":{"execution":{"iopub.status.busy":"2024-04-13T13:14:36.836272Z","iopub.execute_input":"2024-04-13T13:14:36.836666Z","iopub.status.idle":"2024-04-13T13:14:36.843341Z","shell.execute_reply.started":"2024-04-13T13:14:36.836639Z","shell.execute_reply":"2024-04-13T13:14:36.842052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction with best model\ndtest = xgb.DMatrix(X_test)\nxgb_y_pred = 0\nxgb_y_pred = model.predict(dtest)\n\nprint('\\nFirst XGBoost predictinos:')\nprint(pd.DataFrame(xgb_y_pred).head())","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:16:03.570924Z","iopub.execute_input":"2024-04-13T16:16:03.571360Z","iopub.status.idle":"2024-04-13T16:16:36.465069Z","shell.execute_reply.started":"2024-04-13T16:16:03.571330Z","shell.execute_reply":"2024-04-13T16:16:36.463795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:16:36.467225Z","iopub.execute_input":"2024-04-13T16:16:36.468487Z","iopub.status.idle":"2024-04-13T16:16:36.476330Z","shell.execute_reply.started":"2024-04-13T16:16:36.468441Z","shell.execute_reply":"2024-04-13T16:16:36.475111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:16:52.339616Z","iopub.execute_input":"2024-04-13T12:16:52.341947Z","iopub.status.idle":"2024-04-13T12:16:53.125450Z","shell.execute_reply.started":"2024-04-13T12:16:52.341902Z","shell.execute_reply":"2024-04-13T12:16:53.124274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_tree(model, num_trees=2)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:16:53.127054Z","iopub.execute_input":"2024-04-13T12:16:53.127768Z","iopub.status.idle":"2024-04-13T12:16:58.221567Z","shell.execute_reply.started":"2024-04-13T12:16:53.127726Z","shell.execute_reply":"2024-04-13T12:16:58.220336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.to_graphviz(model, num_trees=2)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:16:58.222928Z","iopub.execute_input":"2024-04-13T12:16:58.223280Z","iopub.status.idle":"2024-04-13T12:16:58.531221Z","shell.execute_reply.started":"2024-04-13T12:16:58.223252Z","shell.execute_reply":"2024-04-13T12:16:58.529957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingRegressor\ngbg =  HistGradientBoostingRegressor(loss='absolute_error')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T14:18:42.196443Z","iopub.execute_input":"2024-04-13T14:18:42.196943Z","iopub.status.idle":"2024-04-13T14:18:42.202941Z","shell.execute_reply.started":"2024-04-13T14:18:42.196906Z","shell.execute_reply":"2024-04-13T14:18:42.201437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid\nparam_grid = {\n    'max_leaf_nodes': [30,40,50],\n    'learning_rate': [0.1, 0.01],\n    'max_depth': [9,11,13],\n  \n}\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(gbg, param_grid, cv=5)\n\n# Fit the GridSearchCV object to the data\ngrid_search.fit(X_train, y_train)\n\n# Print the best parameters\nprint(\"Best parameters:\", grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:41:25.084568Z","iopub.execute_input":"2024-04-13T16:41:25.085088Z","iopub.status.idle":"2024-04-13T16:50:37.573721Z","shell.execute_reply.started":"2024-04-13T16:41:25.085051Z","shell.execute_reply":"2024-04-13T16:50:37.572374Z"},"trusted":true},"execution_count":177,"outputs":[{"name":"stdout","text":"Best parameters: {'learning_rate': 0.1, 'max_depth': 9, 'max_leaf_nodes': 50}\n","output_type":"stream"}]},{"cell_type":"code","source":"gbg =  HistGradientBoostingRegressor(loss='absolute_error',learning_rate = 0.1, \n                                     max_depth= 9, max_leaf_nodes=30 )\ngbg.fit(X_train, y_train)\nprint(gbg.score(X_train, y_train))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:00:40.606913Z","iopub.execute_input":"2024-04-13T17:00:40.607500Z","iopub.status.idle":"2024-04-13T17:00:49.333555Z","shell.execute_reply.started":"2024-04-13T17:00:40.607456Z","shell.execute_reply":"2024-04-13T17:00:49.330386Z"},"trusted":true},"execution_count":185,"outputs":[{"name":"stdout","text":"0.02942548528558453\n","output_type":"stream"}]},{"cell_type":"code","source":"gbg_y_pred = 0\ngbg_y_pred = rfg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:00:49.337479Z","iopub.execute_input":"2024-04-13T17:00:49.338035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbg_y_pred","metadata":{"execution":{"iopub.status.busy":"2024-04-13T17:00:23.831590Z","iopub.execute_input":"2024-04-13T17:00:23.832226Z","iopub.status.idle":"2024-04-13T17:00:23.844358Z","shell.execute_reply.started":"2024-04-13T17:00:23.832185Z","shell.execute_reply":"2024-04-13T17:00:23.842130Z"},"trusted":true},"execution_count":184,"outputs":[{"execution_count":184,"output_type":"execute_result","data":{"text/plain":"array([ 0.01254125, -0.00218453,  0.00803545, ..., -0.02524342,\n       -0.02524342, -0.02524342])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-04-13T14:29:43.034281Z","iopub.execute_input":"2024-04-13T14:29:43.035083Z","iopub.status.idle":"2024-04-13T14:29:43.041513Z","shell.execute_reply.started":"2024-04-13T14:29:43.035048Z","shell.execute_reply":"2024-04-13T14:29:43.040286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\ngbm = lgb.LGBMRegressor(**hyper_params)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:24:32.206850Z","iopub.execute_input":"2024-04-13T16:24:32.207340Z","iopub.status.idle":"2024-04-13T16:24:32.215143Z","shell.execute_reply.started":"2024-04-13T16:24:32.207308Z","shell.execute_reply":"2024-04-13T16:24:32.213638Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n}\n# Create a LightGBM dataset for training with features X_train and labels Y_train\ntrain_data = lgb.Dataset(X_train, label=y_train)\n \n# Create a LightGBM dataset for testing with features X_val and labels Y_val,\n# and specify the reference dataset as train_data for consistent evaluation\ntest_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n# Set the number of rounds and train the model with early stopping\nnum_round = 100\nbst = lgb.train(params, train_data, num_round, valid_sets=[\n                test_data])","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:24:34.322289Z","iopub.execute_input":"2024-04-13T16:24:34.322864Z","iopub.status.idle":"2024-04-13T16:24:38.236110Z","shell.execute_reply.started":"2024-04-13T16:24:34.322798Z","shell.execute_reply":"2024-04-13T16:24:38.234975Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.105289 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3828\n[LightGBM] [Info] Number of data points in the train set: 151068, number of used features: 36\n[LightGBM] [Info] Start training from score 0.012393\n","output_type":"stream"}]},{"cell_type":"code","source":"gbm_y_pred = 0\ngbm_y_pred = bst.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:25:58.613535Z","iopub.execute_input":"2024-04-13T16:25:58.614236Z","iopub.status.idle":"2024-04-13T16:26:05.662261Z","shell.execute_reply.started":"2024-04-13T16:25:58.614203Z","shell.execute_reply":"2024-04-13T16:26:05.661006Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## STEP 5 : Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'ParcelId': test_df['ParcelId'],\n})\ntest_dates = {\n    '201610': pd.Timestamp('2016-09-30'),\n    '201611': pd.Timestamp('2016-10-31'),\n    '201612': pd.Timestamp('2016-11-30'),\n    '201710': pd.Timestamp('2017-09-30'),\n    '201711': pd.Timestamp('2017-10-31'),\n    '201712': pd.Timestamp('2017-11-30')\n}\nl = [0.0,0.01,0.02,0.10,0.11,0.12]\ni=0\nfor label, test_date in test_dates.items():\n    print(\"Predicting for: %s ... \" % (label))\n    #submission[label] = (xgb_y_pred + gbg_y_pred + gbm_y_pred)/3 * (1+l[i])\n   # i = i+1\n    submission[label]=gbg_y_pred\n    \nsubmission.to_csv('submission_final9.csv', float_format='%.6f',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:54:39.304662Z","iopub.execute_input":"2024-04-13T16:54:39.305477Z","iopub.status.idle":"2024-04-13T16:55:52.313146Z","shell.execute_reply.started":"2024-04-13T16:54:39.305431Z","shell.execute_reply":"2024-04-13T16:55:52.311586Z"},"trusted":true},"execution_count":182,"outputs":[{"name":"stdout","text":"Predicting for: 201610 ... \nPredicting for: 201611 ... \nPredicting for: 201612 ... \nPredicting for: 201710 ... \nPredicting for: 201711 ... \nPredicting for: 201712 ... \n","output_type":"stream"}]},{"cell_type":"code","source":"#test_dates.items()\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:57:41.396382Z","iopub.execute_input":"2024-04-13T16:57:41.397277Z","iopub.status.idle":"2024-04-13T16:57:41.419854Z","shell.execute_reply.started":"2024-04-13T16:57:41.397207Z","shell.execute_reply":"2024-04-13T16:57:41.418301Z"},"trusted":true},"execution_count":183,"outputs":[{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"   ParcelId    201610    201611    201612    201710    201711    201712\n0  10754147  0.012541  0.012541  0.012541  0.012541  0.012541  0.012541\n1  10759547 -0.002185 -0.002185 -0.002185 -0.002185 -0.002185 -0.002185\n2  10843547  0.008035  0.008035  0.008035  0.008035  0.008035  0.008035\n3  10859147  0.023949  0.023949  0.023949  0.023949  0.023949  0.023949\n4  10879947 -0.003013 -0.003013 -0.003013 -0.003013 -0.003013 -0.003013","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ParcelId</th>\n      <th>201610</th>\n      <th>201611</th>\n      <th>201612</th>\n      <th>201710</th>\n      <th>201711</th>\n      <th>201712</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10754147</td>\n      <td>0.012541</td>\n      <td>0.012541</td>\n      <td>0.012541</td>\n      <td>0.012541</td>\n      <td>0.012541</td>\n      <td>0.012541</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10759547</td>\n      <td>-0.002185</td>\n      <td>-0.002185</td>\n      <td>-0.002185</td>\n      <td>-0.002185</td>\n      <td>-0.002185</td>\n      <td>-0.002185</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10843547</td>\n      <td>0.008035</td>\n      <td>0.008035</td>\n      <td>0.008035</td>\n      <td>0.008035</td>\n      <td>0.008035</td>\n      <td>0.008035</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10859147</td>\n      <td>0.023949</td>\n      <td>0.023949</td>\n      <td>0.023949</td>\n      <td>0.023949</td>\n      <td>0.023949</td>\n      <td>0.023949</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10879947</td>\n      <td>-0.003013</td>\n      <td>-0.003013</td>\n      <td>-0.003013</td>\n      <td>-0.003013</td>\n      <td>-0.003013</td>\n      <td>-0.003013</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission.to_csv('submission_final5.csv', float_format='%.6f',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T16:22:15.615039Z","iopub.execute_input":"2024-04-13T16:22:15.615463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import xgboost as xgb\n# ##### RUN XGBOOST\n\n# print(\"\\nSetting up data for XGBoost ...\")\n# # xgboost params\n# y_mean = np.mean(y_train)\n# xgb_params = [{\n#     'eta': 0.037,\n#     'max_depth': 5,\n#     'subsample': 0.80,\n#     'objective': 'reg:linear',\n#     'eval_metric': 'rmse',\n#     'lambda': 0.8,   \n#     'alpha': 0.4, \n#     'base_score': y_mean,\n#     'silent': 1\n# },{\n#     'eta': 0.037,\n#     'max_depth': 5,\n#     'subsample': 0.80,\n#     'objective': 'reg:squaredlogerror',\n#     'eval_metric': 'rmsle',\n#     'lambda': 0.8,   \n#     'alpha': 0.4, \n#     'base_score': y_mean,\n#     'silent': 1\n# },{\n#     'eta': 0.037,\n#     'max_depth': 5,\n#     'subsample': 0.80,\n#     'objective': 'reg:pseudohubererror',\n#     'eval_metric': 'mphe',\n#     'lambda': 0.8,   \n#     'alpha': 0.4, \n#     'base_score': y_mean,\n#     'silent': 1\n# }]\n\n\n# dtrain = xgb.DMatrix(X_train, y_train)\n# dtest = xgb.DMatrix(X_test)\n\n# num_boost_rounds = 100\n# print('num_boost_rounds = '+str(num_boost_rounds))\n\n# # train model\n# print('\\nTraining XGBoost')\n# y_pred = 0\n# for i in xgb_params:\n#     model = xgb.train(dict(i, verbosity=0),\n#                   dtrain, num_boost_round=num_boost_rounds)\n\n#     print('\\nPredicting with XGBoost....')\n#     y_pred += model.predict(dtest)\n# y_pred = y_pred/len(xgb_params)\n\n# print('\\nFirst XGBoost predictinos:')\n# print(pd.DataFrame(y_pred).head())","metadata":{},"execution_count":null,"outputs":[]}]}