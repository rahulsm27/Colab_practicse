{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNro13GrYhIcDlfBESZN18R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulsm27/Colab_practicse/blob/main/DQN%2BPER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install -q swig\n",
        "!pip install -q gymnasium[box2d]\n"
      ],
      "metadata": {
        "id": "lQOMeChdGkDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumtree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blVtwbxdHRav",
        "outputId": "415d7cd7-d7a1-4faf-8f28-6fc90f2d5f0b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumtree\n",
            "  Downloading sumtree-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sumtree\n",
            "Successfully installed sumtree-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8JH7J0XqIVjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumtree import SumTree\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5zhuzKNHP0k",
        "outputId": "1ce591fb-37de-474f-fe86-96e5d05c1d75"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import namedtuple, deque\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "import torch\n",
        "import operator\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, experiences_per_sampling, seed, compute_weights):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            experiences_per_sampling (int): number of experiences to sample during a sampling iteration\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.experiences_per_sampling = experiences_per_sampling\n",
        "\n",
        "        self.alpha = 0.5\n",
        "        self.alpha_decay_rate = 0.99\n",
        "        self.beta = 0.5\n",
        "        self.beta_growth_rate = 1.001\n",
        "        self.seed = random.seed(seed)\n",
        "        self.compute_weights = compute_weights\n",
        "        self.experience_count = 0\n",
        "\n",
        "        self.experience = namedtuple(\"Experience\",\n",
        "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.data = namedtuple(\"Data\",\n",
        "            field_names=[\"priority\", \"probability\", \"weight\",\"index\"])\n",
        "\n",
        "        indexes = []\n",
        "        datas = []\n",
        "        for i in range(buffer_size):\n",
        "            indexes.append(i)\n",
        "            d = self.data(0,0,0,i)\n",
        "            datas.append(d)\n",
        "\n",
        "        self.memory = {key: self.experience for key in indexes}\n",
        "        self.memory_data = {key: data for key,data in zip(indexes, datas)}\n",
        "        self.sampled_batches = []\n",
        "        self.current_batch = 0\n",
        "        self.priorities_sum_alpha = 0\n",
        "        self.priorities_max = 1\n",
        "        self.weights_max = 1\n",
        "\n",
        "\n",
        "\n",
        "    def update_priorities(self, tds, indices):\n",
        "        for td, index in zip(tds, indices):\n",
        "            N = min(self.experience_count, self.buffer_size)\n",
        "\n",
        "            updated_priority = td[0]\n",
        "            if updated_priority > self.priorities_max:\n",
        "                self.priorities_max = updated_priority\n",
        "\n",
        "            if self.compute_weights:\n",
        "                updated_weight = ((N * updated_priority)**(-self.beta))/self.weights_max\n",
        "                if updated_weight > self.weights_max:\n",
        "                    self.weights_max = updated_weight\n",
        "            else:\n",
        "                updated_weight = 1\n",
        "\n",
        "            old_priority = self.memory_data[index].priority\n",
        "            self.priorities_sum_alpha += updated_priority**self.alpha - old_priority**self.alpha\n",
        "            updated_probability = td[0]**self.alpha / self.priorities_sum_alpha\n",
        "            data = self.data(updated_priority, updated_probability, updated_weight, index)\n",
        "            self.memory_data[index] = data\n",
        "\n",
        "    def update_memory_sampling(self):\n",
        "        \"\"\"Randomly sample X batches of experiences from memory.\"\"\"\n",
        "        # X is the number of steps before updating memory\n",
        "        self.current_batch = 0\n",
        "        values = list(self.memory_data.values())\n",
        "        random_values = random.choices(self.memory_data,\n",
        "                                       [data.probability for data in values],\n",
        "                                       k=self.experiences_per_sampling)\n",
        "        self.sampled_batches = [random_values[i:i + self.batch_size]\n",
        "                                    for i in range(0, len(random_values), self.batch_size)]\n",
        "\n",
        "    def update_parameters(self):\n",
        "        self.alpha *= self.alpha_decay_rate\n",
        "        self.beta *= self.beta_growth_rate\n",
        "        if self.beta > 1:\n",
        "            self.beta = 1\n",
        "        N = min(self.experience_count, self.buffer_size)\n",
        "        self.priorities_sum_alpha = 0\n",
        "        sum_prob_before = 0\n",
        "        for element in self.memory_data.values():\n",
        "            sum_prob_before += element.probability\n",
        "            self.priorities_sum_alpha += element.priority**self.alpha\n",
        "        sum_prob_after = 0\n",
        "        for element in self.memory_data.values():\n",
        "            probability = element.priority**self.alpha / self.priorities_sum_alpha\n",
        "            sum_prob_after += probability\n",
        "            weight = 1\n",
        "            if self.compute_weights:\n",
        "                weight = ((N *  element.probability)**(-self.beta))/self.weights_max\n",
        "            d = self.data(element.priority, probability, weight, element.index)\n",
        "            self.memory_data[element.index] = d\n",
        "        print(\"sum_prob before\", sum_prob_before)\n",
        "        print(\"sum_prob after : \", sum_prob_after)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        self.experience_count += 1\n",
        "        index = self.experience_count % self.buffer_size\n",
        "\n",
        "        if self.experience_count > self.buffer_size:\n",
        "            temp = self.memory_data[index]\n",
        "            self.priorities_sum_alpha -= temp.priority**self.alpha\n",
        "            if temp.priority == self.priorities_max:\n",
        "                self.memory_data[index]._replace(priority = 0)\n",
        "                self.priorities_max = max(self.memory_data.items(), key=operator.itemgetter(1))[1][0]\n",
        "            if self.compute_weights:\n",
        "                if temp.weight == self.weights_max:\n",
        "                    self.memory_data[index]._replace(weight = 0)\n",
        "                    self.weights_max = max(self.memory_data.items(), key=operator.itemgetter(2))[1][2]\n",
        "\n",
        "        priority = self.priorities_max\n",
        "        weight = self.weights_max\n",
        "        self.priorities_sum_alpha += priority ** self.alpha\n",
        "        probability = priority ** self.alpha / self.priorities_sum_alpha\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory[index] = e\n",
        "        d = self.data(priority, probability, weight, index)\n",
        "        self.memory_data[index] = d\n",
        "\n",
        "    def sample(self):\n",
        "        sampled_batch = self.sampled_batches[self.current_batch]\n",
        "        self.current_batch += 1\n",
        "        experiences = []\n",
        "        weights = []\n",
        "        indices = []\n",
        "\n",
        "        for data in sampled_batch:\n",
        "            experiences.append(self.memory.get(data.index))\n",
        "            weights.append(data.weight)\n",
        "            indices.append(data.index)\n",
        "\n",
        "        states = torch.from_numpy(\n",
        "            np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(\n",
        "            np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(\n",
        "            np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(\n",
        "            np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(\n",
        "            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones, weights, indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1RqvwhwPA2D",
        "outputId": "366e0a81-dbbf-4425-cfb2-40f944a335d2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh3lXqeDF6bK",
        "outputId": "3cb574b2-7b63-45fd-f67f-80ac1f3c4245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:   cpu\n",
            "Episode 0,  score: -99.7577656729626\n",
            "Episode 20,  score: -547.468916823508\n",
            "Episode 40,  score: -363.40281129923136\n",
            "Episode 60,  score: -289.21218442716065\n",
            "Episode 80,  score: -220.46230282724798\n",
            "Episode 100,  score: -301.8867098237258\n",
            "Episode 120,  score: -218.67333824275102\n",
            "Episode 140,  score: -279.0619520598478\n",
            "Episode 160,  score: -155.3835023164041\n",
            "Episode 180,  score: -308.4364099998279\n",
            "Episode 200,  score: -135.22359641821373\n",
            "Episode 220,  score: -118.46021934294282\n",
            "Episode 240,  score: -183.35165102848805\n",
            "Episode 260,  score: -172.11376025899474\n",
            "Episode 280,  score: -11.067817332777636\n",
            "Episode 300,  score: -329.0281792306222\n",
            "Episode 320,  score: -145.43008843203415\n",
            "Episode 340,  score: -166.2317382615377\n",
            "Episode 360,  score: -211.6041753045741\n",
            "Episode 380,  score: -211.77670018416973\n",
            "Episode 400,  score: -450.1961170697697\n",
            "Episode 420,  score: -302.4083964982169\n",
            "Episode 440,  score: -61.19628386183707\n",
            "Episode 460,  score: -405.85159858770885\n",
            "Episode 480,  score: -386.0954627665159\n",
            "Episode 500,  score: -428.2292179818667\n",
            "Episode 520,  score: -257.639231070364\n",
            "Episode 540,  score: -281.57349052286133\n",
            "Episode 560,  score: -270.97976876991424\n",
            "Episode 580,  score: -145.1015153412538\n",
            "Episode 600,  score: -239.97469010193504\n",
            "Episode 620,  score: -479.21569024220054\n",
            "Episode 640,  score: -187.44060940443904\n",
            "Episode 660,  score: -317.9083802176225\n",
            "Episode 680,  score: -190.6408644583575\n",
            "Episode 700,  score: 36.372023172788545\n",
            "Episode 720,  score: -122.15421218134\n",
            "Episode 740,  score: -356.1594168743115\n",
            "Episode 760,  score: -454.1595887744765\n",
            "Episode 780,  score: -447.52176613175203\n",
            "Episode 800,  score: -241.09172192466102\n",
            "Episode 820,  score: -245.1775617217399\n",
            "Episode 840,  score: -337.17699574419976\n",
            "Episode 860,  score: -271.32818781854564\n",
            "Episode 880,  score: -469.01734201289406\n",
            "Episode 900,  score: -401.68793426652127\n",
            "Episode 920,  score: -382.01671952568046\n",
            "Episode 940,  score: -470.49958660193056\n",
            "Episode 960,  score: -223.43279110849298\n",
            "Episode 980,  score: -236.4530841585608\n",
            "Episode 1000,  score: -209.3610092366057\n",
            "Episode 1020,  score: -199.2237018833983\n",
            "Episode 1040,  score: -445.9193429739329\n",
            "Episode 1060,  score: -358.79370377510054\n",
            "Episode 1080,  score: -319.12506354166413\n",
            "Episode 1100,  score: -269.7588338841082\n",
            "Episode 1120,  score: -205.95014373784014\n",
            "Episode 1140,  score: -178.0556594103984\n",
            "Episode 1160,  score: -417.65293043316586\n",
            "Episode 1180,  score: -172.14877275688625\n",
            "Episode 1200,  score: -427.7851207769483\n",
            "Episode 1220,  score: -160.03857398615855\n",
            "Episode 1240,  score: -419.6303942831333\n",
            "Episode 1260,  score: -345.08130335172564\n",
            "Episode 1280,  score: 33.439423317426474\n",
            "Episode 1300,  score: -147.96352390049964\n",
            "Episode 1320,  score: -265.3788022457667\n",
            "Episode 1340,  score: -443.8583873035065\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import imageio\n",
        "from gym.envs import box2d\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "#### If you encounter error related to \"libiomp5md.dll\", you can try the code in the next line.\n",
        "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:  \", device)\n",
        "# ==================================================================\n",
        "#  Define Basic Classes\n",
        "#  - Finish get_state_values() function in class DQN\n",
        "#  - Finish forward() function and get_state_values() function in DuelingDQN class for bonus\n",
        "# ==================================================================\n",
        "\n",
        "#### Definition of Standard Experience Replay Buffer\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'terminated'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "#### Definition of Standard DQN network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.L1 = nn.Linear(n_observations, 64)\n",
        "        self.L2 = nn.Linear(64, 64)\n",
        "        self.L3 = nn.Linear(64, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.L1(x))\n",
        "        x = F.relu(self.L2(x))\n",
        "        return self.L3(x)\n",
        "\n",
        "    def get_state_action_values(self, state_batch, action_batch):\n",
        "        q_values = self(state_batch)\n",
        "        row_index = torch.arange(0, state_batch.shape[0])\n",
        "        selected_actions_q_values = q_values[row_index, action_batch]\n",
        "        return selected_actions_q_values\n",
        "\n",
        "    def get_state_values(self, state_batch):\n",
        "        q_values = self(state_batch)\n",
        "        # Get the maximum Q value for each state\n",
        "        max_q_values, _ = q_values.max(dim=1)\n",
        "        return max_q_values\n",
        "\n",
        "#### Definition of Dueling Networkss\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        # self.feature_layer = nn.Linear(n_observations, 64)\n",
        "\n",
        "        # Value stream\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(n_observations, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "        # Advantage stream\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(n_observations, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        value = self.value_stream(x)\n",
        "        advantages = self.advantage_stream(x)\n",
        "\n",
        "        # Subtracting the mean of the advantages to make them have zero mean\n",
        "        # This ensures the Q values are properly centered around the value function\n",
        "        Q_values = value + (advantages - advantages.mean(dim=1, keepdim=True))\n",
        "\n",
        "        return Q_values\n",
        "\n",
        "    # Get Q(s,a)\n",
        "    def get_state_action_values(self, state_batch, action_batch):\n",
        "        q_values = self(state_batch)\n",
        "        row_index = torch.arange(0, state_batch.shape[0])\n",
        "        selected_actions_q_values = q_values[row_index, action_batch]\n",
        "        return selected_actions_q_values\n",
        "\n",
        "    def get_state_values(self, state_batch):\n",
        "        # Forward pass through the network to get the Q values for each action\n",
        "        Q_values = self.forward(state_batch)\n",
        "\n",
        "        # Find the maximum Q value among all actions for each state\n",
        "        max_state_values, _ = Q_values.max(dim=1)\n",
        "\n",
        "        return max_state_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#=========================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ===================================\n",
        "#  Hyperparameters\n",
        "# ===================================\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.003\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.002\n",
        "LR = 1e-4\n",
        "# ==================================\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "#  Initialize Environment and Networks\n",
        "# =====================================================================\n",
        "env = gym.make('LunarLander-v2')\n",
        "#### Get the dimension of action and state\n",
        "n_actions = env.action_space.n\n",
        "n_observations = env.observation_space.shape[0]\n",
        "\n",
        "\n",
        "#### Initilize DQN/DDQN Networks and optimizer\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "\n",
        "#### Initilize Dueling Networks and optimizer\n",
        "policy_net_duel = DuelingDQN(n_observations, n_actions).to(device)\n",
        "target_net_duel = DuelingDQN(n_observations, n_actions).to(device)\n",
        "target_net_duel.load_state_dict(policy_net_duel.state_dict())\n",
        "## Only update the parameter for the policy network\n",
        "optimizer_duel = optim.AdamW(policy_net_duel.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "#### Initizalize Experience Replay Buffer\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "#### Initizalize Priority Experience Replay Buffer\n",
        "        # Replay memory\n",
        "\n",
        "# prioritized experience replay\n",
        "UPDATE_NN_EVERY = 1\n",
        "UPDATE_MEM_EVERY = 20          # how often to update the priorities\n",
        "UPDATE_MEM_PAR_EVERY = 3000     # how often to update the hyperparameters\n",
        "EXPERIENCES_PER_SAMPLING = math.ceil(BATCH_SIZE * UPDATE_MEM_EVERY / UPDATE_NN_EVERY)\n",
        "\n",
        "\n",
        "pmemory = ReplayBuffer(action_size=4, buffer_size=int(1e5), batch_size=64, experiences_per_sampling=EXPERIENCES_PER_SAMPLING, seed=0, compute_weights =False)\n",
        "\n",
        "# ======================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================================================================\n",
        "#  Define Main Algorithms\n",
        "#  - Finish the update of optimize_model_DQN() amd optimize_model_DDQN()\n",
        "#  - Finish the update of optimize_model_DN() for bonus\n",
        "# ===============================================================================================================\n",
        "\n",
        "#### Implementation of vanilla DQN\n",
        "def optimize_model_DQN():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    states, actions, rewards, next_states, terminateds = zip(*memory.sample(BATCH_SIZE))\n",
        "\n",
        "    # Convert to tensors\n",
        "    state_batch = torch.tensor(np.array(states), device=device, dtype=torch.float)\n",
        "    action_batch = torch.tensor(actions, device=device).unsqueeze(-1) # Actions are used as indices\n",
        "    reward_batch = torch.tensor(rewards, device=device, dtype=torch.float)\n",
        "    next_state_batch = torch.tensor(np.array(next_states), device=device, dtype=torch.float)\n",
        "    terminated_batch = torch.tensor(terminateds, device=device, dtype=torch.float)\n",
        "\n",
        "    # Compute Q(s, a)\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute max_a Q(s', a) for all next states.\n",
        "    next_state_values = target_net(next_state_batch).max(1)[0].detach()\n",
        "\n",
        "    # If next state is terminal, then there is no next Q value\n",
        "    next_state_values = next_state_values * (1 - terminated_batch)\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)  # Clipping gradients to avoid explosion\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "#### Implementation of Double DQN\n",
        "def optimize_model_DDQN():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    states, actions, rewards, next_states, terminateds = zip(*memory.sample(BATCH_SIZE))\n",
        "\n",
        "    # Convert to tensors\n",
        "    state_batch = torch.tensor(np.array(states), device=device, dtype=torch.float)\n",
        "    action_batch = torch.tensor(actions, device=device).unsqueeze(-1)  # Actions are used as indices\n",
        "    reward_batch = torch.tensor(rewards, device=device, dtype=torch.float)\n",
        "    next_state_batch = torch.tensor(np.array(next_states), device=device, dtype=torch.float)\n",
        "    terminated_batch = torch.tensor(terminateds, device=device, dtype=torch.float)\n",
        "\n",
        "    # Compute Q(s, a) using the policy network\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Select the best action in the next state using the policy network (action selection)\n",
        "    next_state_actions = policy_net(next_state_batch).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "    # Evaluate the selected action's Q value using the target network (action evaluation)\n",
        "    next_state_values = target_net(next_state_batch).gather(1, next_state_actions).squeeze(-1)\n",
        "\n",
        "    # If next state is terminal, then there is no next Q value\n",
        "    next_state_values = next_state_values * (1 - terminated_batch)\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "#### Implementation of Double DQN + Dueling Network\n",
        "def optimize_model_DN():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    # Extract transitions from the replay memory\n",
        "    states, actions, rewards, next_states, terminateds = zip(*memory.sample(BATCH_SIZE))\n",
        "\n",
        "    # Convert arrays into tensors\n",
        "    state_batch = torch.tensor(np.array(states), device=device, dtype=torch.float)\n",
        "    action_batch = torch.tensor(actions, device=device).unsqueeze(-1)\n",
        "    reward_batch = torch.tensor(rewards, device=device, dtype=torch.float)\n",
        "    next_state_batch = torch.tensor(np.array(next_states), device=device, dtype=torch.float)\n",
        "    terminated_batch = torch.tensor(terminateds, device=device, dtype=torch.float)\n",
        "\n",
        "    # Compute Q(s, a) using the policy network\n",
        "    state_action_values = policy_net_duel(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Determine the best action for each next state using the policy network\n",
        "    next_state_actions = policy_net_duel(next_state_batch).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "    # Evaluate the selected actions' Q values using the target network\n",
        "    next_state_values = target_net_duel(next_state_batch).gather(1, next_state_actions).squeeze(-1)\n",
        "\n",
        "    # Apply terminal mask to ensure no next state value is considered for terminal states\n",
        "    next_state_values = next_state_values * (1 - terminated_batch)\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer_duel.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_duel.step()\n",
        "# ==============================================================================================================================\n",
        "\n",
        "#### Implementation of Double DQN + Dueling Network + PriorityRelay\n",
        "def optimize_model_DNP():\n",
        "    if len(pmemory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "\n",
        "    # Extract transitions into separate arrays\n",
        "    states, actions, rewards, next_states, terminateds, weights, indices = pmemory.sample()\n",
        "\n",
        "    # Convert arrays into tensors\n",
        "    state_batch = torch.tensor(np.array(states), device=device, dtype=torch.float)\n",
        "    action_batch = torch.tensor(actions, device=device).unsqueeze(-1)\n",
        "    reward_batch = torch.tensor(rewards, device=device, dtype=torch.float)\n",
        "    next_state_batch = torch.tensor(np.array(next_states), device=device, dtype=torch.float)\n",
        "    terminated_batch = torch.tensor(terminateds, device=device, dtype=torch.float)\n",
        "\n",
        "    # Compute Q(s, a) using the policy network\n",
        "    state_action_values = policy_net_duel(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Determine the best action for each next state using the policy network\n",
        "    next_state_actions = policy_net_duel(next_state_batch).max(1)[1].unsqueeze(-1)\n",
        "\n",
        "    # Evaluate the selected actions' Q values using the target network\n",
        "    next_state_values = target_net_duel(next_state_batch).gather(1, next_state_actions).squeeze(-1)\n",
        "\n",
        "    # Apply terminal mask to ensure no next state value is considered for terminal states\n",
        "    next_state_values = next_state_values * (1 - terminated_batch)\n",
        "\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
        "\n",
        "    # Compute TD errors for prioritized experience replay\n",
        "    td_errors = (state_action_values - expected_state_action_values.unsqueeze(1)).abs().squeeze(-1)\n",
        "\n",
        "    # Update priorities in the prioritized experience replay memory\n",
        "    pmemory.update_priorities( td_errors.detach().cpu().numpy(),indices)\n",
        "\n",
        "    # Compute loss weighted by importance sampling weights\n",
        "    weighted_loss = (weights * F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1), reduction='none')).mean()\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer_duel.zero_grad()\n",
        "    weighted_loss.backward()\n",
        "    optimizer_duel.step()\n",
        "# ==============================================================================================================================\n",
        "\n",
        "\n",
        "# ===============================================================================================================\n",
        "#  Main Train Loop\n",
        "#  - Finish the epsilon greedy exploration\n",
        "# ===============================================================================================================\n",
        "\n",
        "#### Training Episodes\n",
        "NUM_EPISODES = 2000\n",
        "\n",
        "#### Training Loop. If the input algorithm == \"DQN\", it will utilize DQN to train.\n",
        "#### Similarly, if the input algorithm == \"DDQN\", it will utilize DDQN to train. If the input algorithm == \"DN\", it will utilize Dueling Networks to train\n",
        "def train_models(algorithm):\n",
        "    episode_returns = []  # To store returns from each episode\n",
        "\n",
        "    for iteration in range(NUM_EPISODES):\n",
        "        current_episode_return = 0  # Reset the return for the current episode\n",
        "        state, _ = env.reset()  # Reset the environment\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        # Initialize time step (for updating every UPDATE_NN_EVERY steps)\n",
        "        t_step_nn = 0\n",
        "        # Initialize time step (for updating every UPDATE_MEM_PAR_EVERY steps)\n",
        "        t_step_mem_par = 0\n",
        "        # Initialize time step (for updating every UPDATE_MEM_EVERY steps)\n",
        "        t_step_mem = 0\n",
        "\n",
        "\n",
        "\n",
        "        while not (terminated or truncated):\n",
        "            # Epsilon-Greedy Exploration\n",
        "            eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * iteration / EPS_DECAY)\n",
        "            if random.random() > eps:\n",
        "                # Convert state to tensor\n",
        "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    if algorithm in [\"DQN\", \"DDQN\"]:\n",
        "                        action_values = policy_net(state_tensor)\n",
        "                    elif algorithm == \"DN\" or algorithm == \"DNP\":\n",
        "                        action_values = policy_net_duel(state_tensor)\n",
        "                    action = action_values.max(1)[1].item()  # Choose the action with the highest Q-value\n",
        "            else:\n",
        "                action = random.randrange(n_actions)  # Choose a random action\n",
        "\n",
        "            # Take action in the environment\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            if not algorithm == 'DNP':\n",
        "                memory.push(state, action, reward, next_state, terminated)  # Save the transition in memory\n",
        "            else:\n",
        "                pmemory.push(state, action, reward, next_state, terminated)\n",
        "                        # Learn every UPDATE_NN_EVERY time steps.\n",
        "                t_step_nn = (t_step_nn + 1) # UPDATE_NN_EVERY\n",
        "                t_step_mem = (t_step_mem + 1) # UPDATE_MEM_EVERY\n",
        "                t_step_mem_par = (t_step_mem_par + 1) # UPDATE_MEM_PAR_EVERY\n",
        "                if t_step_mem_par == 0:\n",
        "                    memory.update_parameters()\n",
        "                if t_step_nn == 0:\n",
        "                    # If enough samples are available in memory, get random subset and learn\n",
        "                    if memory.experience_count > EXPERIENCES_PER_SAMPLING:\n",
        "                      optimize_model_DN()\n",
        "\n",
        "                if t_step_mem == 0:\n",
        "                    memory.update_memory_sampling()\n",
        "\n",
        "\n",
        "\n",
        "            current_episode_return += reward\n",
        "\n",
        "            # Update the target network parameters\n",
        "            if algorithm in [\"DQN\", \"DDQN\"]:\n",
        "                for target_par0am, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
        "                    target_param.data.copy_(TAU * policy_param.data + (1.0 - TAU) * target_param.data)\n",
        "            elif algorithm == \"DN\" or algorithm == \"DNP\":\n",
        "                for target_param, policy_param in zip(target_net_duel.parameters(), policy_net_duel.parameters()):\n",
        "                    target_param.data.copy_(TAU * policy_param.data + (1.0 - TAU) * target_param.data)\n",
        "\n",
        "            if terminated or truncated:\n",
        "                if iteration % 20 == 0:\n",
        "                    print('Episode {},  score: {}'.format(iteration, current_episode_return))\n",
        "                episode_returns.append(current_episode_return)\n",
        "            else:\n",
        "                state = next_state  # Update the state\n",
        "\n",
        "            # Optimize the model according to the selected algorithm\n",
        "            if algorithm == \"DQN\":\n",
        "                optimize_model_DQN()\n",
        "            elif algorithm == \"DDQN\":\n",
        "                optimize_model_DDQN()\n",
        "            elif algorithm == \"DN\":\n",
        "                optimize_model_DN()\n",
        "\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.title('Training with ' + algorithm)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    # plt.plot(episode_returns)\n",
        "\n",
        "    # Plot the full episode returns without markers\n",
        "    plt.plot(episode_returns, color='blue', alpha=0.5)\n",
        "\n",
        "    # Create a subset of episode_returns with only the points after every 20 episodes\n",
        "    marker_indices = [i for i in range(len(episode_returns)) if i % 20 == 0]\n",
        "    marker_returns = [episode_returns[i] for i in marker_indices]\n",
        "\n",
        "    # Plot the subset of episode returns with circular markers\n",
        "    plt.plot(marker_indices, marker_returns, 'o', markersize=5, color='orange')\n",
        "\n",
        "    plt.savefig(\"Training with \" + algorithm)\n",
        "    plt.show()\n",
        "\n",
        "# ===============================================================================================================\n",
        "#  Functions to generate videos for visulization\n",
        "#  (This part is only for playing. You do not need to upload any video for homework)\n",
        "#  To play with your own policy, you can save your policy network and put it into generate_videos(policy) as policy\n",
        "#  You also need to complete a line in generate_videos(policy) to play.\n",
        "#  This line is the same as the line you finishes in train_models(algorithm), choosing the best action based on your model.\n",
        "# ===============================================================================================================\n",
        "def generate_videos(policy):\n",
        "    env = gym.make('LunarLander-v2', render_mode = 'rgb_array')\n",
        "    n_actions = env.action_space.n\n",
        "    n_observations = env.observation_space.shape[0]\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    img = env.render()\n",
        "    frames.append(img)\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "    while not (terminated or truncated):\n",
        "        if policy == \"random\":\n",
        "            action = np.random.randint(0, n_actions)\n",
        "        else:\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                # Example for using the DuelingDQN policy network\n",
        "                action_values = policy_net_duel(state_tensor)\n",
        "            action = action_values.max(1)[1].item()\n",
        "        next_state, _, terminated, truncated, _ = env.step(action)\n",
        "        img = env.render()\n",
        "        frames.append(img)\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "        state = next_state\n",
        "    imageio.mimsave(f'lunar_lander_{policy}.mp4', frames, fps=60)\n",
        "# ===============================================================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_models(\"DNP\")\n",
        "generate_videos(\"DNP\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "04v-tUMoGblA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}